AGL Technology Questionnaire						
RFP Questionnaire ID	Functional Area	Question	Provide details to address AGL's questions	References	Response Definition	Customisation Definition
RES.001	General Architecture & Strategy	Describe your overall data architecture and how it scales across multi-cloud and hybrid environments.	Databricks lakehouse architecture solves AGL's multi-cloud challenge: Kaluza and Salesforce on AWS, corporate systems on Azure, with your Synapse migration underway. Three core design principles eliminate data silos while enabling unified governance.  Compute-storage separation enables independent scaling. Compute runs in managed planes while data stays in ADLS Gen2 or S3. Serverless compute provisions in 5-10 seconds, classic clusters autoscale from 2 to 2000+ nodes based on demand. Photon engine delivers up to 12x better price-performance for SQL workloads, directly supporting your Business Intelligence value driver.  Unity Catalog provides single governance across all clouds and regions, and delivers fine-grained access control, lineage, and auditing across multiple workspaces. Cross-cloud data access via Delta Sharing and Lakehouse Federation enables direct S3 queries from Azure without ETL, eliminating costly data replication.  Open data formats (Delta Lake, Apache Iceberg™) ensure workload portability. Consistent APIs across AWS, Azure, and GCP prevent vendor lock-in. Delta Sharing facilitates secure data exchange across clouds and on-premises without duplication.  For AGL specifically, you can query Kaluza billing data in AWS S3 directly from Azure, ingest Salesforce CRM data via Lakeflow Connect with zero custom code, integrate Splunk security logs from AWS with Azure operational data, and build Customer 360 analytics combining data from both clouds. All under unified governance with consistent data classification, row-column security, and audit logging. This architecture reduces cross-cloud egress costs through selective replication while maintaining a single platform supporting your Climate Transition Action Plan (CTAP) objectives.  	References: What is a data lakehouse? Introduction to the well-architected data lakehouse Guiding principles for the lakehouse Interoperability and usability for the data lakehouse How FedEx built a scalable, enterprise-grade data platform with IT automation and Databricks Unity Catalog	Full	No (Configuration)
RES.002	General Architecture & Strategy	How do you embed security and compliance controls at every layer of your architecture?	Databricks implements defense-in-depth security across identity, network, data, and monitoring layers, with controls consistently applied across Azure and AWS.  Identity and access integrate with Microsoft Entra ID for SSO and MFA. Unity Catalog manages permissions as policy-as-code using SQL GRANT statements or Terraform, with all changes audited in system.access.audit tables (365-day retention).  Network isolation uses VNet Injection with Azure Private Link, eliminating the need for public IPs through Secure Cluster Connectivity. Serverless compute provides multi-layer isolation with deny-by-default Network Policies controlling outbound traffic.  Data protection applies TLS 1.2+ in transit and AES-256 at rest. Customer-Managed Keys from Azure Key Vault enable sovereign key control. Unity Catalog enforces fine-grained RBAC, row filters, column masks, and attribute-based policies. Automated Data Classification detects and masks sensitive fields, including PII.  Monitoring includes system tables with 10-minute freshness, diagnostic logs streamed to Splunk, and comprehensive audit logging. Databricks maintains SOC 2, ISO 27001, HIPAA, and IRAP certifications.  To support AGL's IT/OT convergence requirements, IRAP PROTECTED capability (Public Preview) provides Private Link for operational telemetry and automated PII masking across energy infrastructure domains, aligning with your Assets and Business Intelligence value drivers.  	References: Sync users and groups automatically from Microsoft Entra ID - Databricks (on AWS and Azure) Infosec Registered Assessors Program (IRAP) - Databricks (on AWS and Azure) Manage network policies for serverless compute Customer-managed keys for encryption Unity Catalog attribute-based access control (ABAC) Access control in Unity Catalog Cloud Computing Compliance Criteria Catalog (C5) AI Architecture: Building Enterprise AI Systems with Governance How to scale data governance with Attribute-Based Access Control in Unity Catalog A Unified Approach to Data Exfiltration Protection on Databricks	Full	No (Configuration)
RES.003	General Architecture & Strategy	Explain your disaster recovery architecture and RPO/RTO guarantees.	Databricks disaster recovery leverages a stateless compute architecture and highly durable cloud storage to support active-passive DR across AGL's multi-region, multicloud topology (Azure australiasoutheast/australiaeast and AWS ap-southeast-2).  Key Architecture Principles:  Stateless Compute: Databricks clusters are ephemeral and stateless—compute resources process data but do not store it. Clusters can be terminated and recreated without data loss because all data persists in cloud storage rather than on cluster nodes. This enables DR by allowing compute to be spun up in any region where data is replicated.  Cloud Storage Durability: Customer data relies on highly durable cloud storage—AWS S3 provides 99.999999999% (11 nines) durability with regional redundancy across availability zones,  while Azure ADLS Gen2 offers zone-redundant (ZRS) or geo-zone-redundant storage (GZRS) with similar high durability guarantees.   Workspace Replication: Paired workspaces are deployed using Infrastructure-as-Code (Terraform/CI-CD) to replicate workspace assets. Data replication uses Delta Lake Deep Clone between ADLS Gen2 and S3 to maintain transactional consistency.   Governance and Metadata: Unity Catalog provides centralised governance with scripted metadata replication to maintain permissions and lineage in secondary regions. Cross-cloud scenarios use Databricks-to-Databricks Delta Sharing for metadata federation.   RPO/RTO: Databricks does NOT provide RPO/RTO guarantees—these are customer-defined based on replication frequency and the implementation of the DR strategy. RPO depends on Delta Deep Clone frequency or cloud provider replication SLAs; RTO depends on failover automation and pre-provisioned infrastructure.   Execution: Lakeflow Spark Declarative Pipelines and Jobs deploy to all regions. Batch pipelines restart on failover; streaming workloads require checkpoint management in customer storage.  Identity (Entra ID/SCIM), networking (VNet/PrivateLink on Azure, VPC/PrivateLink on AWS), and monitoring (Azure Monitor, CloudWatch, Splunk integration) follow AGL standards.  While Databricks does have a business continuity plan, a backup policy and procedure that is routinely tested, and disaster recovery plans, Databricks does not offer disaster recovery capabilities to customers. As such, we cannot provide RTO or RPO objectives. The datasets customers write as a part of your usage of Databricks are typically stored in your account in S3/ADLS/GCS or similar, so customers will typically maintain their own backup process. Customers can use the capabilities of Databricks to implement their own required disaster recovery capabilities. We provide recommendations so that customers can backup your data in customers systems rather than providing backup services ourselves.   Databricks performs backups of our infrastructure (and uses modern cloud architectures like stateless services). Databricks follows industry best-practice standards in designing, documenting and testing its BC/DR programs. These programs are independently validated, at least annually, as part of the our SOC 1 and SOC 2, and our ISO 27001, 27017 and 27018 certification reviews. 	References: Best practices for reliability How Databricks Managed Disaster Recovery Helps Capital One Achieve Lakehouse Resilience Disaster Recovery	Partial	No (Customisation)
RES.004	General Architecture & Strategy	What differentiates your platform for supporting generative AI and LLM workloads?	Databricks differentiates through enterprise-grade governance and legal protection that eliminates the fragmentation of multi-vendor AI stacks.  Our Multi-AI Indemnity covers all major frontier models (OpenAI, Anthropic, Meta, Google) deployed through Model Serving, protecting AGL from IP claims including judgements, settlements and legal fees. This provides legal certainty for production AI deployments without vendor lock-in.  Unity Catalog delivers centralised governance across all AI assets: models, prompts, vector indexes, agent workflows and AI/BI dashboards. Every agent is traceable from development through production with automated lineage, fine-grained access controls and comprehensive audit trails. This prevents governance gaps when AI tools operate outside enterprise data platforms, directly supporting AGL's Business Intelligence value driver.  For rapid development, Agent Bricks Agent Bricks enables domain-specific agents with automated prompt optimisation and quality evaluation. Business users build production-ready agents in hours rather than weeks. AI/BI extends this to natural language analytics for structured data queries.  Technical flexibility includes direct deployment of multiple foundation models via OpenAI-compatible APIs, native Agent Bricks Vector Search eliminating external database complexity, and Lakebase providing sub-10ms operational data access for real-time agent context. MLflow 3 ensures every experiment and deployment is reproducible and auditable.  This unified approach accelerates AGL's AI initiatives while maintaining governance standards required for regulated energy operations. AI needs the context of your enterprise data to be truly useful.  	References: Databricks launches first Multi-AI Indemnity to protect enterprise AI innovation Agent Bricks documentation Agent Bricks Vector Search Lakebase OLTP instances MLflow 3 installation Agent Bricks capabilities for GenAI Introduction to generative AI apps on Azure Databricks Build gen AI apps on Databricks Data and AI Strategy Platform Focus	Full	Yes
RES.005	General Architecture & Strategy	How do you incorporate automation and AI-driven optimisation in your platform roadmap?	Databricks embeds AI-driven optimisation throughout our platform roadmap, automatically improving performance without increasing operational overhead for AGL's data teams.  Our Predictive Optimisation uses AI to maintain Unity Catalog tables automatically - running OPTIMIZE, VACUUM, and ANALYZE based on usage patterns. Enabled by default since November 2024, production deployments show 70% faster queries over three years through automatic liquid clustering, Bloom filters, and Native IO optimisations. Over 2,400 customers have achieved up to 20x query performance improvements and 2x storage cost reductions.  Compute efficiency evolves continuously through AI-driven resource management. Serverless SQL warehouses use machine learning to scale resources to workload demand, with 5x faster downscaling for improved total cost of ownership. Engine improvements delivered 5x performance gains since 2022, with an additional 25% improvement in June 2025 - automatically applied with no configuration changes required.  Real workloads demonstrate measurable impact: ETL workloads 31% faster year-over-year, BI queries 73% faster over two years.  For AGL, this means your data platform becomes more efficient over time without manual tuning - freeing your teams to focus on delivering insights that support AI/BI value drivers and data-driven decision-making for Australia's lower-emissions future rather than infrastructure management.  	References: Databricks SQL Accelerates Customer Workloads 5x in Just Three Years Announcing Automatic Liquid Clustering Predictive Optimization Automatically Delivers Faster Queries and Lower TCO Predictive Optimization Documentation How We Debug 1000s of Databases with AI at Databricks	Full	Yes
