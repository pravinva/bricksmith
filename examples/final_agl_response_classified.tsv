AGL Technology Questionnaire						
RFP Questionnaire ID	Functional Area	Question	Provide details to address AGL's questions	References	Response Definition	Customisation Definition
RES.001	General Architecture & Strategy	Describe your overall data architecture and how it scales across multi-cloud and hybrid environments.	Databricks lakehouse architecture solves AGL's multi-cloud challenge: Kaluza and Salesforce on AWS, corporate systems on Azure, with your Synapse migration underway. Three core design principles eliminate data silos while enabling unified governance.  Compute-storage separation enables independent scaling. Compute runs in managed planes while data stays in ADLS Gen2 or S3. Serverless compute provisions in 5-10 seconds, classic clusters autoscale from 2 to 2000+ nodes based on demand. Photon engine delivers up to 12x better price-performance for SQL workloads, directly supporting your Business Intelligence value driver.  Unity Catalog provides single governance across all clouds and regions, and delivers fine-grained access control, lineage, and auditing across multiple workspaces. Cross-cloud data access via Delta Sharing and Lakehouse Federation enables direct S3 queries from Azure without ETL, eliminating costly data replication.  Open data formats (Delta Lake, Apache Iceberg™) ensure workload portability. Consistent APIs across AWS, Azure, and GCP prevent vendor lock-in. Delta Sharing facilitates secure data exchange across clouds and on-premises without duplication.  For AGL specifically, you can query Kaluza billing data in AWS S3 directly from Azure, ingest Salesforce CRM data via Lakeflow Connect with zero custom code, integrate Splunk security logs from AWS with Azure operational data, and build Customer 360 analytics combining data from both clouds. All under unified governance with consistent data classification, row-column security, and audit logging. This architecture reduces cross-cloud egress costs through selective replication while maintaining a single platform supporting your Climate Transition Action Plan (CTAP) objectives.  	References: What is a data lakehouse? Introduction to the well-architected data lakehouse Guiding principles for the lakehouse Interoperability and usability for the data lakehouse How FedEx built a scalable, enterprise-grade data platform with IT automation and Databricks Unity Catalog	Full	Yes
RES.002	General Architecture & Strategy	How do you embed security and compliance controls at every layer of your architecture?	Databricks implements defense-in-depth security across identity, network, data, and monitoring layers, with controls consistently applied across Azure and AWS.  Identity and access integrate with Microsoft Entra ID for SSO and MFA. Unity Catalog manages permissions as policy-as-code using SQL GRANT statements or Terraform, with all changes audited in system.access.audit tables (365-day retention).  Network isolation uses VNet Injection with Azure Private Link, eliminating the need for public IPs through Secure Cluster Connectivity. Serverless compute provides multi-layer isolation with deny-by-default Network Policies controlling outbound traffic.  Data protection applies TLS 1.2+ in transit and AES-256 at rest. Customer-Managed Keys from Azure Key Vault enable sovereign key control. Unity Catalog enforces fine-grained RBAC, row filters, column masks, and attribute-based policies. Automated Data Classification detects and masks sensitive fields, including PII.  Monitoring includes system tables with 10-minute freshness, diagnostic logs streamed to Splunk, and comprehensive audit logging. Databricks maintains SOC 2, ISO 27001, HIPAA, and IRAP certifications.  To support AGL's IT/OT convergence requirements, IRAP PROTECTED capability (Public Preview) provides Private Link for operational telemetry and automated PII masking across energy infrastructure domains, aligning with your Assets and Business Intelligence value drivers.  	References: Sync users and groups automatically from Microsoft Entra ID - Databricks (on AWS and Azure) Infosec Registered Assessors Program (IRAP) - Databricks (on AWS and Azure) Manage network policies for serverless compute Customer-managed keys for encryption Unity Catalog attribute-based access control (ABAC) Access control in Unity Catalog Cloud Computing Compliance Criteria Catalog (C5) AI Architecture: Building Enterprise AI Systems with Governance How to scale data governance with Attribute-Based Access Control in Unity Catalog A Unified Approach to Data Exfiltration Protection on Databricks	Full	Yes
RES.003	General Architecture & Strategy	Explain your disaster recovery architecture and RPO/RTO guarantees.	Databricks disaster recovery leverages a stateless compute architecture and highly durable cloud storage to support active-passive DR across AGL's multi-region, multicloud topology (Azure australiasoutheast/australiaeast and AWS ap-southeast-2).  Key Architecture Principles:  Stateless Compute: Databricks clusters are ephemeral and stateless—compute resources process data but do not store it. Clusters can be terminated and recreated without data loss because all data persists in cloud storage rather than on cluster nodes. This enables DR by allowing compute to be spun up in any region where data is replicated.  Cloud Storage Durability: Customer data relies on highly durable cloud storage—AWS S3 provides 99.999999999% (11 nines) durability with regional redundancy across availability zones,  while Azure ADLS Gen2 offers zone-redundant (ZRS) or geo-zone-redundant storage (GZRS) with similar high durability guarantees.   Workspace Replication: Paired workspaces are deployed using Infrastructure-as-Code (Terraform/CI-CD) to replicate workspace assets. Data replication uses Delta Lake Deep Clone between ADLS Gen2 and S3 to maintain transactional consistency.   Governance and Metadata: Unity Catalog provides centralised governance with scripted metadata replication to maintain permissions and lineage in secondary regions. Cross-cloud scenarios use Databricks-to-Databricks Delta Sharing for metadata federation.   RPO/RTO: Databricks does NOT provide RPO/RTO guarantees—these are customer-defined based on replication frequency and the implementation of the DR strategy. RPO depends on Delta Deep Clone frequency or cloud provider replication SLAs; RTO depends on failover automation and pre-provisioned infrastructure.   Execution: Lakeflow Spark Declarative Pipelines and Jobs deploy to all regions. Batch pipelines restart on failover; streaming workloads require checkpoint management in customer storage.  Identity (Entra ID/SCIM), networking (VNet/PrivateLink on Azure, VPC/PrivateLink on AWS), and monitoring (Azure Monitor, CloudWatch, Splunk integration) follow AGL standards.  While Databricks does have a business continuity plan, a backup policy and procedure that is routinely tested, and disaster recovery plans, Databricks does not offer disaster recovery capabilities to customers. As such, we cannot provide RTO or RPO objectives. The datasets customers write as a part of your usage of Databricks are typically stored in your account in S3/ADLS/GCS or similar, so customers will typically maintain their own backup process. Customers can use the capabilities of Databricks to implement their own required disaster recovery capabilities. We provide recommendations so that customers can backup your data in customers systems rather than providing backup services ourselves.   Databricks performs backups of our infrastructure (and uses modern cloud architectures like stateless services). Databricks follows industry best-practice standards in designing, documenting and testing its BC/DR programs. These programs are independently validated, at least annually, as part of the our SOC 1 and SOC 2, and our ISO 27001, 27017 and 27018 certification reviews. 	References: Best practices for reliability How Databricks Managed Disaster Recovery Helps Capital One Achieve Lakehouse Resilience Disaster Recovery	Full	Yes
RES.004	General Architecture & Strategy	What differentiates your platform for supporting generative AI and LLM workloads?	Databricks differentiates through enterprise-grade governance and legal protection that eliminates the fragmentation of multi-vendor AI stacks.  Our Multi-AI Indemnity covers all major frontier models (OpenAI, Anthropic, Meta, Google) deployed through Model Serving, protecting AGL from IP claims including judgements, settlements and legal fees. This provides legal certainty for production AI deployments without vendor lock-in.  Unity Catalog delivers centralised governance across all AI assets: models, prompts, vector indexes, agent workflows and AI/BI dashboards. Every agent is traceable from development through production with automated lineage, fine-grained access controls and comprehensive audit trails. This prevents governance gaps when AI tools operate outside enterprise data platforms, directly supporting AGL's Business Intelligence value driver.  For rapid development, Agent Bricks Agent Bricks enables domain-specific agents with automated prompt optimisation and quality evaluation. Business users build production-ready agents in hours rather than weeks. AI/BI extends this to natural language analytics for structured data queries.  Technical flexibility includes direct deployment of multiple foundation models via OpenAI-compatible APIs, native Agent Bricks Vector Search eliminating external database complexity, and Lakebase providing sub-10ms operational data access for real-time agent context. MLflow 3 ensures every experiment and deployment is reproducible and auditable.  This unified approach accelerates AGL's AI initiatives while maintaining governance standards required for regulated energy operations. AI needs the context of your enterprise data to be truly useful.  	References: Databricks launches first Multi-AI Indemnity to protect enterprise AI innovation Agent Bricks documentation Agent Bricks Vector Search Lakebase OLTP instances MLflow 3 installation Agent Bricks capabilities for GenAI Introduction to generative AI apps on Azure Databricks Build gen AI apps on Databricks Data and AI Strategy Platform Focus	Full	Yes
RES.005	General Architecture & Strategy	How do you incorporate automation and AI-driven optimisation in your platform roadmap?	Databricks embeds AI-driven optimisation throughout our platform roadmap, automatically improving performance without increasing operational overhead for AGL's data teams.  Our Predictive Optimisation uses AI to maintain Unity Catalog tables automatically - running OPTIMIZE, VACUUM, and ANALYZE based on usage patterns. Enabled by default since November 2024, production deployments show 70% faster queries over three years through automatic liquid clustering, Bloom filters, and Native IO optimisations. Over 2,400 customers have achieved up to 20x query performance improvements and 2x storage cost reductions.  Compute efficiency evolves continuously through AI-driven resource management. Serverless SQL warehouses use machine learning to scale resources to workload demand, with 5x faster downscaling for improved total cost of ownership. Engine improvements delivered 5x performance gains since 2022, with an additional 25% improvement in June 2025 - automatically applied with no configuration changes required.  Real workloads demonstrate measurable impact: ETL workloads 31% faster year-over-year, BI queries 73% faster over two years.  For AGL, this means your data platform becomes more efficient over time without manual tuning - freeing your teams to focus on delivering insights that support AI/BI value drivers and data-driven decision-making for Australia's lower-emissions future rather than infrastructure management.  	References: Databricks SQL Accelerates Customer Workloads 5x in Just Three Years Announcing Automatic Liquid Clustering Predictive Optimization Automatically Delivers Faster Queries and Lower TCO Predictive Optimization Documentation How We Debug 1000s of Databases with AI at Databricks	Full	Yes
RES.006	Data Quality & Profiling	Does your solution offer rule-based and ML-driven data quality checks at scale?	Yes. Databricks provides both rule-based and ML-driven data quality checks at scale.  Rule-based checks use Lakeflow Spark Declarative Pipelines with SQL-based EXPECT constraints. You define quality gates with configurable actions (warn, drop, or fail) on invalid records. Expectation metrics are queryable from pipeline event logs and version-controlled for consistent promotion across environments. This ensures repeatable quality standards as data products move from development to production.  ML-driven checks leverage Data Quality Monitoring (currently in Beta) for automated anomaly detection across freshness, completeness, and statistical patterns. The system learns normal data behaviour and flags deviations without manual rule creation. Results log to system tables for alerting and root cause analysis, with downstream impact metrics to prioritise remediation.  Both approaches integrate with Unity Catalog for column-level lineage tracking, role-based access controls, and audit logging. This unified governance layer ensures quality checks are enforced consistently across AGL's data estate whilst maintaining visibility into data provenance and quality trends.  For AGL's Business Intelligence value driver, this means quality gates prevent bad data propagation, anomaly detection catches regressions early, and lineage accelerates root cause analysis when issues arise.  	References: Data quality monitoring | Databricks on AWS Data profiling | Databricks on AWS Best practices for data and AI governance - Azure Databricks Building High-Quality and Trusted Data Products with Databricks	Full	Yes
RES.007	Data Quality & Profiling	How do you handle real-time data quality checks for streaming pipelines?	Databricks handles real-time data quality through Lakeflow Spark Declarative Pipelines with integrated expectations that validate data as it streams. Quality rules are defined declaratively in SQL or Python within table definitions, checking schema, nullability, referential integrity, and business logic inline with data flow.  Configurable violation actions (fail, drop, or alert) provide flexible handling based on criticality. All validation metrics are automatically captured in queryable event logs, providing audit trails for regulatory compliance and SLA monitoring critical to AGL's energy market operations.  The unified batch-streaming approach applies identical quality logic across both processing modes, eliminating dual-stack complexity. For AGL's energy market data, this means consistent validation across real-time meter readings and historical analysis without maintaining separate rule sets.  Unity Catalog delivers near real-time column-level lineage with centralised RBAC and comprehensive auditing via system.access.audit. Event logs expose granular metrics (passed records, failed records, dropped records per expectation) accessible via SQL for operational dashboards.  Lakeflow Connect Zerobus and native Azure Event Hubs integration supports low-latency, validated streams for consumer-facing services, aligning with the Australian Energy Market Commission's requirements for real-time consumer energy data access, mandated from 1 January 2028.  	References: Monitor pipelines Best practices for data and AI governance Processing Millions of Events from Thousands of Aircraft with One Declarative Pipeline Introducing Streaming Observability in Lakeflow Jobs and Lakeflow Spark Declarative Pipelines	Full	Yes
RES.008	Data Quality & Profiling	What is your approach to embedding data quality rules into CI/CD workflows?	Databricks handles real-time data quality through Lakeflow Spark Declarative Pipelines with integrated expectations that validate data as it streams. Quality rules are defined declaratively in SQL or Python within table definitions, checking schema, nullability, referential integrity, and business logic inline with data flow.  Configurable violation actions (fail, drop, or alert) provide flexible handling based on criticality. All validation metrics are automatically captured in queryable event logs, providing audit trails for regulatory compliance and SLA monitoring critical to AGL's energy market operations.  The unified batch-streaming approach applies identical quality logic across both processing modes, eliminating dual-stack complexity. For AGL's energy market data, this means consistent validation across real-time meter readings and historical analysis without maintaining separate rule sets.  Unity Catalog delivers near real-time column-level lineage with centralised RBAC and comprehensive auditing via system.access.audit. Event logs expose granular metrics (passed records, failed records, dropped records per expectation) accessible via SQL for operational dashboards.  Lakeflow Connect Zerobus and native Azure Event Hubs integration supports low-latency, validated streams for consumer-facing services, aligning with the Australian Energy Market Commission's requirements for real-time consumer energy data access, mandated from 1 January 2028.  	References: Best practices and recommended CI/CD workflows on Databricks Best practices for data and AI governance Building High-Quality and Trusted Data Products with Databricks CI/CD on Databricks	Partial	Yes
RES.009	Data Quality & Profiling	Explain how your solution applies ML-driven anomaly detection to improve data quality.	Databricks delivers ML-driven anomaly detection through a two-layer approach that strengthens AGL's Business Intelligence value driver by catching quality issues before they impact downstream analytics and reporting.  The first layer is automated ML monitoring at the schema level. Databricks builds per-table models of freshness and completeness patterns across Unity Catalog tables, surfacing health indicators to data consumers and logging detection results for alerting. This runs as a background job without modifying data, respecting Unity Catalog's privilege model to ensure detections are visible only to authorised users. The capability reduces manual validation effort and accelerates identification of data drift or upstream system failures.  The second layer is in-pipeline enforcement through Lakeflow Spark Declarative Pipelines. Lakeflow Expectations validate each record using SQL or Python rules with configurable actions: warn, drop, or fail. Quality metrics are captured in the pipeline event log with full lineage visible in Unity Catalog for end-to-end audit queries.  As ML anomaly detection is currently in Beta, AGL should adopt staged enablement and validate alert thresholds in non-production environments first. For critical ingestion paths supporting CTAP reporting and customer analytics, rely on Expectations to enforce quality until broader maturity is achieved.  	References: Anomaly detection - Azure Databricks Data quality monitoring - Azure Databricks Training 10,000 Anomaly Detection Models on One Billion Records with Explainable Predictions	Full	Yes
RES.010	Data Quality & Profiling	How do you measure and report data quality KPIs for executive dashboards?	Databricks delivers executive data quality visibility through Unity Catalog metric views that standardise KPIs across your organisation. We define business-critical measures like freshness, completeness, accuracy, and timeliness once, then surface them consistently across Power BI dashboards, alerts, and BI tools supporting AGL's Business Intelligence value driver.  Lakehouse Monitoring automates quality tracking with schema-level anomaly detection and detailed profiling with drift analysis. Custom metrics defined via SQL expressions automatically populate AI/BI dashboards showing trends, anomalies, and KPI scores over time. This provides executives with real-time visibility into data health across your renewable generation, customer, and trading systems.  Lakeflow Spark Declarative Pipelines enforce quality rules at the pipeline level, logging results to event logs and system tables for full auditability across bronze-silver-gold layers. Unity Catalog system tables retain 365 days of monitoring results, queryable via Databricks SQL to build executive reports or configure proactive alerts.  Dashboards integrate with Entra ID for role-based access and export logs to Azure Monitor or Sentinel for enterprise observability, supporting AGL's governance requirements.  	References: Data quality monitoring overview Unity Catalog metric views Lakehouse Monitoring GA announcement Data profiling dashboard Anomaly detection Unity Catalog Governance in Action	Full	Yes
RES.011	Data Quality & Profiling	What pre-built rules exist for validating NEM wholesale market data and SCADA telemetry?	Databricks does not provide pre-packaged NEM or SCADA validation templates. Instead, it offers configurable frameworks that allow AGL to define domain-specific data quality rules tailored to wholesale market data and telemetry requirements.  Lakeflow SDP expectations enable inline quality rules in SQL or Python to validate value ranges, timestamp integrity, and mandatory fields. Expectations can warn, drop, or fail records on violations and support reusable patterns stored in Delta tables for consistent application across pipelines.  Delta Lake table constraints enforce schema-level checks such as NOT NULL and CHECK conditions with regex or ranges to block invalid writes at the table boundary, complementing pipeline-level expectations.  Auto Loader with rescued data column handles evolving SCADA telemetry payloads by capturing unexpected fields, type mismatches, and schema variations in a dedicated column. This prevents pipeline failures when telemetry formats change.  Unity Catalog lineage and system tables provide end-to-end governance and audit trails. Quality metrics from expectations are logged to pipeline event logs and queryable via system tables for monitoring and root-cause analysis.  Real-time streaming ingestion from Azure Event Hubs or AWS Kinesis supports exactly-once semantics and schema evolution for continuous SCADA data flows.  This approach allows AGL to codify energy market-specific validation logic whilst maintaining flexibility as NEM rules and telemetry standards evolve.  	References: Clean and validate data with batch or stream processing Data quality monitoring Best practices for reliability	Full	Yes
RES.012	Data Quality & Profiling	How do you ensure data quality for interval metering and settlement processes?	Databricks ensures interval metering and settlement data quality through four integrated layers supporting AGL's NEM obligations and billing accuracy.  Delta Lake enforced constraints block invalid interval data at write time using NOT NULL and CHECK constraints for value ranges, patterns, and business rules. ACID transactions guarantee consistent settlement datasets across concurrent operations, with time travel enabling rollback when validation errors occur.  Lakeflow SDP expectations apply declarative quality rules in streaming and batch flows with configurable actions (warn, drop, fail) on violations. Expectation metrics log to pipeline event logs for SLA monitoring, whilst quarantine patterns isolate invalid records for audit and remediation before settlement runs.  Lakehouse Monitoring tracks freshness and completeness across metering tables with anomaly detection and statistical profiling. Auto-generated dashboards visualise quality trends and drift, with alerts configured on system.dataqualitymonitoring.table_results for proactive notification.  Unity Catalog provides fine-grained access control, end-to-end column-level lineage tracing metering data through settlement transformations, and audit logs with 365-day retention for AEMO compliance and regulatory traceability.  	References: Delta Lake constraints Lakeflow expectations Data quality monitoring AWS Data quality monitoring Azure Data profiling Lakehouse reliability Best practices for data governance	Full	Yes
RES.013	Data Privacy	Does your platform comply with GDPR, CCPA, and other major privacy regulations?	Databricks provides a general data processing service that can be used by customers in a variety of ways. Your choice of processing activities, including what data you collect and run through the Platform Services, determines which regulations are applicable to you. We can provide information on our products and services along with our security controls for you to evaluate whether your use of the Databricks Services will allow you to comply with your specific compliance obligations, but are unable to contractually take on your specific compliance obligations.  Importantly, Databricks itself is compliant with key privacy regulations such as the GDPR, CCPA, Swiss Data Protection Act, and UK Data Protection Laws, which are included in our default DPA language (https://databricks.com/dpa). Additionally, due to the way the Databricks platform is architected, customers retain control of most of their Customer Data within their own cloud service provider storage buckets and generally store little Customer Personal Data within the Databricks services themselves.  	References: Data governance with Databricks Find Sensitive Data at Scale with Data Classification in Unity Catalog Infosec Registered Assessors Program (IRAP) Canada Protected B	Full	Yes
RES.014	Data Privacy	How do you enforce privacy policies dynamically across distributed data environments?	Databricks enforces dynamic privacy policies across distributed environments through Unity Catalog's query-time evaluation of fine-grained access controls. Policies apply consistently regardless of data location or platform.  Unity Catalog evaluates row filters and column masks at query runtime using SQL user-defined functions against Microsoft Entra ID identities. Policy expressions check account-level group membership, eliminating workspace-specific configuration and ensuring identity-aware access control scales across your entire data estate.  Attribute-based access control (Public Preview) enables centralised policy definition through governed tags applied at catalog, schema, or table scope. Policies inherit downward automatically, reducing administrative overhead while maintaining granular control.  Lakehouse Federation extends Unity Catalog governance to external platforms including MySQL, PostgreSQL, Snowflake, Azure SQL, and AWS Redshift. Row and column security, lineage tracking, and audit logging apply to federated data without data movement, supporting distributed architectures while maintaining unified governance.  Storage-layer enforcement uses Azure managed identities and network isolation through VNet injection and Private Link. Data access occurs only through Unity Catalog securables, preventing policy bypass via direct storage access and maintaining security boundaries across multi-cloud environments.  	References: Access control in Unity Catalog Data governance with Databricks Completing the Lakehouse Vision - Unified Governance	Full	Yes
RES.015	Data Privacy	What capabilities exist for automated detection of sensitive data in unstructured sources?	Databricks provides three integrated capabilities for automated sensitive data detection across unstructured sources.  Unity Catalog Data Classification automatically scans catalogd data assets and applies PII tags such as email address and phone number. Results surface in system tables, and attribute-based access control policies then mask or filter columns based on these tags, enforcing least-privilege access.  Microsoft Purview integration extends detection to unstructured files in ADLS Gen2 containers including CSV, JSON, DOC, and PDF formats. Purview scans apply built-in sensitive information types and sensitivity labels whilst ingesting Unity Catalog metadata for unified discovery and lineage across the platform.  Agent Bricks Guardrails provide real-time PII detection and safety filtering on model serving and AI agent endpoints, preventing leakage of sensitive text in prompts or responses.  For unstructured data ingestion, Lakeflow Connect brings semi-structured and unstructured data into Delta Lake tables where Unity Catalog classification and ABAC policies apply automatically.  Security is enforced through Entra ID managed identities, Private Link connectivity, and serverless network-controlled compute to mitigate exfiltration risks. All detections and access events are auditable through Unity Catalog system tables.  	References: Data Classification | Databricks on AWS Find Sensitive Data at Scale with Data Classification in Unity Catalog Build an unstructured data pipeline for RAG | Databricks on AWS Data and AI governance for the data lakehouse | Databricks on AWS	Full	Yes
RES.016	Data Privacy	How do you support privacy-by-design in AI/ML workflows?	Databricks embeds privacy-by-design through Unity Catalog's hierarchical access controls, which enforce governance at catalog, schema, and table levels with row filters, column masks, and attribute-based policies. Data lineage tracks privacy-sensitive fields end-to-end across Delta Lake and Iceberg tables, supporting compliance reporting for AGL's customer and operational data across Retail, Markets, and Corporate domains.  Identity management integrates Microsoft Entra ID for SSO and least-privilege access, with credentials secured in Azure Key Vault. Private networking via VNet injection and Azure Private Link eliminates public exposure. Serverless workloads use Network Connectivity Configurations to privately access ADLS and 60+ Azure services including Azure OpenAI.  For AI/ML workflows, MLflow models registered in Unity Catalog inherit governance policies automatically. Agent Bricks Gateway enforces safety filters and PII detection, with payload logging to auditable Unity Catalog tables. System tables capture fine-grained usage, permissions changes, and endpoint activity for compliance reporting aligned with TCFD and GRI frameworks.  Storage firewalls and end-to-end Private Link prevent data exfiltration while enabling governed AI innovation across AGL's energy hubs and customer platforms.  	References: Data and AI governance for the data lakehouse Announcing Advanced Security and Governance in Agent Bricks Gateway Privacy-centric collaboration on AI with Databricks Clean Rooms Best practices for data and AI governance	Full	Yes
RES.017	Data Privacy	How do you enforce compliance with Australian Privacy Principles (APPs) for customer energy data?	Databricks will give you the platform and the mechanisms to enforce Australian Privacy Principles compliance through Unity Catalogue's centralised governance framework, mapping directly to APP requirements across AGL's customer energy data estate, but we do not enforce APPs.  For APP 6, 10 and 11 (use, quality and security), Unity Catalog provides fine-grained table, row and column-level policies with attribute-based access controls. These policies define once and secure everywhere, ensuring permissible use of customer energy data and evidencing quality controls.  Identity controls integrate with Microsoft Entra ID for single sign-on and least-privilege access (APP 11). Private networking via Azure Private Link keeps workspace traffic on the Microsoft backbone, ensuring customer data remains within Australian regions (APP 8, 11). Data Exfiltration Protection restricts egress to approved endpoints only.  System tables capture all audit events streaming to Azure Monitor, providing evidence of policy enforcement and supporting transparency obligations (APP 1, 11). Delta Lake's governed retention and VACUUM capabilities permanently remove data files post-deletion, supporting defensible purge requirements (APP 10, 11). Row filters and masks implement anonymity requirements in governed views, with lineage supporting access and correction workflows.  	References: Infosec Registered Assessors Program (IRAP) Prepare your data for GDPR compliance Data Classification Find Sensitive Data at Scale with Data Classification in Unity Catalog	Full	Yes
RES.018	Data Privacy	Do you provide accelerators for secure handling of Personally Identifiable Information in Australian Utilities Industry?	Yes. Databricks provides Automatic Data Classification in Unity Catalog which can help accelerate secure PII handling in Australian utilities.  This capability uses agentic AI to automatically discover, classify and tag PII across your lakehouse, then enforces policy-based protection through attribute-based access controls. Classification runs incrementally as data changes, triggering governance workflows and remediation where needed. The outputs integrate directly with Unity Catalog's row filters and column masks to enforce least-privilege access at scale.  For AGL's Azure environment, we integrate natively with Microsoft Entra ID for SSO and MFA, use managed identities via the Databricks Access Connector for secure ADLS Gen2 access, and deliver audit logs to Azure Monitor for compliance evidence. End-to-end column lineage supports impact analysis for privacy assessments.  This approach directly supports AGL's Consumer Data Right obligations by reducing manual PII identification effort and accelerating compliant data sharing across business domains, while maintaining alignment with your existing Microsoft Cloud security standards.  	References: Find Sensitive Data at Scale with Data Classification in Unity Catalog Infosec Registered Assessors Program (IRAP) Data governance with Databricks Prepare your data for GDPR compliance	Full	Yes
RES.019	Identity & Access Management	Does your solution integrate with enterprise identity providers (Azure AD, Okta)?	Yes. Databricks integrates natively with Microsoft Entra ID (Azure AD) and Okta for enterprise single sign-on and identity management.  Integration capabilities include:  1. SSO protocols: OIDC and SAML 2.0 support for seamless authentication 2. Automated provisioning: SCIM 2.0 for user and group synchronisation at the account level 3. Advanced Entra ID features: Automatic identity management including just-in-time provisioning and nested group support, reducing operational overhead  These integrations operate at the Databricks account level, enabling centralised identity governance across all workspaces. For AGL, this means your existing identity infrastructure extends directly into the Data Intelligence Platform, maintaining consistent access controls and audit trails across your data and AI estate without requiring separate identity management workflows.  	References: Identity best practices - Azure Databricks Automatic Identity Management for Entra ID is now Generally Available in Azure Databricks Identity best practices - Databricks on AWS	Full	Yes
RES.020	Identity & Access Management	How do you implement fine-grained, context-aware access control for large-scale data platforms?	Databricks implements fine-grained, context-aware access control through Unity Catalog's hierarchical governance model, integrated with Azure-native identity and networking controls.  Unity Catalog enforces privileges at catalog, schema, table, and column levels, with dynamic row filters and column masks applied at query time. Attribute-based access control (ABAC) policies centralise filter logic across multiple catalogs, reducing administrative overhead while maintaining granular control over sensitive datasets.  Context-aware enforcement combines Microsoft Entra ID single sign-on with Conditional Access policies that evaluate device posture, network location, and user risk before granting workspace access. IP access lists and context-based ingress policies provide network-layer controls. Private Link connectivity (front-end and back-end) ensures that control-plane and data-plane traffic remains on the Microsoft backbone, eliminating exposure to the public internet.  Storage access to ADLS Gen2 uses managed identities via Databricks Access Connectors, with storage credentials and external locations defined in Unity Catalog. This approach supports storage firewalls and private endpoints while maintaining governed access patterns.  Auditing through Unity Catalog system tables (system.access.audit) and Azure diagnostic logs captures fine-grained usage, permission changes, and data lineage at the table and column level.  For AGL, this architecture supports operational separation across energy trading, customer, and network data domains while meeting compliance requirements for high-value telemetry and market data. The approach aligns with AGL's need for transparent, auditable access controls as the organisation scales its data and AI capabilities.  	References: Access control in Unity Catalog How to scale data governance with Attribute-Based Access Control in Unity Catalog Identity best practices Security, compliance, and privacy for the data lakehouse	Full	Yes
RES.021	Identity & Access Management	What is your approach for managing privileged access in multi-tenant environments?	Databricks manages privileged access in multi-tenant environments through Unity Catalog, enforcing least-privilege access and strong tenant isolation across AGL's AWS and Azure deployments. With Microsoft Entra ID serving as the identity bridge connecting AGL's enterprise directory to the Databricks Data Intelligence Platform across AWS and Azure deployments.  Unity Catalog provides hierarchical privileges at catalog, schema, table, view, and model levels with explicit ownership controls. Workspace binding restricts which workspaces can access specific catalogs and credentials, enabling business-unit separation. Workspace admins receive no automatic Unity Catalog privileges; account and metastore admin roles require explicit assignment and are fully audit-logged.  Identity controls integrate Microsoft Entra ID for SSO, MFA, and SCIM-based user and group synchronisation. Azure managed identities through the Access Connector govern storage access to external locations, combined with storage firewalls and private endpoints. Secrets management uses Azure Key Vault-backed scopes with read-only access and scope-level ACLs.  Network isolation leverages Azure Private Link and serverless Network Connectivity Configurations, with IP access lists for conditional ingress control. All privileged activity is captured in Unity Catalog system tables and Azure logs. Token management policies limit personal access tokens in favour of short-lived OAuth and Entra tokens for administrative access.  	References: Identity best practices Sync users and groups from your identity provider using SCIM Security best practices for the Databricks Data Intelligence Platform Introducing next-level identity security at Databricks	Full	Yes
RES.022	Identity & Access Management	How do you integrate zero-trust principles into your IAM architecture?	Databricks implements zero-trust IAM through identity verification, least-privilege enforcement, network isolation, and continuous monitoring.  Identity verification: Microsoft Entra ID integration enforces SSO, MFA, and Conditional Access for every session, validating identity, device compliance, and sign-in risk. SCIM automates just-in-time user lifecycle management.  Least-privilege access: Unity Catalog centralises RBAC at catalog, schema, and table levels with row filters and column masks applied at query time. Attribute-based access control uses governed tags to dynamically enforce least-privilege without per-object configuration.  Network isolation: Azure Private Link eliminates public internet exposure for all connections. Secure Cluster Connectivity ensures clusters connect via private relay only.  Token management: OAuth 2.0 (user-to-machine and machine-to-machine) with token management APIs enforce rotation policies and eliminate long-lived credentials.  Assume breach: System Tables capture all authentication, authorisation, and data access events with end-to-end lineage for real-time threat detection.  Encryption: Data is encrypted in transit (TLS 1.2+) and at rest (AES-256), with optional customer-managed keys via Azure Key Vault.  	References: Identity best practices Configure authorization in a Databricks app Best practices for security, compliance, and privacy Introducing next-level identity security at Databricks Security best practices for the Databricks Data Intelligence Platform	Full	Yes
RES.023	Identity & Access Management	What innovations do you offer for adaptive authentication based on risk scoring?	Databricks leverages Microsoft Entra ID Protection for adaptive authentication, continuously scoring risk against billions of global threat signals. Every sign-in is evaluated in real time for compromised credentials, impossible travel patterns, malicious IP addresses, and anomalous user behaviour.  Based on calculated risk levels, Entra ID Protection dynamically enforces appropriate controls: step-up MFA challenges, mandatory password resets, or access blocks. These policies integrate seamlessly with Unity Catalog's fine-grained access controls, ensuring authenticated sessions respect data governance rules throughout.  For AGL's multi-cloud environment spanning AWS and Azure, this approach delivers consistent risk-based authentication across both platforms without a separate identity infrastructure. Risk policies can be tuned to AGL's specific threat profile and compliance requirements, with full audit trails captured for security operations and regulatory reporting.  	References: Identity best practices Authentication and access control Introducing next-level identity security at Databricks Automatic Identity Management for Entra ID is now Generally Available in Azure Databricks	Partial	Yes
RES.024	MLOps	Does your platform provide CI/CD pipelines for ML model deployment and monitoring?	Yes. Databricks provides complete CI/CD pipelines for ML model deployment and monitoring, unified under Unity Catalog governance across your AWS and Azure environments.  MLflow Model Registry enables automated model versioning and promotion across development, staging, and production using Champion/Challenger aliases. Webhook-triggered pipelines integrate with Azure DevOps and GitHub Actions on model registration or stage transitions.  Databricks Asset Bundles define ML pipelines, training jobs, serving endpoints, and monitoring dashboards as declarative YAML configurations. These bundles deploy atomically across environments with one-click rollback, ensuring consistency and reducing deployment risk.  The deploy-code pattern promotes training code through environments rather than artifacts, ensuring production models use production data and code. Unity Catalog tracks complete lineage from source tables through feature pipelines, training data, registered models, serving endpoints, and inference tables, supporting AGL's regulatory compliance and audit requirements. This is unique to Databricks: owning lineage end-to-end for both Data and AI.  Model Serving supports zero-downtime deployment to auto-scaling REST endpoints with A/B testing and traffic splitting. Inference Tables automatically log request and response payloads to Delta tables for debugging, retraining, and compliance audits. Approval gates and test coverage thresholds integrate with your Azure DevOps workflows.  This unified approach supports AGL's Business Intelligence value driver by accelerating model deployment while maintaining governance and auditability.  	References: Best practices and recommended CI/CD workflows on Databricks MLOps Stacks: model development process as code MLOps workflows on Databricks MLOps Best Practices - MLOps Gym: Crawl	Full	Yes
RES.025	MLOps	How do you ensure reproducibility and governance of ML pipelines across environments?	Databricks ensures ML pipeline reproducibility and governance through Unity Catalog's unified lineage framework, tracking complete relationships from source Delta tables through feature engineering, model training, and production deployments in a single auditable system.  Delta Lake time travel provides point-in-time reproducibility by versioning training datasets alongside models. You can recreate exact training conditions from any historical state and audit which data produced specific predictions, critical for AEMO reporting and energy trading compliance at AGL.  Managed MLflow captures parameters, metrics, code versions, and dataset references for every experiment run. The three-level namespace isolates development, staging, and production environments while maintaining cross-environment visibility and complete audit trails. Automated lineage links registered models back to source tables and feature transformations, enabling end-to-end impact analysis when upstream data changes.  For AGL's energy forecasting and trading models supporting a lower-emissions future, this integrated approach means regulatory auditors can trace any prediction through the entire chain of model versions, training data snapshots, and feature engineering logic within a single, governed platform, rather than stitching together separate data and ML tooling.  	References: MLOps Stacks: model development process as code Best practices for operational excellence MLOps Best Practices - MLOps Gym: Crawl	Full	Yes
RES.026	MLOps	Describe your automated model drift detection and retraining strategy.	Databricks provides automated model drift detection and retraining through Lakehouse Monitoring, MLflow, and Unity Catalog working as an integrated MLOps platform.  Lakehouse Monitoring continuously tracks statistical properties of inference tables, detecting data drift using KS-test, PSI, Chi-square, and other statistical methods. It computes drift scores comparing current distributions against training baselines and generates configurable alerts when thresholds are exceeded.  Automated retraining workflows are triggered via SQL alerts monitoring drift metrics. When significant distribution changes or performance degradation are detected, Lakeflow Jobs execute the training pipeline using fresh production data. This supports both scheduled periodic retraining and event-driven retraining based on drift thresholds.  MLflow Model Registry integrated with Unity Catalog maintains full model lineage and supports Champion/Challenger comparison, where retrained models are validated against production models before promotion. Inference Tables automatically log all Model Serving requests and responses to Delta Lake, feeding continuous drift analysis.  For AGL, this is critical for energy forecasting and trading models sensitive to seasonal patterns, weather changes, market dynamics, and evolving customer behaviour. Automated drift detection ensures demand forecasting accuracy and trading optimisation adapt to changing renewable generation patterns without manual intervention.  	References: Model deployment patterns CI/CD for machine learning Best practices for operational excellence	Full	Yes
RES.027	MLOps	How do you integrate explainability and fairness checks into production workflows?	Databricks embeds explainability and fairness checks directly into production ML workflows through three integrated layers.  During development, MLflow automatically logs explainability artifacts, including SHAP values and feature importance, alongside model versions. Unity Catalog packages these artifacts with models, ensuring reproducible explanations across environments.  Pre-deployment validation gates enforce fairness checks using Fairlearn integration. Models must pass bias assessments before promotion to production via Unity Catalog aliases, preventing biased models from reaching customers.  In production, Lakehouse Monitoring automatically calculates fairness metrics for classification models, including predictive parity, equal opportunity, and statistical parity across protected groups. Inference tables capture all serving requests in Delta format, enabling continuous bias analysis through SQL dashboards. When fairness metrics exceed defined thresholds, automated retraining workflows trigger with bias mitigation techniques built into Lakeflow Jobs.  Unity Catalog lineage traces every model to exact training data and code versions, providing audit trails for Australian Privacy Principles compliance and regulatory reporting.  For AGL, this integrated approach supports responsible AI deployment across customer segmentation and demand forecasting applications. It maintains customer trust while meeting regulatory obligations across Customer Markets and Energy Markets platforms.  	References: MLOps workflows on Databricks Best practices for operational excellence on Azure Databricks Enable measurement: Supporting infrastructure MLOps Best Practices - MLOps Gym: Crawl	Full	Yes
RES.028	MLOps	What differentiates your approach to scaling MLOps for generative AI models?	Databricks scales GenAI MLOps through unified governance and continuous improvement loops connecting AI systems directly to enterprise data. For AGL, this delivers faster time-to-value for customer-facing AI whilst maintaining rigorous controls for regulated energy operations.  Our differentiation centres on three capabilities. MLflow 3.0 provides GenAI-specific tracing, LLM judge evaluations with human review, and deployment guardrails through Inference Tables and AI Gateway. Daily quality tracking enables rapid iteration on customer experience improvements.  Agent Bricks offers no-code agent building with reinforcement learning from subject matter expert feedback. Our research team maintains latest techniques and selects optimal models for your use cases.  Model Serving delivers unified APIs across custom, foundation and external models with serverless auto-scaling to 25,000+ queries per second and sub-50ms latency. The Agent Framework enables production-quality retrieval-augmented generation with single-line deployment and streaming responses.  Agent Framework & Evaluation enables production-quality RAG with one-line deployment, streaming responses, SME review app, and Inference Tables for unified monitoring.   Lakebase (managed Postgres) integrates OLTP directly with the Lakehouse: CDC sync to Delta, real‑time Feature Serving, ACID app state, and Unity Catalog governance for transactional tables.   MCP (Model Context Protocol) enables governed tool calling at scale, featuring managed servers for UC Functions, Vector Search, Genie, and DBSQL, as well as custom servers via Databricks Apps with on-behalf-of-user authentication. The MCP ability even extends into the Databricks Marketplace.  Unity Catalog provides unified access controls and lineage across all AI assets, with encryption, audit trails and rate-limited traffic supporting AGL's security and compliance requirements. Databricks Lakehouse integrates transactional workloads through change data capture sync to Delta, real-time feature serving and governed tool calling at scale.  	References: AI and machine learning on Databricks MLflow for ML model lifecycle Unlocking the Potential of AI Agents: From Pilots to Production Success Batch Inference on Fine Tuned Llama Models with Agent Bricks Model Serving	Full	Yes
RES.029	MLOps	Do you provide pre-built ML pipelines for energy load forecasting and wholesale price prediction?	Yes. Databricks provides AutoML forecasting and Solution Accelerators purpose-built for energy load and price prediction, deployable on both AWS and Azure.  AutoML forecasting automatically selects optimal algorithms, including Prophet, Auto-ARIMA and DeepAR, on serverless compute. It generates production-ready notebooks, registers models to Unity Catalog for governance, and provides batch inference and real-time serving options. Solution Accelerators deliver ready-to-use blueprints that compress proof-of-concept timelines to weeks.  Energy-specific partner frameworks such as Celebal's PUFF offer pre-built pipelines covering load, generation, price and weather forecasting, deployable directly on the platform. All pipelines benefit from Unity Catalog governance, ensuring controlled access to data and models across your organisation.  Configuration adapts to your data sources, including AMI smart meter data, weather feeds and NEM pricing curves, with flexible forecast horizons and granularity to match your trading requirements.  For AGL, accurate load and price forecasts are critical for optimising day-ahead offers and hedging strategies in the volatile NEM environment. These accelerators reduce build time while providing governed, repeatable pipelines that support your renewable generation growth and firming capacity strategy under CTAP.  	References: Forecasting with AutoML (classic compute) A Framework for Multi-Model Forecasting on Databricks MLOps workflows on Databricks Unlock the Predictive Power of Your Time Series Data	Full	Yes
RES.030	MLOps	How do you support predictive maintenance for generation assets using IoT data?	Databricks enables predictive maintenance for AGL's generation and battery assets by unifying operational technology and IT data streams into a governed lakehouse architecture, reducing unplanned downtime and optimising maintenance schedules across your portfolio.  For AGL's SCADA environment, we support direct integration from Ignition and HiveMQ through Zerobus Ingest patterns, forwarding RabbitMQ messages into Delta Lake tables. Alternatively, high-fidelity historian data from AVEVA CONNECT (OSI PI) flows natively via Delta Sharing with 5-minute latency, bringing asset hierarchy metadata from PI AF directly into the lakehouse for health modelling.  Telemetry streams are persisted in Delta Lake with ACID guarantees and time travel across medallion layers, enabling both real-time anomaly detection and historical backtesting. Unity Catalog provides centralised governance, auditing, and lineage across sensor data, feature stores, and ML models to ensure traceable maintenance decisions and regulatory compliance.  ML models for failure prediction and demand forecasting deploy via Agent Bricks Model Serving for low-latency scoring. Lakeflow Spark Declarative Pipelines orchestrate end-to-end workflows from ingestion through model training to insights delivery.  Maintenance teams access predictions through AIBI Dashboards (or your BI tooling of choice) via Databricks SQL Warehouse or custom Databricks Apps for field diagnostics, supporting operational excellence across AGL's diverse generation assets.  	References: MLOps workflows on Databricks Best practices for operational excellence Distributed ML for IoT Evolution of Utilities: The Rise Of The Data Intelligent Utility	Full	Yes
RES.031	Data Orchestration	Does your solution support event-driven orchestration across heterogeneous data sources?	Yes, Databricks supports event-driven orchestration across heterogeneous data sources through Lakeflow Jobs with multiple trigger mechanisms.  Lakeflow Jobs provides managed orchestration with file arrival monitoring for cloud storage (ADLS Gen2, S3), table update triggers that execute Lakeflow Jobs when Delta tables change, and API-triggered Lakeflow Jobs for external system integration. File arrival triggers check for new data approximately every minute, while table update triggers respond immediately to changes, enabling real-time pipeline execution.  For heterogeneous source integration, Databricks natively connects to Azure Event Hubs for real-time event streams, supports Lakeflow Connect for SaaS applications (Salesforce, Workday, ServiceNow) and databases (SQL Server, PostgreSQL), and uses Auto Loader for scalable event-based ingestion from cloud storage. This enables AGL to orchestrate pipelines across Azure services, on-premises systems, and third-party applications supporting critical energy operations.  All Lakeflow Jobs are governed by Unity Catalog for unified access control, lineage, and auditing, with comprehensive monitoring via system tables and integration with Azure Monitor. This ensures AGL maintains governance and transparency across event-driven data flows, supporting your transition to a lower-emissions future.  	References: Announcing the General Availability of Databricks Lakeflow What is Lakeflow Connect? Ingest data from SQL Server, Salesforce, and Workday with Lakeflow Connect Lakehouse reference architectures	Full	Yes
RES.032	Data Orchestration	How do you guarantee fault-tolerant orchestration for mission-critical workloads?	Databricks guarantees fault-tolerant orchestration through three core capabilities critical for AGL's energy trading, customer platforms, and grid operations.  Lakeflow Jobs delivers configurable retry policies, escalating retry logic, and automatic failure recovery. You define retry thresholds and escalation paths based on workload criticality, ensuring mission-critical pipelines recover automatically from transient failures.  For streaming workloads powering real-time energy management, Structured Streaming provides end-to-end fault tolerance through checkpointing stored in Azure Data Lake Storage Gen2. Pipelines resume from the last successful state after failures, maintaining continuity for time-sensitive operations such as market bidding and grid balancing.  Lakeflow Spark Declarative Pipelines automates dependency management and enforces exactly-once processing semantics. This prevents data corruption from late-arriving events through automatic buffering and logical ordering, ensuring integrity across billing, trading, and operational analytics.  All fault-tolerance parameters are customer-configurable. You control checkpoint intervals, disaster recovery topology (active-passive or active-active across Azure regions), and integration with Azure Monitor for alerting. This flexibility lets you align RTO and RPO targets with each system's business-criticality, from sub-minute recovery for trading systems to longer recovery windows for analytical workloads.  	References: Reliability for the data lakehouse Best practices for reliability Data engineering with Databricks Announcing the General Availability of Databricks Lakeflow Disaster recovery	Full	Yes
RES.033	Data Orchestration	What is your approach for dynamic resource allocation during peak orchestration loads?	Databricks provides three dynamic resource allocation approaches for peak orchestration loads, optimising both cost and performance without manual intervention.  Serverless compute for Lakeflow Jobs and Lakeflow Spark Declarative Pipelines provisions resources within seconds and scales elastically from zero to thousands of nodes. Resources spin up on demand and terminate immediately after job completion.  Enhanced autoscaling in Lakeflow SDP monitors task slot utilisation and queue depth to dynamically adjust compute capacity during pipeline execution. This balances speed and cost for both batch and streaming workloads, scaling up during peaks and down during quiet periods.  Classic clusters support configurable autoscaling from 2 to 2000+ nodes with intelligent workload management. SQL warehouses use Intelligent Workload Management to queue low-priority queries and provision additional clusters during concurrency spikes.  All options use pay-as-you-go billing based on actual work performed rather than idle capacity. Scaling behaviour and cost controls are fully configurable based on your workload profiles and budget policies.  	References: Best practices for cost optimization - Azure Databricks Best practices for operational excellence - Databricks on AWS AI ETL: How Artificial Intelligence Automates Data Pipelines Migrating from Redshift to Databricks: A Field Guide for Data Teams Architecting a High-Concurrency, Low-Latency Data Warehouse on Databricks That Scales	Full	Yes
RES.034	Data Orchestration	How do you enable orchestration for cross-domain workflows (data + ML + LLM) seamlessly?	Databricks Lakeflow Jobs provides native orchestration across data engineering, ML, and LLM workloads through a unified control plane with 99.95% uptime SLA (on Azure). The platform supports diverse task types within a single DAG: Lakeflow Spark Declarative Pipelines for ETL, MLflow for model training and deployment, Agent Bricks for LLM fine-tuning and evaluation, DBSQL for analytics, and AI/BI tasks for semantic model publishing to Power BI. This eliminates the need for separate orchestrators across domains.  For AGL's energy operations, this enables end-to-end workflows, such as orchestrating smart meter data ingestion from 4 million Kaluza customers, triggering retraining of the demand forecasting model when accuracy thresholds are breached, and generating regulatory compliance reports using LLMs. All tasks share Unity Catalog governance, providing consistent audit trails and lineage across Customer Markets, Energy Markets, and Corporate platforms.  The platform supports conditional execution (if/else, for-each), deep monitoring with task-level metrics, and CI/CD integration with Azure DevOps or GitHub Actions. Compute can be configured as serverless for automatic scaling or classic clusters for cost optimisation. Specific orchestration patterns, task dependencies, and deployment strategies are customer configuration-dependent based on workload requirements.  	References: Announcing the General Availability of Databricks Lakeflow The scope of the lakehouse platform LLMOps workflows on Azure Databricks January 2022 Databricks on Google Cloud Release Notes What is Databricks?	Full	Yes
RES.035	Data Orchestration	What capabilities do you have for near-real-time orchestration capabilities & digital-twins?	Databricks provides three integrated capabilities for near-real-time orchestration and digital twins.  Structured Streaming with Real-Time Mode delivers 40-300ms p99 latency for event processing. Lakeflow Spark Declarative Pipelines enable event-driven data flows with stateful stream processing for de-duplication, anomaly detection, and smoothing. The Digital Twin Solution Accelerator combines Zerobus Ingest for direct telemetry ingestion, RDF-based twin modelling, and Lakebase (managed Postgres) for low-latency state serving.  The architecture ingests IoT and SCADA data from Azure Event Hubs, Kafka, or REST APIs into governed Delta Lake tables, applies transformations via Lakeflow Spark Declarative Pipelines, and syncs processed data to Lakebase for real-time dashboard queries while maintaining historical records for analytics. Databricks Apps provides the visualisation layer.  Lakeflow Jobs orchestrates end-to-end pipelines with 99.95% uptime (on Azure), file-arrival triggers, and table-update triggers (Public Preview) for event-driven execution. Unity Catalog provides unified governance, lineage, and auditing across the digital twin lifecycle.  Find attached a link to the new digital twin accelerator which demonstrates this pattern in production, using Zerobus to stream data through Lakeflow Spark Declarative Pipelines, then into serverless OLTP with real-time dashboards.  	References: July 2025 Release Notes - Real-time mode for Structured Streaming Real-time mode in Structured Streaming documentation How to Build Digital Twins for Operational Efficiency Announcing the General Availability of Databricks Lakeflow	Full	Yes
RES.036	Data Orchestration	How do you enable federated queries across OT systems (SCADA, historian databases) and IT systems for grid operations?	Databricks enables federated queries across AGL's OT and IT systems through three complementary patterns that support real-time grid operations while maintaining unified governance.  Lakehouse Federation provides read-only federated queries to operational databases including SQL Server, Oracle, PostgreSQL, and MySQL. Unity Catalog connections push queries down to source systems via JDBC, avoiding data duplication while maintaining unified governance and lineage across OT and IT sources. This supports querying operational data in place without moving it.  For historian data, AVEVA CONNECT integration delivers native access to OSI PI data through Delta Sharing with five-minute refresh intervals (AVEVA Connect limitation). This supports heterogeneous OT environments including IP21, PHD, and custom historians via AVEVA Adapters, preserving asset hierarchy metadata from PI AF for contextual analysis of AGL's generation assets and energy hubs.  SCADA telemetry ingestion uses Lakeflow Connect and Structured Streaming to process data from cloud storage and message buses such as Kafka and Event Hubs into Delta Lake tables with ACID guarantees and schema evolution. This enables real-time processing of grid telemetry for monitoring renewable generation and firming capacity.  Unity Catalog provides centralised governance across all federated sources with fine-grained access control, data lineage tracking, and auditing. This ensures consistent security policies across AGL's grid operations data, whether queried in place or ingested for real-time processing.  	References: Announcing General Availability of Lakehouse Federation What is Lakehouse Federation? What is query federation? Connect to data sources and external services	Full	Yes
RES.037	Data Lineage	Describe your approach for end-to-end lineage tracking across datasets, transformations, and ML models.	Databricks delivers comprehensive end-to-end lineage tracking through Unity Catalog, which automatically captures runtime data lineage across all datasets, transformations, and ML models without additional configuration.  Unity Catalog tracks lineage at both table and column levels across all languages (SQL, Python, R, Scala). It captures relationships between data assets and the notebooks, Lakeflow Jobs, and dashboards that created or consume them. This automated tracking supports AGL's regulatory compliance requirements and operational transparency across energy trading, customer analytics, and renewable asset management.  MLflow integration with Unity Catalog automatically tracks ML model lineage back to source datasets using mlflow.loginput. This records training data, feature transformations, and model outputs with full transparency for responsible AI practices.  Lineage is visualised in near real-time through Catalog Explorer and accessible programmatically via REST API and system tables (system.access.tablelineage and system.access.column_lineage) for impact analysis and compliance reporting. Lakeflow Spark Declarative Pipelines enhances lineage with automated pipeline visualisation, enabling AGL data teams to quickly assess downstream impacts of schema changes or data quality issues.  	References: View data lineage using Unity Catalog Best practices for data and AI governance Responsible AI with the Databricks Data Intelligence Platform Bring your own data lineage	Full	Yes
RES.038	Data Lineage	How do you ensure lineage accuracy during schema changes and data migrations?	Unity Catalog and Delta Lake maintain lineage accuracy throughout schema changes and migrations, critical for AGL's regulatory reporting and energy market operations.  Unity Catalog automatically captures table and column-level lineage in real-time across all queries. As schemas evolve through additions, modifications or type changes, lineage relationships persist without manual intervention. This ensures data provenance remains intact during platform migrations or market data schema updates.  Delta Lake stores schema versions as JSON within its transaction log, creating an immutable history that enables time travel queries and accurate lineage tracking. Schema evolution is supported through mergeSchema options for append operations and MERGE WITH SCHEMA EVOLUTION syntax, automatically adapting to new columns whilst preserving lineage.  Lakeflow Spark Declarative Pipelines extends this with automated schema inference and evolution for JSON and CSV sources, handling schema drift without manual intervention. For CICD-managed migrations, Delta Lake's versioned transaction log provides the consistency layer, whilst Unity Catalog system tables enable programmatic validation of lineage accuracy post-migration.  This architecture ensures AGL maintains complete audit trails for NEM reporting and regulatory compliance throughout data platform evolution.  	References: View data lineage using Unity Catalog Schema evolution in Databricks Lineage system tables reference Delta Lake update schema documentation	Full	Yes
RES.039	Data Lineage	What capabilities exist for lineage visualization and impact analysis?	Unity Catalog provides comprehensive lineage visualisation and impact analysis capabilities included with the platform at no additional licence cost.  The system automatically captures runtime data lineage at table and column levels across all queries, regardless of language. This tracking extends across tables, views, notebooks, jobs, dashboards, and Lakeflow Spark Declarative Pipelines, providing complete visibility into data flows and dependencies across all workspaces sharing a metastore.  Interactive lineage graphs in Catalog Explorer enable visual impact analysis, showing upstream sources and downstream consumers. This allows data teams to understand how changes to data assets affect dependent workloads and perform risk management for schema modifications.  Lineage data is accessible programmatically via REST APIs and Unity Catalog system tables for custom impact analysis, governance reporting, and integration with external tools.  The Bring Your Own Lineage capability allows integration of external lineage metadata from first-mile ETL sources and last-mile BI tools, providing end-to-end lineage visualisation across your entire data estate.  	References: May 2025 Release Notes View data lineage using Unity Catalog Bring your own data lineage Best practices for data and AI governance	Full	Yes
RES.040	Business Glossary & Data Taxonomy	How do you manage hierarchical taxonomies and synonyms for business terms?	Databricks manages hierarchical taxonomies and synonyms through two complementary approaches.  Unity Catalog Metric Views (Public Preview) provides production-grade semantic metadata management. Business terms can include synonyms, display names, format specifications and descriptions directly within metric definitions. This ensures consistent terminology across AI/BI Genie, dashboards and SQL queries. Hierarchical relationships are modelled through dimensions and measures using star or snowflake schemas, allowing flexible querying across business hierarchies with automatic optimisation.  For advanced taxonomy requirements, Ontos (a Databricks Labs community project) extends Unity Catalog with business glossary capabilities, hierarchical ontologies and semantic mapping between business concepts and technical data assets. As a Labs project, Ontos provides additional functionality for organisations requiring sophisticated taxonomy management, though it operates outside standard Databricks support.  Both approaches integrate with Unity Catalog governance, providing lineage tracking, access controls and audit visibility. For AGL, this supports consistent business terminology across energy trading, customer and network operations domains whilst maintaining regulatory compliance and enabling self-service analytics for business users.  The combination allows AGL to define business terms once and apply them consistently across the organisation's data and AI assets.  	References: Unity Catalog Metric Views Semantic Metadata in Metric Views Unity Catalog Business Semantics Ontos Business Glossary Column Synonyms in AI/BI Genie Business Term Mapping Example	Full	Yes
RES.041	Business Glossary & Data Taxonomy	Describe your workflow for glossary term approvals and governance.	AGL's data stewardship teams can govern glossary term approvals through Unity Catalog's ownership controls. Table and object owners manage all metadata modifications, including business glossary terms, column descriptions, and tags.  Only authorised principals with ownership or MODIFY privileges can update metadata, ensuring controlled governance aligned with AGL's data quality standards. A dedicated STEWARDSHIP permission is planned on the product roadmap to enable delegated metadata management without the ability to modify the data itself, allowing data stewards to update glossary terms, classifications, and business metadata without requiring full table ownership.  Access Requests provides self-service approval workflows for glossary terms and datasets. Users discover assets and request permissions, with administrators routing requests to email, Slack, Microsoft Teams, ServiceNow, or Jira. Approvers grant access through the UI or add requesters to authorised groups, maintaining oversight throughout the approval process.  All metadata changes are captured in Unity Catalog system tables, providing complete audit trails for compliance reporting. This traceability supports AGL's governance framework and regulatory requirements across the organisation's data estate. 	References: Unity Catalog ownership and privileges Access request destinations configuration Semantic metadata in metric views Access Requests announcement Databricks SQL release notes 2025 - Semantic metadata feature Best practices for data and AI governance ReaderLink business term mapping case study	Full	Yes
RES.042	Business Glossary & Data Taxonomy	How do you link glossary terms to physical and logical data assets?	Unity Catalog links glossary terms to data assets through three mechanisms, with a fourth in preview.  Table and column comments attach directly to physical assets and are queryable through information_schema views. These support AI-generated documentation and custom glossary terms that persist with the schema, enabling technical teams to maintain precise definitions alongside data structures.  Governed tags provide account-level business classifications with enforced consistency and inheritance. Tags applied at catalog or schema level automatically flow to child objects, supporting Attribute-Based Access Control policies. All tag assignments are captured in audit logs for compliance traceability.  Unity Catalog Metric Views enable semantic metadata including synonyms, display names, and format specifications defined centrally and applied consistently across dashboards, AI/BI, and SQL workloads without altering underlying tables. This separates business terminology from physical schema design.  The Domains feature will add logical grouping by business area such as Sales or Finance, supporting curated discovery experiences aligned with organisational operations rather than technical structure.  These capabilities ensure consistent terminology across AGL's data estate, enable business user discovery, and maintain full audit traceability of metadata changes.  	References: Databricks SQL release notes 2025 What's New in AI/BI - July 2025 Roundup Best practices for data and AI governance Revolutionizing Enterprise Data Analytics at ReaderLink	Full	Yes
RES.043	Data Catalog	Explain your approach for automated data discovery and cataloging across multi-cloud environments.	Unity Catalog provides automated data discovery across AWS, Azure, and GCP through a centralised metadata layer. The system maintains one metastore per region, capturing metadata, lineage, and audit logs automatically as data assets are created or modified.  AGL's existing Data Publishing and Unity Catalog design naturally supports hub-and-spoke discovery where each of the three data platforms (Customer Markets, Energy Markets and Development, Corporate) publishes certified data products to centralized catalogs within their regional Unity Catalog metastore, enabling cross-platform data discovery while maintaining domain ownership, data quality accountability, and access control governance aligned with AGL's FSA objectives.  Discovery operates through multiple interfaces. Catalog Explorer and global workspace search provide privilege-aware filtering, indexing table names, columns, descriptions, and tags. AI-powered semantic search enables business users to query metadata using natural language through Databricks Assistant, whilst technical users leverage metadata APIs for programmatic access.  Cross-cloud data governance (GA) enables direct access to data across cloud providers without migration. AGL teams can access AWS S3 data from Azure-based workspaces, supporting hybrid architectures without data duplication.  Automated lineage tracking maps data flow from source to consumption, surfacing quality signals including certification status and usage patterns. This visibility supports data product discovery across organisational boundaries.  For AGL, this approach directly supports the Business Intelligence value driver by enabling teams to discover relevant data products across the organisation whilst maintaining central governance oversight. Teams gain self-service access to certified data assets without compromising security or compliance requirements, accelerating insights that support the transition to a lower-emissions future.  	References: What is Unity Catalog Discover data Cross-cloud data governance GA announcement Unity Catalog updates at Data + AI Summit 2025 Databricks components and Unity Catalog capabilities Best practices for data and AI governance Best practices for interoperability and usability Building High-Quality and Trusted Data Products	Full	Yes
RES.044	Data Catalog	How do you support federated cataloging and search for distributed data assets?	Yes. Unity Catalog provides federated cataloging and search across distributed data assets through three complementary capabilities.  Lakehouse Federation (GA) enables unified governance and discovery across external databases including MySQL, PostgreSQL, Snowflake, SQL Server, AWS Glue, Hive Metastore, and Azure Synapse without data migration. This directly supports AGL's three-platform architecture spanning Customer Markets, Energy Markets and Development, and Corporate domains. Query federation pushes Unity Catalog queries to external systems via JDBC, while catalog federation accesses foreign tables in object storage with automatic metadata synchronisation and privilege-aware access.  Lakeflow Connect (GA for SQL Server) provides automated managed ingestion with built-in Change Data Capture for incremental replication from Azure SQL Database, AWS RDS SQL Server, and on-premises systems. Ingested tables are automatically catalogd with schema evolution and unified access controls.  Cross-region and cross-cloud data sharing uses Databricks-to-Databricks Delta Sharing to federate metadata across Unity Catalog metastores, creating shared catalogs that appear as native objects with local access control and lineage tracking. This supports AGL's hybrid AWS-Azure architecture including Kaluza billing data and Salesforce CRM integration.  Unified search operates through Catalog Explorer, global workspace search, and information_schema queries with consistent fine-grained access controls regardless of physical data location.  	References: What is Lakehouse Federation? Announcing General Availability of Hive Metastore and AWS Glue Federation in Unity Catalog Run federated queries on Salesforce Data 360 Best practices for data and AI governance	Full	Yes
RES.045	Data Catalog	What differentiates your catalog in terms of AI-driven recommendations?	Unity Catalog differentiates through platform-native AI capabilities requiring no additional licensing. AI/BI Genie enables business users to ask natural language questions and receive intelligent follow-up suggestions based on Unity Catalog metadata, usage patterns, and certified metrics. Databricks Assistant provides context-aware recommendations including /findTables for discovering relevant datasets and automated code optimisation. You also have the ability to prompt and add skills to the Databricks Assistant, enabling agentic   AI-generated documentation automatically enriches catalog metadata with table descriptions and column comments based on schema analysis. Semantic search surfaces relevant assets through intelligent understanding of relationships and business context, not keyword matching.  Critically, all AI-driven recommendations respect Unity Catalog governance automatically, enforcing row-level security, column masks, and access controls while maintaining complete audit trails. This enables AGL's data teams to deliver self-service analytics without compromising compliance or data protection requirements.  	References: October 2023 Release Notes Databricks SQL release notes 2025 AI/BI concepts What's new with Databricks Unity Catalog at Data + AI Summit 2025	Full	Yes
RES.046	Data Classification	How do you implement automated classification of sensitive data (PII, PCI, PHI)?	Databricks implements automated sensitive data classification through Unity Catalog Data Classification, an agentic AI system that scans, detects, and tags PII, PCI, and PHI across your data estate.  The system uses pattern recognition, metadata analysis, and large language models to identify 12+ sensitive data categories including credit cards, Social Security numbers, email addresses, phone numbers, driver licences, passports, bank accounts, and personal names. New tables and columns are scanned within 24 hours of creation.  Unity Catalog lineage enables initial comprehensive scans, then incremental rescanning of only changed data, reducing ongoing classification costs by up to 75 per cent. Classification results are logged to system tables for audit reporting.  catalog owners review high-confidence detections through an interactive UI, then enable automatic tagging to apply governed tags across all existing and future detections. These tags integrate with Attribute-Based Access Control policies to automatically mask or filter sensitive columns based on user permissions, supporting AGL's governed data access requirements across Customer Markets, Energy Markets, and Corporate platforms.  	References: Data Classification in Unity Catalog Find Sensitive Data at Scale with Data Classification Attribute-Based Access Control (ABAC) October 2025 Release Notes	Full	Yes
RES.047	Data Classification	Describe your integration strategy with DLP and compliance tools.	Databricks integrates with enterprise DLP programmes through three complementary layers that support AGL's data governance requirements across customer, operational, and environmental datasets.  Unity Catalog provides the governance foundation. Automated Data Classification (Public Preview) detects PII across tables, Attribute-Based Access Control enables dynamic row filtering and column masking, and comprehensive audit logs stream to SIEM platforms for centralised monitoring. System tables expose audit logs, access patterns, and data lineage as queryable Delta tables, enabling security teams to integrate Databricks into existing DLP workflows.  Network-level controls are configured through customer-managed VNets with Azure Firewall, Private Link endpoints, and Service Endpoint Policies that restrict data egress to authorised destinations. Workspace settings disable DBFS access and results downloads to prevent data leakage.  Microsoft Purview integration enables metadata scanning and classification synchronisation, allowing security teams to apply consistent DLP policies across Azure estates whilst Unity Catalog enforces operational access controls. The Compliance Security Profile supports regulatory frameworks including GDPR, HIPAA, and PCI-DSS, providing auditors expected visibility and enforcement capabilities.  	References: Data governance with Databricks Best practices for data and AI governance AI Architecture: Building Enterprise AI Systems with Governance Guiding principles for the lakehouse	Full	Yes
RES.048	Data Classification	How do you enforce classification policies dynamically during data movement?	Unity Catalog enforces classification policies dynamically at query time through Attribute-Based Access Control (ABAC). When users query any table, Unity Catalog evaluates classification tags and applies access controls and masking policies in real-time, maintaining consistent governance across transformation pipelines.  Data Classification (Public Preview) automatically detects and tags sensitive data in new tables and columns within 24 hours of creation. This continuous identification reduces manual tagging overhead while maintaining policy coverage as data transforms through bronze, silver, and gold layers.  For data movement scenarios, Unity Catalog's column-level lineage enables data stewards to trace sensitive data flows downstream and verify tag coverage systematically. Governed tags provide standardised vocabularies for consistent classification across all data assets, supporting AGL's governance requirements for customer data and operational intelligence.  Currently, classification tags require reapplication when data transforms between layers. However, Unity Catalog's query-time enforcement ensures policies remain active wherever tagged data resides, providing robust protection for sensitive information throughout complex transformation workflows.  This approach supports AGL's data governance needs while maintaining compliance with privacy requirements across the organisation's data platform.  	References: Data Classification on Databricks (AWS) Data Classification on Databricks (GCP) Unity Catalog ABAC on Azure Databricks	Full	Yes
RES.049	Data Lifecycle Management	Explain your policy-driven approach for data retention and archival.	Databricks delivers policy-driven data retention and archival through Delta Lake table properties integrated with Azure storage lifecycle management.  Delta Lake table properties control retention at the table level. The delta.deletedFileRetentionDuration property (default 7 days) determines how long deleted files remain available for time travel queries, supporting audit requirements and operational recovery. The delta.logRetentionDuration property independently controls metadata retention, allowing flexible governance policies.  For archival, Databricks integrates with ADLS Gen 2 lifecycle management to automatically tier data across Hot, Cool, Cold, and Archive storage based on access patterns. Delta Lake archival support synchronises through the delta.timeUntilArchived table property, allowing queries to skip archived files while maintaining metadata access. Queries fail early when archived data restoration is required, preventing unexpected delays.  Unity Catalog's UNDROP command provides 7-day recovery windows for accidentally dropped managed tables, supporting both compliance requirements and operational safety for AGL's data teams.  Predictive Optimisation automates VACUUM operations to remove stale data files based on configured retention settings, ensuring only active data remains in hot tiers while lifecycle policies transition historical data to lower-cost storage.  	References: Table properties reference Archival support in Azure Databricks Azure Storage lifecycle management Work with table history UNDROP TABLE command	Full	Yes
RES.050	Data Lifecycle Management	How do you enforce lifecycle policies across hybrid and multi-cloud environments?	Databricks integrates with native cloud lifecycle management rather than enforcing policies centrally. Each cloud provider manages lifecycle independently: AWS S3 Lifecycle, Azure ADLS Gen2 Lifecycle, and GCS Lifecycle policies configured at storage account or bucket level.  Delta Lake archival support (Public Preview) coordinates with these cloud-native policies through the delta.timeUntilArchived table property. Databricks automatically skips archived files during queries and fails early when archived data is required, ensuring predictable query performance as data transitions between hot, cool, and archive tiers.  Unity Catalog provides unified governance for metadata, access control, and audit logging across multi-cloud deployments while delegating storage lifecycle management to each cloud provider. This approach enables AGL to apply cost optimisation and retention strategies independently within each cloud tenant whilst maintaining consistent data governance and compliance controls across your hybrid infrastructure.  For AGL's multi-cloud strategy, this means configuring lifecycle policies separately within each environment, with Delta tables synchronised to respect those policies through table properties. This balances cost efficiency with operational flexibility as you scale renewable generation data and customer analytics across cloud platforms.  	References: Archive Delta tables - Azure Databricks Archive Delta tables - Databricks on AWS Announcing General Availability of Cross-Cloud Data Governance Azure Storage lifecycle management Interoperability and usability for the data lakehouse	Full	Yes
RES.051	Data Lifecycle Management	Describe your automated purging and audit capabilities for expired data.	Databricks automates data purging through Predictive Optimisation, which schedules VACUUM operations on Unity Catalog managed tables. This removes unreferenced data files including deleted rows, previous versions, and compacted files based on configurable retention periods from 7 to 90 days, aligning with AGL's regulatory requirements whilst reducing storage costs.  Audit capabilities support compliance through two mechanisms. First, the system.storage.predictiveoptimizationoperations_history table records every VACUUM operation with timestamps, bytes deleted, files removed, and compute costs. Second, Unity Catalog audit logs in system.access.audit capture all table deletions, VACUUM operations, and data access patterns with 365-day retention.  These audit trails provide complete forensic visibility into data access and deletion activities, supporting AGL's governance frameworks and regulatory reporting obligations. Predictive Optimisation ensures consistent policy enforcement across AGL's data estate without manual intervention, reducing operational overhead whilst maintaining compliance posture.  	References: Predictive optimization in Databricks Remove unused data files with VACUUM Predictive optimization operations history System tables reference Table properties reference Prepare your data for GDPR compliance Data governance with Databricks	Full	Yes
RES.052	Data Protection	How do you implement encryption at rest and in transit using industry standards?	Databricks implements AES-256 encryption for all data at rest across notebooks, workspace storage, managed disks, and query results. Customer-managed keys from Azure Key Vault provide workspace-level control over three encryption layers: managed services including notebooks and queries, DBFS root storage, and temporary compute storage. This envelope encryption approach gives AGL complete key lifecycle control through Azure Key Vault while maintaining dual-layer protection.  All data in transit uses TLS 1.2 or higher, with TLS 1.3 support available, for communications between users, control plane, compute plane, and Azure storage services. Inter-cluster traffic can optionally use AES-128 over TLS 1.3. Azure VNet encryption provides hardware-level protection for traffic within AGL's virtual networks.  The customer-managed key approach provides flexibility to meet evolving regulatory requirements through controlled key rotation and revocation, ensuring AGL maintains sovereignty over encryption keys while Databricks manages the underlying infrastructure.	References: July 2023 GCP Release Notes - Customer-managed keys October 2023 GCP Release Notes - CMK General Availability Customer-managed keys for encryption on AWS Azure Databricks Customer-Managed Keys	Full	Yes
RES.053	Data Protection	Describe your integration with enterprise key management systems.	Databricks integrates natively with Azure Key Vault, Azure Key Vault Managed HSM, and AWS KMS for customer-managed keys, enabling workspace-level encryption control across notebooks, storage, and compute resources.  Three CMK features are configurable: managed services encryption for notebooks, secrets, and Databricks SQL queries; DBFS root encryption for workspace storage accounts; and managed disk encryption for compute resources.  The integration uses envelope encryption architecture where your keys from Key Vault or KMS encrypt data encryption keys, which are then re-encrypted with Databricks-managed keys. This dual-layer approach ensures you retain complete control through key rotation, access policies, and RBAC permissions.  Configuration is straightforward through Azure Portal, Azure CLI, PowerShell, ARM templates, or AWS CloudFormation. You grant Key Vault Crypto Service Encryption User role to Databricks service principals for automated key operations.  Key lifecycle management supports automated rotation with zero downtime. Databricks detects new key versions and applies them to future encryption operations while maintaining backward compatibility with previously encrypted data.  	References: July 2023 GCP Release Notes - Customer-managed keys October 2023 GCP Release Notes - CMK General Availability Customer-managed keys for encryption on AWS Azure Databricks Customer-Managed Keys	Full	Yes
RES.054	Data Protection	What capabilities exist for field-level encryption for sensitive attributes?	Databricks provides native field-level encryption for protecting sensitive customer attributes through built-in cryptographic functions and Unity Catalog access controls.  The platform includes AES encryption functions (aes_encrypt and aes_decrypt) supporting 128, 192, and 256-bit keys with GCM mode for authenticated encryption. These integrate directly into SQL workflows, enabling encryption of customer identifiers, payment details, or vulnerability indicators at the column level.  Unity Catalog column masks deliver granular access control. Authorised users decrypt data transparently based on permissions, whilst direct key access remains restricted. This enforces least-privilege principles across analytical workloads.  Envelope encryption patterns using Data Encryption Keys (DEK) and Key Encryption Keys (KEK) enable key rotation without rewriting encrypted datasets. Integration with Azure Key Vault provides external key management and enhanced security controls.  Implementation involves configuring crypto functions within Unity Catalog schemas and defining column-level access policies. This approach protects customer data whilst maintaining analytical capabilities, supporting AGL's regulatory compliance requirements and data governance standards for customer information across the platform.  	References: Data security and encryption Row filters and column masks Access control in Unity Catalog	Full	Yes
RES.055	Data Audit & Monitoring	Explain your real-time audit logging architecture for all data operations.	Databricks delivers comprehensive real-time audit logging through Unity Catalog system tables and diagnostic log delivery, supporting AGL's data governance and compliance requirements.  Unity Catalog automatically captures all data operations including table access, metadata queries, permission changes, and data sharing activities. These logs are accessible via SQL queries across all workspaces. This provides AGL's data governance team with visibility for compliance monitoring and forensic analysis.  For integration with AGL's existing monitoring infrastructure, diagnostic logs deliver audit events in JSON format to Azure Event Hub, Azure Storage, or Log Analytics with typical 15-minute latency. Workspace administrators can enable verbose audit logging to capture granular command-level details including SQL query text and notebook cell execution for detailed compliance investigations.  System tables retain audit data for 365 days at both workspace and account levels. Real-time lineage tracking captures table-level and column-level data lineage across all operations, available in system.access.tablelineage and system.access.columnlineage tables for impact analysis and regulatory reporting.  This architecture requires metastore admin enablement of system tables, with optional verbose logging configuration per workspace based on AGL's compliance requirements.  	References: Best practices for data and AI governance Audit log reference Unity Catalog Governance in Action: Monitoring, Reporting, and Lineage	Full	Yes
RES.056	Data Audit & Monitoring	How do you integrate audit logs with SIEM tools for compliance?	Databricks provides three native patterns for SIEM integration that support AGL's compliance and operational requirements.  For Azure deployments, diagnostic logs stream to Azure Event Hub, Azure Storage, or Log Analytics with 15-minute latency. This captures workspace-level and account-level events including Unity Catalog data access operations in JSON format. Azure Sentinel integration supports real-time Event Hub ingestion, ADLS Gen2 batch export, and custom table ingestion via Log Analytics Workspace API.  The Databricks Add-on for Splunk enables bidirectional integration without custom code. Security analysts can run Databricks SQL queries from Splunk, execute notebooks from Splunk UI, and push enrichment results back to Splunk indexes via HTTP Event Collector. This maintains single-pane-of-glass operations for SOC teams.  System tables provide a cost-effective alternative where security teams query system.access.audit using SQL and export filtered results to external SIEM platforms. This enables 365-day free retention in Delta Lake while sending only critical events to expensive SIEM storage.  All patterns support AGL's compliance frameworks while optimising for operational efficiency and cost management.  	References: Configure audit log delivery - Azure Databricks Audit log system table reference - Azure Databricks Databricks Add-on for Splunk Cybersecurity Analytics and AI Solution Accelerator Best practices for data and AI governance - Azure Databricks Audit log reference - Databricks on AWS Securing the Grid: A Practical Guide to Cyber Analytics for Energy & Utilities	Full	Yes
RES.057	Data Audit & Monitoring	Describe your anomaly detection approach for access and usage patterns.	With configuration, Databricks can detect anomalies in access and usage patterns through the system.access.audit table, which captures all user activities and data access operations. Security teams build SQL-based detection queries to identify suspicious patterns including repeated failed logins, unusual access times, simultaneous remote sessions, privilege escalation attempts, high-volume data operations, and unauthorised permission changes. These queries integrate with Databricks SQL alerts for automated notifications.  For AGL's cross-cloud environment, the Databricks Add-on for Splunk enables bidirectional integration with your existing Splunk SIEM on AWS. Security teams run threat hunting queries from Splunk against Databricks security telemetry across both AWS and Azure deployments, whilst enriched alerts flow back to Splunk indexes via HTTP Event Collector. This approach minimises cross-cloud data movement and associated egress costs.  Advanced behavioural analytics leverage MLflow to build custom anomaly detection models combining time series analysis, clustering algorithms, and supervised learning. These models analyse unified security and operational telemetry to identify sophisticated threats including account takeover, lateral movement, and data exfiltration attempts at scale.  Enhanced Security Monitoring provides behavior-based file integrity monitoring and malware detection agents on classic compute that automatically flag suspicious host activity including unauthorized file changes, unexpected processes, and boundary violations with detailed forensic logs delivered to system.access.audit for investigation and automated response workflows.	References: Anomaly detection - Azure Databricks Evolving Your SIEM Detection Rules: A Journey from Simple to Sophisticated Audit log reference - Databricks on Google Cloud Training 10,000 Anomaly Detection Models on One Billion Records with Explainable Predictions	Full	Yes
RES.058	DevSecOps	How do you embed security scans and compliance checks into CI/CD pipelines?	Databricks embeds security and compliance checks into CI/CD pipelines through native integration with Azure DevOps, GitHub Actions, and Jenkins using Databricks Asset Bundles.  Asset Bundles define infrastructure as code with validation gates before deployment. Pipeline stages can include static code analysis using pylint with Databricks Labs plugin, secrets scanning through audit log analysis, unit and integration testing with pytest, and infrastructure validation via bundle validate commands. OAuth workload identity federation provides secure authentication without long-lived secrets across dev-staging-production environments.  Runtime security operates through biweekly maintenance updates covering operating system patches, Python library updates, Java security fixes, and Apache Spark bug fixes. Contractual SLAs ensure critical vulnerabilities are patched within 14 days, high severity within 30 days, and medium severity within 60 days from vendor patch availability. Clusters automatically use latest base images on restart.  The Security Analysis Tool continuously monitors workspace configurations against Databricks security best practices, detecting deprecated runtime versions, misconfigured access controls, and compliance drift. Enhanced Security Monitoring adds CIS Level 1 hardened images, behaviour-based malware monitoring, file integrity monitoring, and antivirus scanning with logs delivered to system.access.audit for SIEM integration supporting AGL's Splunk on AWS environment.  For HIPAA, PCI-DSS, or FedRAMP workloads, automatic cluster updates can be permanently enabled for continuous compliance.  	References: Best practices and recommended CI/CD workflows on Databricks Runtime maintenance updates Monitoring notebook command logs with static analysis tools GitHub Actions for Databricks CI/CD CI/CD with Jenkins on Databricks Databricks Pylint	Full	Yes
RES.059	DevSecOps	Describe your policy-as-code approach for data security.	Databricks implements policy-as-code through Unity Catalog, where all permissions are declarative SQL GRANT statements, Terraform resources, or REST API calls version-controlled in Git. This enables CI/CD promotion of security policies across environments with complete auditability through the system.access.audit table, recording user identity, timestamp, and modifications for compliance reporting.  Identity management integrates with Microsoft Entra ID through SCIM or Automatic Identity Management (GA), synchronising users, groups, and service principals in real-time. Group-based permissions automatically inherit to members, enabling centralised governance with least-privilege access aligned to organisational roles.  Attribute-Based Access Control (ABAC) policies in Public Preview provide high-leverage governance through tag-driven row filters and column masks. Administrators define policies once at catalog or schema level using governed tags that automatically inherit to all current and future tables.  Data Classification in Public Preview uses AI to automatically detect and tag sensitive data (credit cards, email addresses, phone numbers) within 24 hours for new tables. Classification tags automatically trigger ABAC policies, creating end-to-end automated workflow from detection to protection.  For AGL, this approach ensures consistent security enforcement across customer data, operational systems, and regulatory reporting as your data estate scales to support the lower-emissions future.  	References: Access control in Unity Catalog April 2025 Release Notes Data governance with Databricks Unity Catalog attribute-based access control (ABAC) Data Classification Automatic Identity Management	Full	Yes
RES.060	DevSecOps	What integrations exist with vulnerability management tools?	Databricks provides three primary integration paths for AGL vulnerability management.  First, Enhanced Security Monitoring delivers vulnerability scan reports of container host VMs directly to workspace administrators. Reports cover known CVEs and include use an enhanced hardened images, with security event logs flowing to the system.access.audit table for integration with AGL's Splunk SIEM on AWS.  Second, HiddenLayer Model Scanner integrates with Unity Catalog and MLflow Model Registry to detect malware, backdoors, and supply chain vulnerabilities in third-party AI/ML models. Scans trigger automatically via MLflow webhooks during model registration or stage transitions.  While you have the option to install any software on Databricks systems, tools like Anti-Virus, Anti-Malware, and Endpoint Detection and Response (EDR) tools will often not work because they require full root access to run. Part of Databricks' security design is that all user code runs in an unprivileged container -- users can run as root within the container, but they do not have root access to the underlying system. Databricks tries to make as much available as possible to help security teams, but most endpoint security tools will not function in this environment. Customers have successfully run periodic system AV scans (not using on-access scanning which would require more privileges) along with vulnerability scans, but this is up to the capabilities of your security tools.  Databricks maintains SOC 2 Type II and ISO 27001 certifications, with security patches applied through the Databricks Runtime maintenance updates.  	References: Enhanced Security Monitoring Deploying Third-Party Models Securely with HiddenLayer Databricks Runtime Maintenance Updates	Full	Yes
RES.061	DataOps	Explain your approach for automated deployment and version control of data pipelines.	Databricks provides a three-layer approach to pipeline deployment and version control, addressing AGL's regulatory compliance and operational excellence requirements.  Version Control Foundation: Databricks Asset Bundles define pipelines as infrastructure-as-code in YAML, stored in Git repositories. This includes Lakeflow Spark Declarative Pipelines definitions, notebooks, job configurations, and cluster specifications. Git integration through Databricks Repos enables standard branch management, pull requests, and code review workflows.  Automated Deployment: Asset Bundles deploy via databricks bundle deploy commands, triggered through Azure DevOps Pipelines or GitHub Actions. This supports promotion across dev-staging-production environments with environment-specific configurations. OAuth workload identity federation provides secure authentication without managing long-lived credentials. The Databricks CLI and Terraform provider enable integration with existing CI/CD tooling.  Data Versioning and Audit: Delta Lake's transaction log maintains complete version history of all table operations, including schema changes. Time travel queries (VERSION AS OF, TIMESTAMP AS OF) enable point-in-time analysis for regulatory reporting. The RESTORE command supports rollback to previous versions. DESCRIBE HISTORY provides full audit trails of modifications, critical for AGL's compliance requirements in energy sector operations including customer billing and market reporting.  This approach reduces deployment risk, accelerates development cycles, and provides audit capabilities required for regulatory oversight.  	References: Lakeflow Spark Declarative Pipelines release notes and the release upgrade process Create a source-controlled pipeline Databricks Asset Bundles FAQs Announcing the General Availability of Databricks Asset Bundles	Full	Yes
RES.062	DataOps	How do you integrate with Git-based workflows for pipeline management?	Databricks provides native Git integration through two primary mechanisms: Git folders and Databricks Asset Bundles.  Git folders integrate directly with GitHub, GitLab, Azure DevOps, Bitbucket, and AWS CodeCommit. Teams perform standard Git operations (clone, branch, commit, push, pull, merge) for notebooks, Python/SQL files, and pipeline configurations through the UI or CLI. This supports collaborative development with full version control.  Databricks Asset Bundles enable infrastructure-as-code using YAML definitions that bundle source code, Lakeflow Jobs, pipelines, and cluster configurations together. Bundles automatically track Git metadata (repository origin, branch) for deployment traceability across environments.  CI/CD workflows integrate with Azure DevOps, GitHub Actions, and Jenkins to automate bundle validation, deployment, and testing. Teams use databricks bundle validate and databricks bundle deploy commands to promote changes from development through staging to production environments.  SQL developers version control .sql files with automated syntax validation, while dashboards are versioned as JSON files with automated deployment through bundles. This provides end-to-end pipeline management with full Git-based workflows.  	References: Git integration with Databricks repos Databricks Asset Bundles CI/CD on Azure Databricks Best practices for CI/CD workflows Create a source-controlled pipeline Databricks Asset Bundles FAQs	Full	Yes
RES.063	DataOps	Describe your automated testing and validation strategy for data pipelines.	Databricks delivers automated testing across three layers for production pipeline reliability.  Unit and integration testing uses native pytest integration within notebooks and CI/CD pipelines. The workspace testing interface (Public Preview) provides automated test discovery and execution through a dedicated sidebar, validating transformation logic before deployment.  Data quality validation is embedded in Lakeflow Spark Declarative Pipelines through declarative EXPECT clauses. These SQL-based constraints can retain, drop, or fail on invalid records, with all expectation metrics automatically logged to pipeline event logs for continuous monitoring. Quality checks live within the pipeline definition rather than requiring separate validation jobs.  Databricks Labs DQX framework extends quality validation with automated data profiling and rule generation. It supports in-transit validation before data writes and post-factum monitoring, with capabilities to quarantine bad data and generate detailed row-level and column-level quality reports.  For test data generation, dbldatagen creates realistic synthetic datasets at scale without exposing production data, supporting comprehensive performance and functional testing across distributed compute.  This testing strategy enables faster deployment cycles for AGL's data platform supporting critical energy operations across the National Electricity Market.  	References: Lakeflow Spark Declarative Pipelines release notes and the release upgrade process Best practices and recommended CI/CD workflows on Databricks Develop Lakeflow Spark Declarative Pipelines Data Reliability Explained	Full	Yes
RES.064	Data Observability	How do you monitor data freshness, volume, and schema changes in real time?	Databricks monitors all three dimensions through integrated observability built into the platform.  Data Freshness: Lakeflow Spark Declarative Pipelines track backlog metrics (bytes, records, files, seconds) across streaming sources including Kafka, Kinesis, Delta, and Auto Loader. Pipeline event logs expose these as queryable Delta tables, enabling SQL-based dashboards and immediate alerts for data delays. Unity Catalog tracks table update timestamps and commit history for batch workloads.  Volume Monitoring: Pipeline event logs capture real-time throughput including numoutputrows, processing rates, and resource utilisation per flow. The streaming observability UI visualises these metrics, making throughput degradation immediately visible. Event logs integrate with Databricks SQL alerts or external monitoring tools via standard SQL queries.  Schema Changes: Auto Loader detects schema evolution during incremental processing and logs all changes to pipeline event logs. Configurable evolution modes (fail, rescue, or auto-add columns) ensure schema drift is tracked without manual intervention. All changes are versioned in Delta Lake transaction logs for full audit history.  This unified observability across ingestion, transformation, and consumption layers reduces mean time to detection for data quality issues, supporting AGL's operational reliability requirements.  	References: How Observability in Lakeflow Helps You Build Reliable Data Pipelines Introducing Streaming Observability in Lakeflow Jobs and Lakeflow Spark Declarative Pipelines July 2025 Release Notes - Real-time Streaming Mode Data quality monitoring	Full	Yes
RES.065	Data Observability	Describe your anomaly detection and alerting mechanisms for data pipelines.	Databricks provides integrated anomaly detection and alerting for data pipelines, supporting AGL's operational reliability requirements for energy data systems.  Unity Catalog includes automatic data quality monitoring that detects freshness and completeness anomalies by analysing historical commit patterns and row volumes. Deviations from expected patterns trigger alerts, with results logged to queryable system tables for root cause analysis. Lakeflow Jobs can be configured with thresholds with warnings and timeouts for duration and streaming backlog thresholds.  Lakeflow pipelines support declarative expectations that validate data quality during execution. You define constraints with configurable actions on violations: warn, drop invalid records, or fail the pipeline. The platform captures passed and failed record counts in pipeline event logs stored as Delta tables, enabling SQL-based monitoring without external tooling.  Pipeline event logs expose real-time metrics including backlog volumes, output row counts, autoscaling events, and expectation results. These logs support operational dashboards and custom monitoring through event hooks that trigger Python functions on specific pipeline events.  Databricks SQL alerts enable query-based conditions with notification destinations including email, Slack, Microsoft Teams, PagerDuty, and custom webhooks. This integrates with AGL's existing incident management workflows for rapid response to data quality issues affecting customer-facing systems.  	References: May 2025 Release Notes Lakeflow Spark Declarative Pipelines Release 2023.41 Monitor Pipelines Documentation Data Quality Monitoring Documentation How Observability in Lakeflow Helps You Build Reliable Data Pipelines	Full	Yes
RES.066	Data Observability	What capabilities exist for lineage-based impact analysis?	Unity Catalog delivers comprehensive lineage-based impact analysis through automatic capture of runtime table and column-level lineage across all languages and workloads, without additional tooling or cost.  The platform tracks relationships between data assets and the notebooks, jobs, dashboards, and ML models that create or consume them. This enables downstream impact assessment when evaluating schema changes, table deprecations, or data quality issues, and upstream root cause analysis to trace problems back to source jobs or pipelines.  Lineage visualisation appears in near real-time through Catalog Explorer and is accessible programmatically via system.access.tablelineage and system.access.columnlineage system tables. Cross-workspace lineage aggregation provides unified visibility across all workspaces attached to a Unity Catalog metastore, with privilege-aware access controls maintaining security while enabling enterprise-wide impact analysis.  Programmatic access via REST API and system tables supports custom impact analysis queries, automated change management workflows, and integration with existing governance tools. Data Quality Monitoring leverages lineage to prioritise anomalies by downstream impact and identify upstream job failures as root causes.  	References: View data lineage using Unity Catalog Lineage system tables reference Announcing General Availability of Data Lineage in Unity Catalog	Full	Yes
RES.067	Model Marketplace	Describe your curated marketplace for pre-trained models and its governance.	Databricks Marketplace provides a curated platform for pre-trained AI models with comprehensive Unity Catalog governance. The marketplace offers over 75 models including foundation models from Meta, Mistral, and Google, plus industry-specific models from providers like John Snow Labs, all accessible through a single governed platform.  Unity Catalog delivers unified governance across all models with centralised access control, auditing, lineage tracking, and usage monitoring. This ensures security and compliance for shared AI assets, directly supporting AGL's regulatory requirements and data governance standards. Pre-trained foundation models are available directly in Unity Catalog under the system.ai schema for immediate deployment with provisioned throughput.  Delta Sharing enables secure model distribution across clouds and regions without data replication, maintaining a single source of truth whilst allowing providers to track consumption. For AGL, this supports secure collaboration across business units and external partners whilst maintaining governance controls aligned with your Business Intelligence value driver.  Model Context Protocol servers are now listed in Marketplace (Public Preview), providing discoverable AI agent tools with Unity Catalog connections managing authentication and governance, extending AGL's governed AI capabilities to external integrations.  	References: December 2025 Release Notes - Databricks-hosted models Databricks GenAI Announcements - Curated model catalog and governance Introducing AI Model Sharing with Databricks Best practices for data and AI governance	Full	Yes
RES.068	Model Marketplace	How do you enable direct deployment of marketplace models into production?	Databricks provides a streamlined path from Marketplace to production through Unity Catalog and Agent Bricks Model Serving integration.  Models from Databricks Marketplace install directly into Unity Catalog as governed assets with centralised access control, lineage tracking, and audit capabilities. Foundation models are pre-installed in the system.ai catalog and available across all workspaces immediately.  Production deployment occurs via Agent Bricks Model Serving with two endpoint options. Pay-per-token endpoints enable rapid deployment with cost efficiency for variable workloads. Provisioned throughput endpoints deliver performance guarantees for production-critical applications. Both support automatic scaling, versioning, and traffic routing.  Deployment executes through the Serving UI or programmatically via MLflow APIs, enabling full CI/CD integration. This supports alias-based deployment strategies for champion/challenger testing and staged rollouts, ensuring production readiness with minimal operational overhead.  For AGL's energy forecasting and customer analytics use cases, this integrated workflow reduces time-to-production whilst maintaining governance and compliance requirements across the model lifecycle.  	References: Upgrade ML Lakeflow Jobs to target models in Unity Catalog Model deployment patterns MLflow 3 deployment jobs Migrate to Model Serving	Full	Yes
RES.069	Model Marketplace	What differentiates your approach to model versioning and rollback?	Databricks unifies model versioning and rollback through MLflow Model Registry integrated with Unity Catalog, providing enterprise governance without external tooling.  Model aliases enable instant production rollback. Champion and Challenger aliases let you reassign production traffic immediately when issues arise. Unity Catalog automatically captures complete lineage from source data through model versions to inference endpoints, ensuring reproducibility and rapid root cause analysis.  MLflow 3 deployment jobs automate the entire workflow inside of Unity Catalog. New model versions trigger governed evaluation and approval pipelines, with all activities logged to Unity Catalog audit logs. This supports human-in-the-loop approvals natively within the platform, eliminating manual orchestration.  Cross-workspace model versioning enables seamless promotion across dev, staging, and production environments using Unity Catalog's three-level namespace. Full lineage and metadata travel with each model while environment-specific governance policies remain enforced.  For AGL, this translates to faster incident response when models degrade, complete audit trails for regulatory requirements, and reduced operational overhead. Teams use standard open-source MLflow APIs while gaining enterprise governance. Delta Lake time travel provides coordinated data versioning, ensuring models and training data remain synchronized across the lifecycle.  	References: Manage model lifecycle in Unity Catalog MLflow deployment jobs MLOps workflows on Databricks Unity Catalog audit logs for model operations Databricks multi-AI indemnity	Full	Yes
RES.070	AI Governance	Explain your policy-driven governance framework for AI models.	Databricks delivers policy-driven AI governance through Unity Catalog, applying consistent access controls, audit trails, and compliance frameworks across all AI assets, data, and analytics.  Three core capabilities enable this:  1. Centralised policy enforcement: MLflow AI Gateway governs all LLM endpoints (external models like OpenAI/Anthropic and Databricks-hosted models) with configurable guardrails for PII detection, safety filtering, role-based permissions, and token-level rate limiting per user or group.  2. Automated compliance capture: MLflow Tracing automatically stores GenAI agent execution traces (prompts, tool calls, responses, intermediate steps) in governed Unity Catalog tables, enabling policy-based access control and SQL-based audit analysis without separate observability platforms.  3. Approval workflow controls: Tag-based deployment policies automate promotion workflows, controlling which teams approve model versions, preventing self-approval, and logging all activities to Unity Catalog audit tables accessible via system.access.audit.  For AGL, this means AI model governance integrates with existing data governance policies, supporting energy sector regulatory compliance whilst enabling safe, auditable deployment of AI capabilities across customer-facing and operational systems.  	References: March 2025 Release Notes - AI Gateway pay-per-token endpoints Best practices for data and AI governance Introducing the Databricks AI Governance Framework RAG application governance and LLMOps Concepts: Generative AI on Azure Databricks	Full	Yes
RES.071	AI Governance	How do you enforce ethical AI guidelines and bias checks?	Databricks enforces ethical AI guidelines through three integrated layers critical for AGL's customer-facing and operational AI systems.  Detection and Assessment: Agent Bricks Agent Evaluation provides built-in LLM judges assessing safety, bias, groundedness, and correctness with plain-language rationales. Custom LLM Judges can also be configured to align with your requirements. Lakehouse Monitoring delivers automated fairness metrics for classification models, including predictive parity and equal opportunity across protected demographic groups. This directly supports AGL's commitment to serving customers experiencing vulnerability with equitable outcomes.   Enforcement: AI Guardrails in Public Preview enforce safety filtering and PII detection at model serving endpoints using Meta Llama Guard, blocking harmful content across violence, hate speech, and other categories. Custom guardrails enable AGL-specific ethical policies to be codified and automatically enforced at runtime.  Continuous Monitoring: MLflow Tracing captures detailed execution traces of AI agents, enabling compliance auditing and validation of ethical guidelines. Configurable alerts trigger when fairness thresholds are exceeded, supporting proactive governance aligned with AGL's regulatory obligations.  The Review App enables subject matter experts to provide human oversight for bias detection, ensuring AI systems align with AGL's values throughout their lifecycle.  	References: Introducing the Databricks AI Governance Framework Responsible AI with the Databricks Data Intelligence Platform Best practices for data and AI governance RAG application governance and LLMOps Governance Risk & Compliance Essential Strategies	Full	Yes
RES.072	AI Governance	Describe your audit trail capabilities for AI model lifecycle events.	Databricks delivers comprehensive audit trails for AI model lifecycle events, supporting AGL's governance requirements across energy operations and regulatory compliance.  MLflow Tracking automatically captures all training events: parameters, metrics, code versions, training datasets, user identity, and timestamps. This metadata flows to Unity Catalog for cross-workspace visibility and centralised governance.  Unity Catalog audit logs record all model registry events including version updates, permission changes, alias assignments, and deletions. Each event captures request parameters, user identity, workspace ID, and timestamps. Logs are stored in the system.access.audit table with 365-day retention and are queryable via SQL for compliance reporting.  MLflow System Tables provide Delta tables for experiment metadata and run history across workspaces, enabling SQL-based analytics and monitoring dashboards to track model reliability trends.  For generative AI applications, MLflow Tracing captures inputs, outputs, intermediate steps, token usage, and execution metadata in real-time, supporting compliance auditing and quality evaluation throughout development and production.  All audit data is queryable, exportable, and integrates with AGL's existing security information and event management systems, providing the transparency required for regulatory oversight and operational accountability in energy market operations.  	References: Introducing the Databricks AI Governance Framework Best practices for data and AI governance RAG application governance and LLMOps Data and AI governance for the data lakehouse	Full	Yes
RES.073	Data Federation	How do you support query federation across heterogeneous data sources?	Databricks supports query federation through Unity Catalog foreign catalogs and Lakehouse Federation, providing unified access to heterogeneous data sources without migration.  Unity Catalog foreign catalogs offer read-only JDBC connectivity to MySQL, PostgreSQL, SQL Server, Oracle, Teradata, Google BigQuery, Amazon Redshift, Snowflake, and Azure Synapse. Queries are automatically translated to the appropriate SQL dialect and pushed down to source systems for distributed execution, minimising data movement.  For enterprise applications, Salesforce Data 360 federation supports JDBC query federation and high-performance file sharing via Data-as-a-Service APIs. The SAP Business Data Cloud Connector enables zero-copy bi-directional data sharing using Delta Sharing, preserving business semantics and supporting cross-cloud scenarios.  All federated sources appear as standard catalogs in Unity Catalog, providing unified access control, automated data lineage tracking, and centralised auditing across the data estate. Materialised views cache frequently-accessed federated data to optimise query performance for recurring analytical workloads.  This architecture supports data integration requirements while maintaining governance and reducing operational complexity across AGL's heterogeneous systems.  	References: What is Lakehouse Federation? What is query federation? Introducing Salesforce Connectors for Lakehouse Federation and Lakeflow Connect Announcing General Availability of Lakehouse Federation for Google BigQuery and Public Preview for Teradata and Oracle Enable Snowflake catalog federation	Full	Yes
RES.074	Data Federation	Describe your optimization strategies for federated queries.	Databricks optimises federated queries through five core strategies that minimise data movement and maximise performance across AGL's distributed data landscape.  Lakeflow Connect now offers row filtering for managed ingestion connectors to improve performance and minimize data duplication. The Salesforce Connector can also incrementally ingest calculation fields.  Query pushdown automatically translates Databricks SQL to native database dialects (MySQL, PostgreSQL, SQL Server, Oracle, Teradata, Snowflake, BigQuery, Redshift). Filters, aggregations, projections, and joins execute at the source using native database engines before transferring results, reducing network overhead and query latency. Lakeflow Connect now offers row filtering for managed ingestion connectors to improve performance and minimize data duplication.  Join pushdown (GA for Redshift, Snowflake, BigQuery; Public Preview for Oracle, PostgreSQL, MySQL, SQL Server, Teradata) delegates join operations to remote engines, leveraging native database capabilities and reducing data transfer volumes.  Materialised views cache frequently accessed federated data as Delta tables with incremental refresh. This provides consistent low-latency access whilst offloading query load from operational databases, accelerating cross-source joins and complex transformations critical for AGL's analytics workloads.  Parallel read optimisation partitions queries across multiple JDBC connections using configurable partition columns and fetch size parameters, enabling efficient batched reads that reduce memory overhead and improve throughput.   Query Profile and EXPLAIN FORMATTED commands expose generated SQL for performance tuning, ensuring transparency aligned with AGL's data governance standards.  	References: Databricks SQL release notes 2024 Announcing General Availability of Lakehouse Federation What is query federation? What is Lakehouse Federation? Incrementally Ingest Salesforce Formula Fields	Full	Yes
RES.075	Data Federation	What caching and materialization options exist for federated query results?	Databricks provides three complementary caching mechanisms for federated query results.  Materialized views (DBSQL/SDP) physically store precomputed federated query results as Delta tables with automatic incremental refresh. When row-tracking and change data feed are enabled, Lakeflow Spark Declarative Pipelines process only changed data from federated sources, reducing compute costs compared to full recomputes. Refresh schedules can be configured via cron or triggered automatically when upstream sources change.  Serverless query result caching provides 24-hour persistent shared cache across all SQL warehouses. Identical query results are stored in workspace system data with automatic invalidation when underlying federated tables update, enabling sub-second response times for repeated queries.  Disk cache automatically stores copies of federated data files on local SSD storage in fast intermediate format, accelerating repeated data reads. The cache automatically detects source changes to invalidate stale data.  These mechanisms work together to minimise cross-system query overhead and offload query load from operational databases, supporting AGL's need for responsive analytics across federated data sources without impacting source system performance.	null	Full	Yes
RES.076	Real-Time Data Management	Explain your architecture for real-time ingestion and processing of streaming data.	Databricks delivers enterprise streaming architecture built on Apache Spark Structured Streaming with Delta Lake enabling unified batch and streaming operations through ACID transaction guarantees.  Spark Real-Time mode in Public Preview delivers ultra-low latency processing with p99 end-to-end latency as low as 5ms for operational workloads.  The architecture comprises three integrated layers. The ingestion layer uses Auto Loader for incremental file processing from cloud storage with exactly-once semantics and automatic schema inference. Structured Streaming connects directly to Apache Kafka, Azure Event Hubs, Amazon Kinesis and other sources with configurable processing intervals from continuous to micro-batch execution.  The processing layer leverages Lakeflow Spark Declarative Pipelines to unify batch and streaming through streaming tables for exactly-once incremental processing and materialised views for incrementally-refreshed aggregations. This declarative approach eliminates manual Spark code whilst providing automatic dependency management and data quality expectations.  The storage layer uses Delta Lake to enable simultaneous streaming writes and analytical reads on the same table with serialisable isolation. This allows real-time ingestion whilst analytics queries execute concurrently without conflicts, critical for operational workloads like energy trading decisions or customer event processing.  For AGL's operational requirements, this architecture supports real-time processing of smart meter data, trading system events and customer interactions whilst maintaining data consistency for regulatory reporting and analytics. The unified platform eliminates complexity of maintaining separate streaming and batch systems, reducing operational overhead and time-to-insight for energy operations.   	References: July 2025 Release Notes - Real-time mode for sub-second latency Streaming tables documentation Structured Streaming concepts Introducing Real-Time Mode in Apache Spark Structured Streaming	Full	Yes
RES.077	Real-Time Data Management	How do you guarantee low-latency delivery for mission-critical workloads?	Databricks guarantees low-latency delivery through three architectural layers optimised for mission-critical operations.  For operational applications requiring sub-second response, Databricks Lakebase (Managed Postgres) delivers single-digit millisecond query latency with high-concurrency point reads and writes. Managed synced tables automatically synchronise Delta tables to Postgres in continuous, triggered, or snapshot modes, eliminating custom integration overhead.  For streaming workloads, Structured Streaming with Real-Time Mode (Public Preview) provides continuous low-latency processing with exactly-once semantics. Delta Lake's ACID transactions ensure data consistency across concurrent streaming and batch operations, critical for scenarios like fraud detection and payment authorisation.  Lakeflow Jobs orchestration includes automatic retries and checkpoint-based recovery for fault-tolerant execution. Photon engine, enabled by default on serverless compute, accelerates SQL and DataFrame workloads without configuration.  For AGL's energy trading systems, grid monitoring, and customer service platforms, this architecture delivers the sub-second response times required for operational decision-making whilst maintaining data consistency across the lakehouse. This supports real-time pricing decisions, grid stability monitoring, and customer interaction workflows that underpin reliable service delivery.  	References: July 2025 Release Notes - Real-time mode for Structured Streaming Databricks Runtime 17.1 Release Notes Real-time mode in Structured Streaming documentation Reliability for the data lakehouse	Full	Yes
RES.078	Real-Time Data Management	Describe your approach to schema evolution in real-time pipelines.	Databricks manages schema evolution in real-time pipelines through three integrated layers that maintain data continuity without pipeline interruptions.  Auto Loader detects schema changes automatically at ingestion. The addNewColumns mode adds new fields, while rescued data columns capture unexpected variations in JSON format for later reconciliation rather than dropping data or failing streams.  Delta Lake enforces schema at write time with explicit evolution controls. The mergeSchema option enables controlled column additions during streaming writes. For CDC patterns, MERGE WITH SCHEMA EVOLUTION syntax handles complex changes while preserving ACID guarantees and exactly-once semantics.  Lakeflow Spark Declarative Pipelines extends these capabilities with declarative expectations that validate schema constraints while permitting evolution. Unity Catalog tracks all schema versions and lineage, providing audit trails for compliance and impact analysis.  This approach supports AGL's real-time analytics requirements for customer behaviour, grid telemetry, and trading systems where data continuity is critical but source schemas evolve frequently. Schema changes become controlled events rather than pipeline failures, enabling continuous operation across renewable generation monitoring, customer platform integration, and market trading systems.  	References: Auto Loader documentation Structured Streaming guide Delta Lake schema evolution Lakeflow Spark Declarative Pipelines release notes June 2025 Release Notes From Events to Insights: Complex State Processing with Schema Evolution in transformWithState Develop Lakeflow Spark Declarative Pipelines	Full	Yes
RES.079	Data Reconciliation	How do you implement automated reconciliation of data across systems?	Databricks provides two reconciliation approaches depending on your scenario.  For migration and one-time validation, Lakebridge Reconciler automates comparison between source systems and Databricks. It performs schema validation, row count checks, and column-level diffs using hash-based matching. The reconciler pushes compute to source systems and transfers minimal data (approximately 32 bytes per row), outputting detailed discrepancy reports to Unity Catalog Delta tables. You configure thresholds, filters, transformations, and column mappings to match validation requirements.  For operational pipelines, Lakeflow Spark Declarative Pipelines expectations define SQL-based quality constraints with configurable actions (warn, drop, or fail) on invalid records. Metrics flow to pipeline event logs for monitoring. Data Quality Monitoring adds automated anomaly detection for freshness and completeness, with profiling metrics queryable via system tables.  Unity Catalog provides end-to-end column-level lineage across all transformations, enabling validation of data movement and tracing discrepancies to source. This supports AGL's requirement for data trust across generation, retail, and customer systems.  	References: Lakebridge Reconciler Expectations in Lakeflow Spark Declarative Pipelines Data Quality Monitoring Tutorial: Build an ETL pipeline using change data capture Best practices for data and AI governance What does it mean to build a single source of truth	Full	Yes
RES.080	Data Reconciliation	Describe your anomaly detection approach during reconciliation.	Databricks detects anomalies during reconciliation through three core mechanisms that ensure data integrity across AGL's energy systems.  Delta Lake's change data feed tracks row-level changes between source and target tables, creating an audit trail that identifies unexpected data patterns in inserts, updates, and deletes. This provides precise reconciliation at the transaction level.  Data Quality Monitoring uses ML forecasting models trained on historical table metrics such as commit times, row counts, and data volumes. The system learns expected patterns including seasonal variations in energy data, then automatically flags anomalies when freshness or completeness falls outside predicted ranges. High-impact tables are prioritised based on downstream usage across AGL's analytics estate.  Reconciliation results are stored in Delta tables with detailed mismatch reports accessible through Unity Catalog. The system.dataqualitymonitoring.table_results logs anomalies with root cause analysis and downstream impact scoring, giving data teams actionable insights.  Custom reconciliation logic can be implemented using Databricks notebooks or Lakeflow pipelines, leveraging Delta Lake's ACID guarantees and schema enforcement to detect structural anomalies. Configurable thresholds enable tolerance-based validation for numeric fields where exact matching is not required.  This approach supports AGL's data governance requirements whilst maintaining flexibility for complex energy market reconciliation scenarios.  	References: Data quality monitoring | Databricks on AWS Data governance with Azure Databricks Best practices for data and AI governance Training 10,000 Anomaly Detection Models on One Billion Records	Full	Yes
RES.081	Data Reconciliation	What workflows exist for reconciliation alerts and remediation?	Databricks provides comprehensive alert and remediation workflows for reconciliation through integrated monitoring and recovery capabilities.  For alerting, Databricks SQL alerts monitor reconciliation outputs in Unity Catalog Delta tables, detecting schema, row, or column mismatches. These alerts trigger notifications via email, Slack, Microsoft Teams, PagerDuty, or webhooks based on configurable thresholds. Data Quality Monitoring automatically identifies freshness and completeness anomalies, logging results to system tables for severity-based alerting. Lakeflow Spark Declarative Pipelines surface expectation failures directly through pipeline notifications.  Remediation workflows minimise recovery time and cost. Lakeflow Jobs repair runs re-execute only failed tasks and their dependencies, avoiding redundant processing. Quarantine patterns isolate invalid records into separate tables for investigation and selective reprocessing. Jobs support automatic retries with exponential backoff and configurable policies. Delta Lake time travel enables rollback to previous table versions using RESTORE commands when data quality issues require remediation.  For AGL, these workflows support data integrity across energy trading, customer billing, and regulatory reporting reconciliation processes, reducing manual intervention and accelerating issue resolution for critical business operations.  	References: May 2025 Release Notes - SQL Alerts Consolidation Lakeflow Spark Declarative Pipelines Release 2023.11 - Pipeline Features Best Practices for Data and AI Governance Data Reliability Explained Data Quality Monitoring Alerts Job Notifications Repair Job Failures	Full	Yes
RES.082	Data Transformation	Explain your support for declarative and code-based transformations.	Databricks supports both declarative and code-based transformations, enabling AGL's teams to select the optimal approach for their requirements and skill sets.  Declarative Transformations: Lakeflow Spark Declarative Pipelines (SDP) provides a SQL-first framework where you declare desired transformations and the platform handles execution. SQL analysts and data engineers define streaming tables and materialised views using standard CREATE statements without managing infrastructure. The framework automatically manages orchestration, dependencies, scaling, recovery, data quality checks, and incremental processing. SDP is interoperable with Apache Spark Declarative Pipelines (open source Spark 4.1+), avoiding vendor lock-in whilst delivering enhanced performance through Databricks Runtime and Photon.  Code-Based Transformations: Multi-language notebooks support Python (PySpark), Scala, SQL, and R for custom transformation logic with complete control. Apache Spark DataFrame APIs provide programmatic access to distributed transformations. Unity Catalog Functions enable reusable Python, SQL, and Scala user-defined functions across workloads. This approach supports complex stateful processing, custom ML pipelines, and external system integrations.  Both approaches benefit from unified Unity Catalog governance, providing consistent access control, lineage, and data quality across all transformation types.  	References: June 2025 Release Notes - Moving streaming tables and materialized views between pipelines Procedural vs. declarative data processing in Databricks Data engineering concepts	Full	Yes
RES.083	Data Transformation	How do you ensure version control and auditability of transformations?	Databricks delivers version control and auditability through three integrated layers.  Git Folders natively integrate with GitHub, GitLab, Azure DevOps, and Bitbucket, version-controlling all transformation code including notebooks, SQL files, Python scripts, and Lakeflow pipeline definitions. This enables standard branching, pull requests, and CI/CD workflows via Databricks Asset Bundles.  Unity Catalog automatically captures runtime data lineage at table and column levels, tracking which notebooks, jobs, and pipelines read or write data. Lineage is visualised in Catalog Explorer or queryable through system tables with 365-day retention. Audit logging records every transformation execution, data access event, and permission change with user identity, timestamp, and request parameters.  Delta Lake's immutable transaction log maintains a complete record of every table operation as JSON commits. Time travel queries and DESCRIBE HISTORY audit all modifications with user and timestamp metadata.  For AGL, this provides compliance audit trails required for energy sector regulatory reporting whilst enabling operational transparency across transformation pipelines supporting customer billing, network analytics, and renewable energy forecasting aligned to Business Intelligence value drivers.  	References: Azure Databricks Repos Unity Catalog data lineage Admin system tables June 2025 Databricks Release Notes Data governance with Databricks Delta table streaming reads and writes AI Architecture: Building Enterprise AI Systems with Governance	Full	Yes
RES.084	Data Transformation	Describe your rollback mechanisms for transformation failures.	Databricks provides three-tier rollback mechanisms for transformation failures, ensuring data reliability for AGL's CTAP reporting and operational analytics.  Table-level rollback uses Delta Lake RESTORE commands to revert tables to any previous version by timestamp or version number within the retention period (7 days default, configurable). DESCRIBE HISTORY provides complete audit metadata for identifying the correct recovery point, supporting compliance requirements.  Pipeline-level recovery through Lakeflow Spark Declarative Pipelines implements automatic retry logic for transient failures. The system retries individual Spark tasks first, then failed flows, then the entire pipeline if necessary, minimising re-execution scope and cost. Streaming transformations recover from checkpoints using exactly-once semantics, resuming from the last successful state without data loss.  Job-level repair via Lakeflow Jobs re-executes only failed tasks and downstream dependencies without re-running successful tasks. Configurable retry policies support automatic task retries with exponential backoff. Conditional task flows enable programmatic error-handling branches for recovery logic.  All rollback actions are tracked through Unity Catalog audit logs and system tables, with alerting available via email, Slack, Microsoft Teams, or webhooks for immediate failure detection and remediation.  	References: Delta Lake time travel and history Lakeflow Spark Declarative Pipelines Job repair and task-level recovery June 2025 Release Notes - Pipeline enhancements Recover a pipeline from streaming checkpoint failure Backfilling historical data with pipelines	Full	Yes
RES.085	Data Cleansing / Enrichment	How do you implement automated cleansing and enrichment workflows?	Databricks implements automated cleansing and enrichment through Lakeflow Spark Declarative Pipelines with built-in quality controls, requiring minimal configuration whilst maintaining production-grade reliability.  Lakeflow Spark Declarative Pipelines enforce data quality expectations inline with transformations using SQL or Python constraints. Quality rules support warn, drop, or fail actions on violations including null checks, range validation, and business logic. Auto Loader provides automatic schema inference with a rescued data column that captures type mismatches and malformed records as JSON for downstream inspection without failing ingestion. Invalid records automatically quarantine into separate tables for investigation and selective reprocessing.  For enrichment, Databricks Feature Engineering enables automatic feature lookups with point-in-time correct joins for time-series data, eliminating training-serving skew. Stream-static joins enrich incremental streaming data with slowly-changing dimensional tables using stateless joins that reference latest dimension versions. AUTO CDC flows handle change data capture with SCD Type 1/2 logic, merging updates and deletes declaratively.  All logic executes within Lakeflow Spark Declarative Pipelines with automatic dependency orchestration, continuous or triggered scheduling, and serverless compute autoscaling, reducing operational overhead whilst ensuring data quality.  	References: Data quality expectations in Lakeflow pipelines Auto Loader schema inference and evolution Databricks data quality monitoring Customer feedback analysis with AI functions for data cleansing	Full	Yes
RES.086	Data Cleansing / Enrichment	Describe your ML-driven enrichment capabilities.	Databricks delivers ML-driven enrichment through Feature Store and automated change data capture workflows that support AGL's customer analytics and operational intelligence requirements.  Feature Store provides automatic feature lookups with point-in-time correct joins for time-series data, eliminating training-serving skew critical for customer behaviour models and demand forecasting. Stream-static joins enrich incremental data with current slowly-changing dimension values, ensuring customer profiles and asset metadata remain accurate without manual intervention. Feature Store is now built on top of Lakebase, our Serverless PostgreSQL offering.  AUTO CDC flows within Lakeflow Spark Declarative Pipelines handle database change capture with SCD Type 1/2 merge logic declaratively. This tracks historical changes to customer accounts, tariff structures, and asset configurations essential for regulatory reporting and customer experience optimisation.  All enrichment workflows execute on serverless compute with automatic scaling and dependency orchestration, reducing operational overhead for AGL's data engineering teams. Data scientists focus on model development rather than feature engineering infrastructure, accelerating time-to-value for initiatives supporting AGL's lower-emissions future and customer vulnerability programmes.  	References: Expectations in Databricks Lakeflow Pipelines Auto Loader Schema Inference and Evolution Lakehouse Monitoring	Full	Yes
RES.087	Data Cleansing / Enrichment	What integrations exist for external data sources and APIs?	Databricks provides four integration approaches for external data sources, all governed through Unity Catalog.  Lakeflow Connect offers managed, no-code connectors for SaaS applications and databases including Salesforce, SQL Server, Workday, ServiceNow, Google Analytics, NetSuite, Dynamics 365, MySQL, PostgreSQL, Oracle, and Teradata. These handle schema evolution, incremental CDC, and change history tracking automatically.  Zerobus Ingest enables direct API write ingestion for high-volume streaming use cases like IoT telemetry and clickstream data, with sub-5-second latency to Delta tables.  Custom PySpark Data Sources allow development teams to build reusable Python connectors for proprietary APIs or industry-specific protocols, integrating with Lakeflow Spark Declarative Pipelines for both batch and streaming patterns.  Lakehouse Federation provides read-only federated queries across MySQL, PostgreSQL, SQL Server, Oracle, Snowflake, BigQuery, and other platforms without data movement. Query pushdown executes on external compute while Unity Catalog maintains governance.  Standard connectors support cloud storage (S3, ADLS Gen2), message buses (Kafka, Event Hubs, Kinesis), and SFTP. All patterns support CI/CD deployment through Databricks Asset Bundles.  	References: Lakeflow Connect documentation Zerobus Ingest documentation Custom PySpark Data Sources documentation Databricks integrations overview	Full	Yes
RES.088	Data Warehouse	Explain your elastic scaling strategy for compute and storage.	Databricks delivers elastic scaling that aligns with AGL's variable energy data workloads and cost optimisation objectives.  For compute, serverless capabilities provide sub-60-second startup times with automatic infrastructure scaling. The platform intelligently scales resources based on real-time demand across notebooks, Lakeflow Jobs, SQL warehouses, and Lakeflow Spark Declarative Pipelines. This eliminates manual capacity planning whilst supporting fluctuating analytical workloads during peak demand periods or market events.  For storage, autoscaling monitors disk space across Spark workers and dynamically attaches managed storage up to 5 TB per virtual machine before capacity constraints occur. This prevents pipeline disruptions during high-volume data ingestion periods common in energy operations.  Billing follows actual compute usage, not provisioning time, supporting AGL's cost discipline. As Databricks improves infrastructure efficiency, these optimisations automatically flow through to reduced costs without requiring re-architecture.  This elastic approach supports AGL's data platform requirements whilst maintaining the cost predictability and operational resilience essential for critical energy operations.  	References: SQL warehouse sizing, scaling, and queuing behavior Autoscaling Best practices for reliability Architecting a High-Concurrency, Low-Latency Data Warehouse on Databricks That Scales	Full	Yes
RES.089	Data Warehouse	How do you optimize queries for performance and cost automatically?	Databricks automatically optimises queries through multiple AI-driven capabilities that require no configuration.  Predictive Optimisation uses AI to manage Unity Catalog tables automatically, running OPTIMIZE for file layouts, VACUUM to reduce storage costs, and ANALYZE for query planning statistics. The system intelligently determines which tables benefit most from maintenance, weighing performance gains against compute costs.  Photon engine provides vectorised query execution on serverless compute and SQL warehouses, accelerating SQL workloads with no code changes. Adaptive Query Execution performs runtime query re-optimisation, automatically switching join strategies, coalescing partitions, and handling data skew based on actual runtime statistics.  For serverless SQL warehouses, Intelligent Workload Management uses machine learning to predict query resource requirements, dynamically scale compute, and prioritise short-running queries. Automatic statistics collection during write operations improves query plan quality by an average of 22 per cent.  All optimisation improvements deploy automatically to existing workloads without version upgrades or configuration changes, supporting AGL's operational efficiency objectives and enabling data teams to focus on business outcomes rather than infrastructure tuning.  	References: Databricks SQL release notes 2025 Query caching documentation SQL warehouse sizing, scaling, and queuing behavior Architecting a High-Concurrency, Low-Latency Data Warehouse on Databricks Predictive Optimization Adaptive Query Execution	Full	Yes
RES.090	Data Warehouse	Describe your integration with BI and analytics tools.	Databricks integrates with BI and analytics tools through multiple connectivity methods, enabling AGL's data teams to serve business stakeholders across preferred platforms.  Native connectors for Power BI and Tableau provide enterprise-grade integration. The Power BI connector supports Import mode, DirectQuery, and composite models with one-click publishing from Catalog Explorer. Microsoft Entra ID integration preserves Unity Catalog permissions including row-level and column-level security. Tableau users can explore tables directly from Databricks and build dashboards in Tableau Cloud with OAuth authentication.  Industry-standard JDBC and ODBC drivers enable connectivity to any SQL-compatible tool including Qlik, Looker, Sigma, and MicroStrategy. Cloud Fetch technology delivers 10x faster data extraction through parallel transfer directly from cloud storage, eliminating single-threaded bottlenecks for large result sets.  Built-in AI/BI capabilities reduce dependency on external tools. AI/BI Genie provides conversational analytics using natural language, whilst AI/BI dashboards offer multi-page reporting with AI-assisted authoring and secure embedding. The Excel connector enables business users to query Databricks data directly with saved queries and refresh capabilities.  All integrations leverage serverless SQL warehouses with Intelligent Workload Management for automatic scaling, supporting efficient self-service analytics across the organisation.  	References: Databricks SQL release notes 2025 Databricks AI/BI Technology partners Power BI with Databricks	Full	Yes
RES.091	BI / Reporting	How do you support interactive dashboards and ad-hoc reporting?	Databricks delivers interactive dashboards and ad-hoc reporting through AI/BI capabilities running on serverless SQL warehouses, supporting AGL's Business Intelligence value driver with self-service analytics at scale.  AI/BI Dashboards provide low-code authoring with 15+ visualisation types, multi-page reports, and cross-filtering that enables users to click data points and dynamically filter related charts. Client-side filtering delivers near-instant interactivity for datasets up to 100,000 rows without re-executing queries. Dashboards support scheduled refreshes, automated PDF snapshots via email, and secure sharing with registered users including those outside the workspace.  Genie enables business users to ask ad-hoc questions in natural language and generate visualisations without SQL knowledge. Each dashboard includes a companion Genie space for follow-up analysis beyond predefined charts, democratising data access across AGL teams.  SQL Editor supports interactive querying with intelligent autocomplete, integrated Databricks Assistant for query optimisation, and real-time collaboration. Query results cache for 24 hours ensuring fast performance on repeated queries.  Serverless SQL warehouses provide sub-second startup times and automatic scaling for concurrent users. Unity Catalog ensures row-level and column-level security policies apply consistently across all dashboards and queries, maintaining governance as AGL scales self-service analytics.  Databricks Apps let you build and run secure custom Python or Node.js apps inside your workspace, leveraging Databricks-managed compute, data services, and Unity Catalog governance.	References: AI/BI Dashboards documentation Genie natural language interface SQL Editor documentation Next-Level Interactivity in AI/BI Dashboards Databricks SQL release notes 2025 What's New in AI/BI Dashboards Fall 2024 Faster AI/BI Dashboards with Materialized Views	Full	Yes
RES.092	BI / Reporting	Describe your scheduling and secure distribution capabilities for reports.	Databricks AI/BI dashboards provide comprehensive scheduling and secure distribution capabilities aligned with AGL's data governance requirements.  Automated Scheduling: Each dashboard supports up to 10 independent refresh schedules with flexible cadence configuration. Schedules execute queries automatically, populate shared result caches for performance, and distribute to up to 100 subscribers per schedule across email, Slack, and Microsoft Teams. Schedule management includes pause, resume, and delete controls that preserve subscriber configurations.  Secure Distribution: Email subscribers receive PDF snapshots. Slack and Teams channels receive PNG previews with PDF attachments and direct dashboard links. Publishers control data access through two modes: embed credentials for broad stakeholder access, or require individual credentials with Unity Catalog permissions enforced. This supports both wide communication and sensitive data protection.  Compliance and Audit: Workspace administrators configure distribution lists for external partners or disable email subscriptions workspace-wide for compliance. All dashboard access, scheduled executions, and deliveries are logged in audit logs. PDFs are automatically deleted after email delivery, with manual downloads retained for 60 days. Role-based permissions integrate with Microsoft Entra ID for automatic identity provisioning.  	References: Schedule and subscribe to AI/BI dashboards AI/BI dashboards AI/BI administration settings	Full	Yes
RES.093	BI / Reporting	What differentiates your approach to AI-driven insights in reporting?	Databricks delivers AI-driven insights through compound AI architecture, not generic LLMs added to traditional BI tools. AI/BI Genie employs specialised agents for planning, SQL generation, and visualisation that learn from your complete data lifecycle including ETL pipelines, Unity Catalog lineage, and query history. This contextual understanding produces more accurate insights than standalone language models.  For AGL's governance requirements, trusted assets provide certified answers marked as verified when using parameterised SQL queries or Unity Catalog functions, eliminating hallucination risks for governed logic. Metric views in Unity Catalog enable centralised definition of business KPIs once, automatically consumed across AI/BI Dashboards and Genie with consistent calculations enforced through row-level security and column masking.  The system continuously improves through knowledge mining from existing query patterns and Genie conversations, capturing AGL-specific business terminology and semantics without upfront modelling. Insights become more relevant to AGL's energy operations over time, supporting data-driven decision-making across customer vulnerability programs, firming capacity planning, and emissions reduction initiatives aligned with your Climate Transition Action Plan.  	References: AI/BI concepts Research Agent in Genie spaces What's New in AI/BI Dashboards - Fall '24 How Business Intelligence Drives Smart Decision-Making	Full	Yes
RES.094	Self-service / Ad-hoc Analysis	Explain your support for self-service data exploration with role-based access.	Databricks delivers governed self-service data exploration through Unity Catalog, which centralises role-based and attribute-based access controls across all data and AI assets on AWS and Azure.  Users discover data through Catalog Explorer, which surfaces assets they have permissions to access with AI-generated insights and usage patterns. Business users query data using natural language via AI/BI Genie, while technical users leverage SQL Editor and notebooks with integrated Databricks Assistant. All tools automatically respect Unity Catalog permissions.  Access control operates hierarchically using ANSI SQL GRANT statements to assign privileges at catalog, schema, table, or column levels. Permissions cascade from parent to child objects and are enforced consistently across all compute types. Fine-grained controls include row filters and column masks that restrict visibility based on group membership or user attributes. Attribute-based access control uses governed tags to define policies once, automatically applying to all current and future tables.  Users request access directly from Catalog Explorer, with notifications sent to asset owners via email or Slack, streamlining governance workflows.  The new Discover experience provides a curated, domain-organised marketplace of certified data and AI assets—augmented by AI recommendations and data‑steward curation—to help users quickly find high‑value, trusted resources.  Built‑in signals highlight certification, deprecation, and data quality, while Unity Catalog delivers lineage, auditing, and fine‑grained access controls to ensure governed use across the platform.	References: Announcing Public Preview of Request for Access in Unity Catalog Data discovery and collaboration in the lakehouse How Leading Companies Are Delivering Trusted, AI-Powered Self-Service Analytics 5 key lessons from implementing AI/BI Genie for self-service marketing insights	Full	Yes
RES.095	Self-service / Ad-hoc Analysis	How do you enable custom queries without coding for business users?	"Databricks enables no-code custom queries through AI/BI Genie, a conversational interface where business users ask questions in natural language and receive instant insights as text, tables, and visualisations.  Genie translates natural language into SQL automatically using a compound AI system. Data analysts configure Genie spaces with curated datasets and AGL-specific business terminology such as ""customers experiencing vulnerability"" or ""firming capacity"", ensuring contextually accurate responses. When Genie uses pre-approved parameterised queries or Unity Catalog functions, responses are marked as ""Trusted"" to indicate certified answers.  AI/BI Dashboards complement this capability with AI-assisted visualisation authoring where users describe charts in natural language. Each dashboard includes a companion Genie space, allowing viewers to ask follow-up questions directly without requesting new reports from data teams.  All interactions respect Unity Catalog row-level and column-level security, ensuring governed data access aligned with AGL's data governance framework. Business users can also upload local Excel or CSV files to blend with Unity Catalog data for ad-hoc analysis.  This capability directly supports AGL's goal of democratising data access while maintaining enterprise-grade governance and security across the organisation.  "	References: What is Genie AI/BI Genie Now Generally Available Databricks AI/BI Genie Magic in the Data: Data Curation for AI/BI Genie 5 key lessons from implementing AI/BI Genie for self-service marketing insights	Full	Yes
RES.096	Self-service / Ad-hoc Analysis	Describe your audit capabilities for self-service actions.	Databricks provides complete audit capabilities for self-service actions through Unity Catalog system tables with 365-day retention.  The system.access.audit table captures every user action including dashboard creation, viewing, sharing, query execution, and Genie space interactions. Each event records user identity, timestamp, source IP, request parameters, and response status. The system.query.history table logs all SQL queries with execution times and query text.  Unity Catalog audit logs track data access events including table reads, permission grants or revocations, and credential usage. Administrators can trace exactly who accessed which datasets and when. Lineage tables automatically capture relationships between notebooks, jobs, dashboards, and data assets.  All audit data is governed by Unity Catalog requiring explicit permissions to access. System tables can be queried using standard SQL and exported to external SIEM tools like Splunk or Azure Monitor for integration with AGL's enterprise monitoring workflows, supporting compliance requirements and security investigations.  	References: Audit log system table reference Audit log reference Best practices for data and AI governance Monitor AI/BI usage with audit logs and alerts	Full	Yes
RES.097	Data Marketplace	Describe your secure marketplace for internal and external data assets.	Databricks delivers secure data marketplace capabilities through Unity Catalog, supporting both internal discovery and external partner collaboration.  Internally, Unity Catalog Discover functions as a curated marketplace where business domains organise certified data products by area such as customer analytics, generation forecasting, or retail operations. AI-powered recommendations surface high-value assets including tables, metrics, dashboards, and AI agents, enriched with ownership, usage insights, and quality signals. Self-service access requests enable collaboration while maintaining governance controls.  For external sharing, Databricks Marketplace provides an open ecosystem, while Private Exchanges enable secure collaboration with specific partners including AEMO, retailers, and technology vendors. Delta Sharing, an open protocol, enables zero-copy data exchange across platforms and clouds without replication, reducing costs and security exposure.  Unity Catalog enforces centralised access control, auditing, and lineage tracking across all shared assets. Attribute-Based Access Control enables flexible, tag-based policies, while automated data classification detects and protects sensitive information like customer PII within 24 hours.  This architecture supports AGL's requirement for secure data collaboration across internal teams and external energy market participants, critical for operating within the National Electricity Market.  	References: Unity Catalog overview Unity Catalog Data AI Summit 2025 updates Databricks Marketplace overview Delta Sharing documentation Share data and AI assets securely Create and manage private exchanges Unlocking the Potential of Private Data Sharing using Databricks Private Exchanges	Full	Yes
RES.098	Data Marketplace	How do you enforce governance policies for marketplace assets?	Databricks enforces marketplace governance through Unity Catalog's centralised security model, combining privilege-based access control, fine-grained policies, and continuous audit monitoring.  Access control starts with the USE MARKETPLACE ASSETS privilege, enabled by default but revocable by metastore admins. Providers need Marketplace Admin role to publish listings, whilst consumers require CREATE CATALOG and USE PROVIDER privileges. All shared assets appear as read-only catalogs, preventing unauthorised modifications.  Fine-grained policies leverage Unity Catalog's native controls. Attribute-Based Access Control (ABAC, Public Preview) enables tag-driven policies with automatic inheritance, supporting dynamic row-level filtering and column-level masking based on data sensitivity. Providers can share dynamic views using CURRENTRECIPIENT() functions for multi-tenant scenarios, whilst recipients apply additional local governance policies as needed.  Delta Sharing protocol enforces provider-to-recipient access through standard SQL GRANT/REVOKE commands, with optional IP access lists and token lifetime controls for open sharing credentials.  Continuous audit monitoring tracks all marketplace activities through system.access.audit and system.marketplace.listingaccess_events tables (365-day retention), capturing share creation, recipient access patterns, query details, and user identities for compliance reporting.  	References: Attribute-Based Access Control (ABAC) Unity Catalog Privileges Delta Sharing Audit Logs System Tables Data Governance with Databricks Databricks Marketplace Provider Policies Unlocking the Potential of Private Data Sharing using Databricks Private Exchanges Unity Catalog Governance in Action - Monitoring, Reporting, and Lineage Best Practices for Data and AI Governance	Full	Yes
RES.099	Data Marketplace	What capabilities exist for monetization of data assets?	Databricks enables AGL to monetise data assets through two integrated capabilities: Databricks Marketplace and Delta Sharing.  Databricks Marketplace provides the commercial platform, hosting over 2,500 listings from 250+ providers. AGL can create public listings or Private Exchanges for controlled distribution to specific partners. Databricks charges no platform fees. The Provider Analytics Dashboard tracks listing views, requests, and installations, enabling AGL to identify leads and initiate licensing conversations directly with interested parties.  Delta Sharing delivers the technical foundation through an open protocol that enables zero-copy data sharing across any platform, cloud, or region without data replication. Approximately 40% of Delta Sharing connections reach non-Databricks platforms including Power BI, Tableau, and pandas, maximising AGL's addressable market. Unity Catalog controls access privileges and supports dynamic row and column filtering for multi-tenant scenarios where different consumers access different data subsets from the same asset.  The cost model includes only cloud egress charges when sharing across regions and Unity Catalog storage for managed data. No compute costs apply for sharing operations.  	References: What is Databricks Marketplace? Share data and AI assets securely Unlocking the Potential of Private Data Sharing using Databricks Private Exchanges Building High-Quality and Trusted Data Products with Databricks	Full	Yes
RES.100	Data Governance Workflow	Explain your policy-driven governance workflow architecture.	Unity Catalog implements policy-driven governance through a hierarchical three-level namespace (catalog.schema.table) where policies cascade automatically from parent to child objects. This inheritance model ensures consistent controls across AGL's expanding data estate without repetitive configuration as renewable energy operations, customer analytics, and grid optimisation workloads scale.  Unity Catalog attribute-based access control (ABAC) enables tag-driven policies that dynamically enforce row filters and column masks. When users query data, Unity Catalog evaluates applicable policies in real-time based on Governed Tags and user context, enforcing least-privilege access without manual per-table rules. This is critical for AGL's customer data containing vulnerability indicators and commercially sensitive pricing information.  Governed Tags provide account-level classification with enforced policies controlling tag application and permitted values. These integrate with Data Classification for AI-powered sensitive data discovery and automated tagging.  All policy enforcement is backed by automated user-level audit logs and column-level lineage captured through system tables. This provides end-to-end visibility for compliance reporting, impact analysis, and root-cause diagnostics across workspaces sharing the metastore.  	References: Data governance with Databricks Best practices for data and AI governance Data and AI governance for the data lakehouse Attribute-Based Access Control in Unity Catalog Governed Tags	Full	Yes
RES.101	Data Governance Workflow	How do you implement approval and escalation mechanisms in workflows?	Databricks implements approval and escalation mechanisms through native Lakeflow Jobs capabilities.  Approval mechanisms include MLflow deployment jobs providing human-in-the-loop approvals for ML model lifecycles. Approvers review evaluation metrics before authorising progression. Unity Catalog governed tags enforce approval gates, preventing task execution until designated teams set approval tags. This supports multi-party approval workflows where Legal and Security teams independently approve before deployment, preventing self-approval by model owners.  Conditional task logic enables approval gates through If/else tasks and Run if dependencies, allowing workflows to branch based on validation outcomes or require specific conditions before proceeding.  Escalation mechanisms include multi-tier notifications to email, Slack, Microsoft Teams, PagerDuty, or webhooks for job events. Up to three destinations per event type enable routing to different escalation tiers. Duration-based thresholds trigger proactive notifications before failures occur.  Webhook integration with JIRA, ServiceNow, and custom systems enables automated ticket creation via structured JSON payloads containing job context.  For AGL's energy data workflows, these mechanisms support governance requirements for critical pipelines affecting customer-facing systems and regulatory reporting under TCFD (Task Force on Climate-related Financial Disclosures) and SASB (Sustainability Accounting Standards Board) frameworks.  	References: MLflow deployment jobs documentation Job notifications documentation Run if dependencies documentation Governed tags documentation	Full	Yes
RES.102	Data Governance Workflow	Describe your customization options for governance workflows.	Unity Catalog provides native governance workflow customisation without third-party tools, enabling AGL to adapt data governance as regulatory and business requirements evolve under CTAP.  Three core capabilities deliver flexible governance:  1. Attribute-Based Access Control (ABAC, Public Preview) enables tag-driven policies defined once at catalog or schema level with automatic inheritance. Custom user-defined functions support row filtering and column masking based on AGL-specific business rules, allowing governance that reflects operational context rather than rigid role hierarchies.  2. Governed tag policies enforce standardised metadata workflows with account-level control over tag keys, values, and assignment permissions. This supports automated classification and operational automation, streamlining compliance workflows for TCFD, SASB, and GRI reporting requirements.  3. Access request workflows in Unity Catalog  (Public Preview) integrate with existing collaboration tools through configurable approval destinations including email, Slack, Microsoft Teams, webhooks, or external systems like JIRA or ServiceNow. This enables self-service data discovery with appropriate oversight.  All governance configurations support Infrastructure-as-Code deployment via REST API, Databricks CLI, SDKs, and Terraform, ensuring policies are version-controlled, auditable, and consistently deployed across environments.  	References: Attribute-Based Access Control in Unity Catalog Configure access request destinations Automate Unity Catalog with Terraform Best practices for data and AI governance Data governance with Databricks	Full	Yes
RES.103	Data Wrangling	How do you support interactive data wrangling for large datasets?	Databricks supports interactive data wrangling at scale through three integrated capabilities that enable AGL's data teams to explore energy consumption patterns, customer behaviour, and operational datasets efficiently.  1. Collaborative notebooks provide real-time coauthoring in Python, SQL, Scala, and R with built-in data profiling, interactive visualisations, and result set filtering. Teams can explore datasets and generate summary statistics using low-code, visual tools in the results table without modifying the underlying code, enabling rapid iteration on customer segmentation or demand forecasting analysis.  2. DBSQL serverless warehouses deliver 2-6 second startup times with intelligent autoscaling and Photon query engine acceleration, optimised for exploratory analysis and business intelligence workloads. Automatic disk caching on SSD-backed workers enables faster repeated access to terabyte-scale datasets, critical for analysing historical energy market data or customer usage patterns.  3. Databricks Assistant accelerates exploration through natural language, generating SQL queries and Python code from business questions, creating visualisations on demand, and providing AI-powered debugging to streamline workflows for analysts across the organisation.  Performance at scale combines Photon vectorised execution delivering 3-8x faster queries, automatic disk caching, and intelligent data skipping to enable sub-second interactive responses on terabyte-scale datasets. All features are enabled by default on serverless compute with no manual tuning required.  	References: Databricks Notebooks SQL Warehouse Types Databricks Assistant FAQ Exploratory Data Analysis on Databricks Databricks SQL Performance Improvements	Full	Yes
RES.104	Data Wrangling	Describe your version control and reusability features for wrangling steps.	Databricks provides version control and reusability for data wrangling through three core capabilities that align with AGL's governance and change management requirements.  1. Databricks Asset Bundles enable infrastructure-as-code where transformation logic, pipelines, and dependencies are defined as version-controlled YAML configurations. Teams write wrangling steps once and deploy across dev-staging-prod environments using parameterised variables for environment-specific settings like catalogs, compute, and schedules. This eliminates manual errors and ensures consistent deployments.  2. Git integration through Git Folders provides native commit-push-pull operations with visual diff comparison. Teams collaborate via feature branches with automated CI/CD triggers, whilst automatic notebook revision history maintains point-in-time snapshots for rollback.  3. Liquibase Connector brings database schema version control to the lakehouse. DDL changes are tracked as versioned migration scripts with automated deployment, rollback capabilities, and audit trails critical for AGL's compliance processes.  Reusability is achieved through parameterisation where common wrangling steps in notebooks or Lakeflow Spark Declarative Pipelines are referenced across bundles. Custom templates enforce organisational standards for data quality checks and naming conventions, ensuring consistency across AGL's data engineering teams.  	References: Databricks Asset Bundles documentation Best practices and recommended CI/CD workflows on Databricks Databricks schema versioning with Flyway and Liquibase Announcing the General Availability of Databricks Asset Bundles	Full	Yes
RES.105	Data Wrangling	What ML-driven wrangling suggestions do you offer?	"Databricks delivers four ML-driven wrangling capabilities that accelerate AGL's data transformation work.  Databricks Assistant (GA) generates SQL and Python code from natural language. Data engineers describe tasks like ""flatten nested JSON meter readings and aggregate by customer"" and receive executable code leveraging Unity Catalog schemas. In Edit mode, the Assistant proposes notebook‑wide refactors and updates across multiple cells from a single prompt; you can review and accept or reject each change (or Accept All) before applying them.   AI/BI Genie (GA) enables business users to wrangle data conversationally. Questions like ""calculate average consumption excluding estimated reads"" translate into optimised SQL with appropriate joins and aggregations. Genie learns AGL's organisational terminology to improve accuracy.  Assistant Autocomplete (GA) provides real-time inline suggestions as analysts type, understanding Databricks-specific patterns including Delta Lake operations and DataFrame transformations. The model uses context from neighbouring cells, runtime schemas and Unity Catalog metadata.  Data Science Agent (Beta) automates automates multi‑step workflows from a single prompt. Users prompt ""perform exploratory analysis on smart meter data and clean anomalies"" and the agent plans steps, generates code, executes transformations, interprets results and fixes errors automatically.  All capabilities integrate with Unity Catalog to maintain AGL's governance standards while accelerating transformation work.  "	References: Databricks Assistant FAQ AI/BI Genie documentation Announcing General Availability of Databricks Assistant Autocomplete Exploratory data analysis on Databricks Tools and techniques	Full	Yes
RES.106	Model Training	Explain your architecture for distributed training of large-scale models.	Databricks distributed training architecture separates orchestration, compute, and data concerns for scalability and flexibility.  For orchestration, TorchDistributor and DeepSpeed integrate with PySpark to support Data Parallel, Fully Sharded Data Parallel, and ZeRO optimisation across single-node multi-GPU and multi-node configurations. Ray on Databricks provides specialised parallel compute for complex workflows, supporting PyTorch, TensorFlow, and HuggingFace with Ray Tune for hyperparameter optimisation.  Serverless GPU Compute eliminates manual cluster configuration while supporting A10 and H100 accelerators. Multi-GPU training works on both types, with multi-node training available on A10s. This reduces operational overhead for data science teams.  The data layer uses Mosaic StreamingDataset for optimised distributed loading with prefetching and interleaving, purpose-built for multi-node training. This maintains correctness guarantees across large datasets stored in Unity Catalog.  MLflow automatically tracks experiments across distributed runs, while Unity Catalog governs training data and resulting models. GPU-aware scheduling distributes tasks efficiently across available accelerators.  For AGL, this architecture enables faster iteration on energy demand forecasting and customer behaviour models critical to renewable generation planning and firming capacity optimisation, while maintaining governance and cost control through serverless infrastructure.  	References: Databricks Runtime 16.4 LTS for Machine Learning Distributed training with DeepSpeed distributor Prepare data for distributed training AI and machine learning on Databricks Distributed Data Parallel (DDP) training	Full	Yes
RES.107	Model Training	How do you monitor and optimize training jobs automatically?	Databricks automates training job monitoring through integrated tooling that reduces manual overhead for AGL's data science teams.  MLflow Autologging captures parameters, metrics, and artefacts automatically from scikit-learn, PyTorch, TensorFlow, XGBoost, and LightGBM without code changes. Every training session becomes an auditable MLflow run, creating complete lineage for model development.  Hyperparameter optimisation runs automatically via Optuna, Ray Tune, and Hyperopt. These frameworks parallelise tuning across cluster workers, log experiments to MLflow, and use Bayesian optimisation to search parameter spaces efficiently. This accelerates time-to-optimal-model for AGL's forecasting and customer analytics workloads.  System tables track job runs, compute resources, and training metrics across workspaces. Teams build SQL dashboards for cost monitoring, performance tracking, and failure alerts without additional instrumentation.  Real-time monitoring includes TensorBoard for training visualisation and cluster metrics showing CPU, GPU, memory, and Spark execution with sub-minute latency. For production models, inference tables log serving endpoint requests to Unity Catalog Delta tables, enabling automated retraining workflows and drift detection through Lakeflow Jobs orchestration.  This integrated approach supports AGL's data-driven decision-making while minimising operational complexity.  	References: MLflow Autologging Hyperparameter tuning with AutoML System tables for jobs MLOps workflows on Azure Databricks Best practices for performance efficiency Forecasting with AutoML Supercharging AI Model Building with Ray and Databricks	Full	Yes
RES.108	Model Training	Describe your integration with GPU/TPU resources.	Databricks provides native integration with NVIDIA GPU resources on AWS and Azure, supporting multiple instance types including H100, A100, A10, T4, and V100 for both single-node and distributed training scenarios.  Databricks Runtime ML pre-configures NVIDIA drivers, CUDA Toolkit, cuDNN, and TensorRT, eliminating manual setup and compatibility issues. GPU-aware scheduling from Apache Spark automatically distributes tasks across available GPUs, with configurable parameters to optimise for distributed training or parallel inference workloads.  Serverless GPU Compute, currently in Public Preview, provides on-demand access to GPU accelerators without cluster management overhead. This supports custom deep learning workloads with automatic scaling and integration with Notebooks, Unity Catalog, and MLflow.  TPU integration is exclusive to Databricks on Google Cloud Platform as TPUs are Google's proprietary accelerators. For AWS and Azure deployments, NVIDIA GPU instances provide the hardware acceleration required for AI and ML workloads.  	References: Serverless GPU compute Distributed Data Parallel (DDP) training Create custom model serving endpoints Accelerated DBRX Inference on Agent Bricks Model Serving	Full	Yes
RES.109	Model Execution / Hosting	How do you support low-latency model inference at scale?	Databricks Agent Bricks Model Serving delivers production-grade, low-latency inference for AGL's customer-facing applications and real-time decision systems.  The platform handles over 25,000 queries per second with sub-50 millisecond overhead latency using serverless compute that automatically scales based on demand. For high-throughput workloads, route optimisation achieves 50,000+ QPS per workspace. GPU acceleration (GPUSMALL, GPUMEDIUM, GPU_LARGE workload types) supports deep learning models and LLMs, while provisioned throughput Foundation Model APIs guarantee consistent performance for production GenAI applications.  Endpoints support configurable provisioned concurrency with autoscaling, calculated as Required Concurrency = Target QPS × Average Latency, with defaults up to 200 concurrent requests (expandable on request). Native integration with Databricks Online Feature Stores provides millisecond-latency feature access, eliminating manual lookups.  Models are exposed as REST APIs with unified MLflow interfaces, OAuth authentication, and comprehensive monitoring through inference tables and MLflow AI Gateway for governance. This architecture ensures AGL can deliver responsive, data-driven customer experiences while maintaining enterprise security and observability standards.  	References: Deploy models using Agent Bricks Model Serving Optimise Model Serving endpoints for production Create custom model serving endpoints Load testing for serving endpoints Fast, Secure and Reliable: Enterprise-grade LLM Inference	Full	Yes
RES.110	Model Execution / Hosting	Describe your deployment strategy for multi-cloud and hybrid environments.	Databricks delivers portable, governance-first deployment across cloud and hybrid environments, preventing vendor lock-in while maintaining consistent security and compliance.  Three core capabilities enable this. First, open standards through MLflow packaging. Models deploy unchanged across AWS, Azure, GCP, on-premises Kubernetes, or edge devices. Your models trained on AWS today serve from Azure or hybrid infrastructure tomorrow without refactoring.  Second, unified governance via Unity Catalog. Centralised access control, auditing, and lineage tracking apply across all deployment targets. Models registered once are accessible across workspaces with policies enforced consistently, whether serving from AWS Sydney, Azure Melbourne, or on-premises infrastructure.  Third, flexible serving patterns. Agent Bricks Model Serving provides REST APIs for serverless endpoints in any supported region. MLflow packaging enables deployment to third-party platforms like AWS SageMaker or custom containers. For hybrid scenarios, external tables access on-premises data under Unity Catalog governance, and Delta Sharing replicates models across environments without data duplication.  This approach supports AGL's current AWS foundation while preserving optionality for future multi-cloud or hybrid requirements driven by regulatory, resilience, or commercial considerations. Models remain portable assets, not cloud-locked liabilities.  	References: MLOps workflows on Databricks How does Databricks support CI/CD for machine learning? Optimize Model Serving endpoints for production Fast, Secure and Reliable: Enterprise-grade LLM Inference	Full	Yes
RES.111	Model Execution / Hosting	What autoscaling capabilities exist for inference workloads?	Databricks Model Serving provides multiple autoscaling patterns for inference workloads, optimising compute costs while maintaining service reliability.  Serverless endpoints scale automatically based on real-time traffic. The platform adjusts provisioned concurrency in increments of one concurrent requests, scaling up instantly during peak demand and reducing capacity as traffic decreases. You configure minimum and maximum concurrency thresholds to balance cost and performance.  Scale-to-zero reduces endpoints to zero capacity after 30 minutes of inactivity, certain optimized compute tiers allow for shorter inactivity timeouts (as low as 60 seconds) to further reduce costs in development environments. , ideal for development environments. Cold-start latency is 10-20 seconds, making this unsuitable for latency-sensitive production workloads requiring guaranteed availability.  Foundation Model APIs with provisioned throughput scale automatically in Model Units, with rapid GPU compute adjustments. You pay only for capacity actively used during each billing period.  Batch inference workloads scale infrastructure automatically without configuration, dynamically provisioning multiple throughput bands to process large datasets with built-in fault tolerance and automatic retries.  Workload sizing options allow appropriate scaling ranges: Small (0-4 concurrent requests), Medium (8-16), Large (16-64). Custom configurations support high-throughput requirements exceeding 200 queries per second and enabling Route Optimization during endpoint creation can reach 50,000+ QPS  	References: Optimize Model Serving endpoints for production Autoscaling Create foundation model serving endpoints Fast, Secure and Reliable: Enterprise-grade LLM Inference	Full	Yes
RES.112	Feature Store	Explain your centralized feature store architecture for ML models.	Databricks centralises feature management through Unity Catalog integration. Any Delta table with a primary key can be treated as a feature table, unifying data and feature governance under a single platform. This supports AGL's ML operations across energy forecasting, customer analytics, and operational models with consistent governance.  The Feature Engineering Client enables batch and streaming feature computation using standard Spark pipelines. For real-time applications like demand forecasting or customer experience optimisation, Databricks Online Feature Stores (Public Preview) deliver millisecond-latency feature access through Lakebase, with automatic synchronisation from offline tables.  Core capabilities include automatic lineage tracking from raw data through features to models, cross-workspace feature sharing for collaboration between teams, and automatic feature lookup at inference time. Models trained with features automatically retrieve correct values during both batch scoring and real-time serving, reducing deployment errors.  Feature Serving endpoints expose features to external applications via REST APIs with Unity Catalog authentication, enabling integration with AGL's existing systems while maintaining security standards.  This architecture accelerates AGL's ML development cycles while ensuring features remain governed, reusable, and traceable across the organisation.  	References: Databricks feature engineering and legacy Workspace Feature Store release notes Announcing Lakebase Public Preview Use features to train models MLOps workflows on Azure Databricks	Full	Yes
RES.113	Feature Store	How do you ensure versioning and reuse of features across models?	Databricks ensures feature versioning and reuse through Unity Catalog's integration with Delta Lake's native versioning capabilities.  Feature tables are Delta tables that automatically create immutable versions for each modification. Time-travel queries retrieve historical feature values for model reproducibility and compliance auditing. Models logged using FeatureEngineeringClient automatically retain references to exact feature table versions used during training, eliminating version drift between training and inference. For time-series features, point-in-time lookups ensure models access feature values as they existed at specific timestamps, preventing data leakage.  Feature reuse is enabled through Unity Catalog's central registry where any Delta table with a primary key can be used as a discoverable feature table accessible across workspaces. The Features UI provides search by feature name, table, or tags. Automatic lineage tracking shows which models consume which features, enabling impact analysis before changes. FeatureSpecs define reusable feature sets that can be referenced across multiple models.  For cross-account sharing, Delta Sharing enables secure feature table distribution while maintaining governance and lineage, supporting multi-team collaboration requirements.  	References: Databricks feature engineering and legacy Workspace Feature Store release notes Work with feature tables in Unity Catalog Use features to train models Feature store overview and glossary What is a Feature Store? A Complete Guide to ML Feature Engineering	Full	Yes
RES.114	Feature Store	Describe your support for real-time feature updates.	Databricks supports real-time feature updates through streaming pipelines that continuously synchronise features from offline Delta tables to Online Feature Stores, enabling low-latency access for production inference workloads.  Feature tables stored as Delta tables in Unity Catalog leverage Change Data Feed to track row-level changes. When publish_table is called with streaming=True, a continuous pipeline propagates changes from offline storage to the Online Feature Store, typically achieving latency in the low seconds range depending on data volume and complexity.  Streaming feature computation pipelines consume real-time data sources via Structured Streaming and write updates to feature tables using fe.writetable. For batch scenarios requiring periodic freshness, scheduled Lakeflow Jobs run publish_table to incrementally refresh online features on configurable intervals.  Models served through Model Serving endpoints look up current feature values from Online Feature Stores, reducing training-serving skew. This capability supports AGL's customer-facing applications requiring fresh data for personalisation, demand forecasting, or operational decision-making aligned with your Business Intelligence value driver.  Databricks has announced that Tecton will join the platform to enhance real-time data serving capabilities. Integration timelines and specific enhancements will be communicated as the acquisition progresses.  	References: Databricks feature engineering and legacy Workspace Feature Store release notes Feature store overview and glossary Model Serving with automatic feature lookup How Do Real-Time Features Work in Machine Learning Announcing the General Availability of Databricks Feature Serving	Full	Yes
RES.115	LLM Tuning	How do you support fine-tuning of large language models with enterprise data?	Databricks supports fine-tuning large language models through Agent Bricks Model Training, currently in Public Preview on AWS and Azure US regions. This enables AGL to create domain-specific models using proprietary energy sector data whilst maintaining complete data sovereignty.  Three fine-tuning approaches are available. Chat completion optimises conversational models, instruction fine-tuning handles structured prompt-response scenarios, and continued pre-training embeds domain knowledge from raw text. Supported models include Meta Llama 3.3 70B, Llama 3.2 (1B and 3B), and Llama 3.1 (8B and 70B).  Training data remains in Unity Catalog with encryption at rest and in transit, full lineage tracking, and governance controls throughout the model lifecycle. The platform automatically provisions GPUs, checkpoints models to MLflow, and registers fine-tuned models to Unity Catalog for one-click deployment via Agent Bricks Model Serving.  AGL retains complete ownership of fine-tuned model weights. The optimised infrastructure delivers up to 2x faster training than open source alternatives, accessible via Python SDK or UI, scaling from thousands of examples to billions of tokens.  	References: Agent Bricks Model Training documentation Databricks release notes Introducing Agent Bricks Model Training blog Foundation Model APIs supported models Characterizing Datasets and Building Better Models with Continued Pre-Training LLMOps workflows on Azure Databricks	Full	Yes
RES.116	LLM Tuning	Describe your parameter-efficient tuning techniques.	"Databricks supports parameter-efficient fine-tuning through custom implementation using open-source libraries. While Agent Bricks Model Training provides full-parameter fine-tuning, organisations can implement Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA) using Hugging Face PEFT on Databricks GPU compute.  LoRA fine-tunes less than 1% of model parameters by training small adapter matrices, significantly reducing GPU memory and training time whilst maintaining quality comparable to full fine-tuning. QLoRA adds 4-bit quantisation, enabling fine-tuning of 70B parameter models on single A100 GPUs.  Databricks provides notebook examples with configurable rank parameters and integration with Hugging Face Transformers, bitsandbytes for quantisation, and DeepSpeed for distributed training. All custom fine-tuning workflows integrate with MLflow for experiment tracking and Unity Catalog for model governance and lineage.  For AGL’s forecasting and customer analytics, this ""custom-governed"" approach offers the best of both worlds: the memory efficiency of parameter-efficient tuning (enabling the use of smaller, cheaper GPU instances) with the enterprise security of the Databricks Data Intelligence Platform  "	References: Efficient Fine-Tuning with LoRA: A Guide to LLMs Large language models (LLMs) on Databricks Foundation Model Training LIMIT: Less Is More for Instruction Tuning Serving Quantized LLMs on NVIDIA H100 Tensor Core GPUs	Full	Yes
RES.117	LLM Tuning	What monitoring and optimization capabilities exist for tuning jobs?	Databricks provides comprehensive monitoring and optimisation capabilities for fine-tuning jobs through MLflow integration and the Foundation Model Fine-tuning service.  Real-time monitoring is available via the get_events() API and MLflow Experiments UI, displaying training progress with automated metrics including training loss, LanguageCrossEntropy, LanguagePerplexity, and TokenAccuracy. Training and evaluation metrics are automatically visualised in the Charts tab, with loss tracking as the primary indicator to detect overfitting. You can specify up to 10 evaluation prompts to monitor model response quality throughout training, with outputs logged at each checkpoint.  Hyperparameter optimisation supports configurable learning rate and training duration through UI or API. All models train using the AdamW optimiser with learning rate warmup. The platform tracks all parameters, metrics, artefacts, and code versions in MLflow for comparison and iteration.  Infrastructure optimisation includes automatic GPU provisioning with fast startup times, periodic checkpoint saving to MLflow artefacts for resumption, and automatic model registration to Unity Catalog with full lineage tracking. Agent Bricks Agent Evaluation enables post-training model comparison using LLM-as-a-Judge techniques. System tables track resource consumption and costs across all fine-tuning workloads.	Fine-tune a foundation model tutorial: https://learn.microsoft.com/en-us/azure/databricks/large-language-models/foundation-model-training/fine-tune-run-tutorial View and manage fine-tuning runs: https://learn.microsoft.com/en-us/azure/databricks/large-language-models/foundation-model-training/view-manage-runs MLflow tracking documentation: https://docs.databricks.com/aws/en/mlflowracking Hyperparameter tuning: https://docs.databricks.com/aws/en/machine-learning/automl-hyperparam-tuning/ Scorers and LLM judges: https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/concepts/scorers	Full	Yes
RES.118	LLM Model Services	Explain your APIs for hosting and serving LLMs securely.	Databricks provides secure LLM hosting through Agent Bricks Model Serving with unified REST APIs, MLflow Deployment APIs, and OpenAI-compatible endpoints for real-time and batch inference.  Three deployment options support different use cases: Foundation Model APIs offer pay-per-token pricing for experimentation with preconfigured endpoints for Meta Llama, DBRX, Anthropic Claude, OpenAI GPT, and Google Gemini. Provisioned Throughput delivers production-grade performance guarantees and supports fine-tuned custom models. External Models centralise governance of third-party LLM providers with credential management and rate limiting. All endpoints scale to over 25,000 queries per second with sub-50ms overhead latency.  Security controls include workspace-level authentication and authorisation, AES-256 encryption at rest and TLS 1.2+ in transit, logical isolation between customer requests, IP Access Lists and Private Link for ingress control, and network policies for serverless egress. Foundation Model APIs operate as stateless workspace APIs with no data persistence. For paid accounts, user inputs are not used to train models or improve services.  Unity Catalog integration provides centralised governance with permissions management, usage tracking, and lineage. Agent Bricks Gateway extends this with rate limiting, payload logging, and guardrails across all model types, supporting AGL's requirements for secure, governed AI deployment at scale.  	References: December 2025 Release Notes - OpenAI GPT-5.2 now available as a Databricks-hosted model January 2025 Release Notes - Databricks-hosted models on Google Cloud Platform Databricks-hosted foundation models available in Foundation Model APIs Santalucia Seguros - Agent Bricks Model Serving for third-party model credential management July 2023 Release Notes - Customer-managed keys for encryption	Full	Yes
RES.119	LLM Model Services	How do you implement autoscaling for LLM inference workloads?	Databricks provides built-in autoscaling for LLM inference through Agent Bricks Model Serving with intelligent dual-window monitoring.  The platform uses a 20-second stable window for baseline traffic patterns and a 4-second panic window for demand spikes. Scale-up occurs immediately when traffic increases, ensuring consistent performance during peak periods. Scale-down follows a conservative approach, reducing capacity gradually after sustained demand decreases.  Serverless endpoints support optional scale-to-zero after 30 minutes of inactivity, reducing costs to zero during idle periods with 10-20 second cold-start recovery. For foundation models, provisioned throughput autoscaling adjusts capacity based on tokens-per-second demand within configured minimum and maximum ranges.  All infrastructure management, GPU orchestration, and capacity planning are fully automated. The platform integrates with Unity Catalog for cost attribution and governance, enabling AGL to track inference costs across business units whilst maintaining sub-50ms overhead latency at scale.  	References: Introducing Simple, Fast, and Scalable Batch LLM Inference on Agent Bricks Model Serving Fast, Secure and Reliable: Enterprise-grade LLM Inference Conduct your own LLM endpoint benchmarking Distributed LLM batch inference	Full	Yes
RES.120	LLM Model Services	Describe your integration options for external applications.	Databricks provides four primary integration pathways, all governed through Unity Catalog for security and compliance.  Lakebase delivers fully managed PostgreSQL-compatible databases with standard PostgreSQL drivers and REST APIs. This enables AGL's customer-facing applications to access curated lakehouse data with transactional consistency and low-latency queries. Synced tables support continuous streaming or scheduled batch synchronisation from Unity Catalog to Postgres.  Model Serving exposes REST APIs through OpenAI-compatible interfaces for high-throughput inference workloads. All endpoints authenticate via OAuth and enforce Unity Catalog permissions, supporting AGL's regulatory requirements.  Online Feature Stores provide low-latency feature serving through REST APIs, enabling real-time use cases such as fraud detection and personalisation engines to consume features via standard HTTP interfaces.  Model Context Protocol integration (currently in beta) enables standardised connections with external AI assistants, exposing Unity Catalog functions, Vector Search, and AI/BI Genie spaces through managed REST endpoints.  All integration options maintain consistent governance with authentication, authorisation, and audit logging managed centrally through Unity Catalog.  	References: Online Feature Store documentation Databricks OLTP (Lakebase) documentation Model Context Protocol integration December 2025 Release Notes - OpenAI GPT-5.2 Model Serving External models in Agent Bricks Model Serving September 2025 Release Notes - Route-optimized serving endpoints	Full	Yes
RES.121	Agentic Workflow	How do you orchestrate multi-step agent workflows with dynamic decision-making?	Databricks orchestrates multi-step agent processes through the Multi-Agent Supervisor pattern within Agent Bricks Agent Framework. The supervisor coordinates specialised agents using LLM-powered task delegation to dynamically route queries, manage interactions, and synthesise results without manual workflow coding.  The architecture combines Agent Bricks Agent Bricks: Knowledge Assistant for document retrieval, Unity Catalog functions for business logic, and external integrations. When a complex query arrives, the supervisor analyses requirements and delegates to appropriate sub-agents in parallel or sequence based on dependencies.  For AGL, this enables coordinating processes across energy operations, customer systems, and regulatory reporting. A query about customer energy usage triggers parallel processes: retrieving customer data, analysing consumption patterns, checking tariff eligibility, and generating compliance reports, all orchestrated automatically.  Unity Catalog integration ensures users only access sub-agents matching their permissions, with automatic conversation redirection when restrictions apply. This governance protects AGL's customer data and maintains regulatory compliance.  MLflow tracing provides end-to-end observability across multi-step processes. The Review App enables subject matter experts to provide feedback for continuous improvement, allowing AGL's teams to monitor and optimise agent performance across operational scenarios.  	References: Agent system design patterns Multi-Agent Supervisor Architecture: Orchestrating Enterprise AI at Scale Concepts: Generative AI on Databricks Build an Autonomous AI Assistant with Agent Bricks Agent Framework	Full	Yes
RES.122	Agentic Workflow	Describe your monitoring and audit capabilities for agent workflows.	Databricks delivers enterprise-grade monitoring and audit capabilities through three integrated layers.  Real-time observability uses MLflow 3 Tracing to capture inputs, outputs, intermediate steps, tool calls, and metadata for every agent interaction with sub-10 second latency. All traces automatically log to MLflow experiments and archive to Unity Catalog Delta tables for long-term analysis.  Production quality assessment runs continuously through configurable LLM judges and custom scorers on sampled traffic, evaluating correctness, safety, groundedness, and guideline adherence. Assessment results attach as feedback to traces within 15-30 minutes, enabling proactive quality management.  Comprehensive audit trails maintain complete governance through inference tables logging all serving requests and responses to Unity Catalog Delta tables, including HTTP status codes, execution duration, requester identity, and complete MLflow traces. The system.access.audit table provides 365-day retention of all workspace and account-level events including agent deployments, endpoint queries, and resource access with detailed timestamps and request parameters.  All monitoring data integrates with Unity Catalog for unified governance, role-based access control, and lineage tracking across development and production environments, supporting transparent operations and compliance requirements.  	References: MLflow 3 for GenAI Generative AI app developer workflow The key to production AI agents Evaluations Concepts Generative AI on Azure Databricks	Full	Yes
RES.123	Agentic Workflow	What integrations exist for external APIs and tools in agent workflows?	Databricks provides governed integration for external APIs and tools through Model Context Protocol (MCP) and Unity Catalog HTTP connections, ensuring secure credential management and consistent access controls.  MCP enables agents to discover and execute tools automatically. Managed MCP servers expose Unity Catalog functions, Vector Search indexes, and DBSQL as standardised tools. External MCP servers connect to third-party services from Databricks Marketplace (GitHub, JIRA, Confluence, Slack) or custom implementations via Unity Catalog HTTP connections with automatic OAuth flows and token refresh. Organisations can host proprietary MCP servers as Databricks Apps to expose custom business logic with built-in governance.  Unity Catalog HTTP connections provide secure credential management for external APIs, supporting OAuth 2.0 (user-to-machine and machine-to-machine) and bearer token authentication. This enables agents to call REST APIs for services like Microsoft Teams, Google Calendar, and Azure AI Search while enforcing Unity Catalog permissions.  Tools created once work with any agent framework (LangChain, OpenAI SDK, LangGraph) and can be reused across internal agents and third-party AI assistants.  	References: Integrate Unity Catalog tools with third party generative AI frameworks AI agent tools Agent system design patterns	Full	Yes
RES.124	LLM Application	Explain your support for building custom applications powered by LLMs.	Databricks provides end-to-end native capabilities for building, deploying, and governing custom LLM applications across AWS and Azure environments.  For development, Agent Bricks Agent Framework (GA) enables production-quality AI agents in Python using any library including LangChain, LangGraph, LlamaIndex, or custom code, with MLflow 3 integration for lifecycle management and Unity Catalog governance. AI Playground offers no-code prototyping to test models and build tool-calling agents with exportable production code. Agent Bricks Agent Bricks (Beta) automatically constructs optimised agent systems for common patterns like Knowledge Assistant RAG chatbots, Information Extraction, and Multi-Agent Supervisor workflows.  For deployment, Model Serving provides scalable REST APIs with automatic credential management, whilst Databricks Apps (GA) supports custom UIs using Streamlit, Gradio, React, or other frameworks on serverless compute with built-in OAuth and Git-based CI/CD.  For AGL specifically, this enables rapid development of customer-facing applications for billing enquiries and outage notifications, plus internal tools for regulatory compliance extraction and operational analytics. Vector Search automatically indexes unstructured data from Delta tables with row-level security, ensuring sensitive customer information remains protected. Foundation model access through pay-per-token APIs (Llama, Claude, GPT, Gemini) provides flexibility whilst AI Gateway ensures unified governance across all model calls.  	References: https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/build-genai-apps https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-apps/ https://learn.microsoft.com/en-gb/azure/databricks/release-notes/product/ https://learn.microsoft.com/en-us/azure/databricks/machine-learning/mlops/llmops https://learn.microsoft.com/en-us/azure/databricks/generative-ai/guide/agent-system-design-patterns https://docs.databricks.com/aws/en/generative-ai/guide/concepts/ https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset https://www.databricks.com/blog/state-ai-enterprise-adoption-growth-trends	Full	Yes
RES.125	LLM Application	How do you enable multi-modal input (text, image, audio) in applications?	Databricks supports multi-modal input through Foundation Model APIs and AI Functions, enabling text, image, and document processing in production applications today.  Vision capabilities process images (JPEG, PNG) alongside text using the aiquery SQL function. Models handle up to 500 images per request for real-time and batch inference scenarios.  Document parsing via aiparsedocument extracts structured content from PDF, Office documents, and images as a single SQL function. The parser preserves layout metadata including tables, headers, and page structure, with optional image rendering to Unity Catalog volumes for retrieval-augmented generation workflows.  Multi-modal embeddings enable semantic search across text and images using models like Cohere Multimodal Embed and CLIP. These embeddings integrate with Vector Search indexes, allowing applications to retrieve images based on textual descriptions without manual tagging.  Audio transcription using Whisper models is planned for Q3 2025, enabling batch audio-to-text conversion for call centre analysis and content processing.  All capabilities integrate with Unity Catalog for governance, MLflow for observability, and Databricks Apps for deployment. Data processing occurs within the Databricks security perimeter with HIPAA compliance available.  	References: Query vision models aiparsedocument function aiquery function October 2025 Release Notes - Multimodal support Introducing Meta Llama 3.2 on Databricks Chat with LLMs and AI Playground Concepts: Generative AI on Databricks	Full	Yes
RES.126	LLM Application	Describe your deployment strategy for LLM applications across environments.	Databricks provides enterprise-grade LLM deployment through three integrated capabilities supporting AGL's governance and operational requirements.  Databricks Asset Bundles enable infrastructure-as-code deployment where LLM agents, model serving endpoints, and pipelines are defined in declarative YAML files with environment-specific configurations. This supports automated promotion across development, staging, and production via CI/CD pipelines, ensuring consistent deployments aligned with AGL's change management processes.  MLflow Model Registry in Unity Catalog provides centralised model lifecycle management with three-level namespace governance reflecting environment separation. Models are versioned automatically with full lineage to training data and code, and promoted using model aliases without redeployment, critical for audit and compliance requirements.  Agent Framework deployment uses agents.deploy() to create scalable Model Serving endpoints with automatic credential provisioning. The framework supports parametrised agent code using ModelConfig, enabling the same codebase to run across environments by loading environment-specific YAML configurations.  Environment isolation follows multi-workspace patterns with separate dev/staging/prod workspaces and Unity Catalog catalog, ensuring fine-grained access controls that prevent accidental production data modification, essential for AGL's data governance framework supporting customer data protection and regulatory compliance.  	References: Databricks Asset Bundles Manage model lifecycle Deploy Agent Framework CI/CD best practices LLMOps workflows on Azure Databricks Best practices for operational excellence Agent system design patterns Introducing AI Model Sharing with Databricks	Full	Yes
RES.127	LLM Application	What pre-trained LLM models or accelerators do you offer for utilities-specific use cases (e.g., outage prediction, regulatory reporting)?	Databricks does not offer pre-trained LLM models specifically for utilities use cases. Instead, we provide foundation models and tooling that AGL can customise with your operational data.  We provide solution accelerator code templates that demonstrate utilities patterns for dispatch optimisation and grid analytics. These are starting-point frameworks you adapt with AGL's data, not pre-trained models.  Through Agent Bricks, you can access foundation models like Meta Llama 3.3 70B Instruct via Foundation Model APIs and fine-tune them using Agent Bricks Model Training on your specific data including outage reports, regulatory documents and operational logs. This ensures models reflect AGL's grid topology, customer base and regulatory environment.  For outage prediction, you would build custom models using Agent Bricks AutoML or traditional ML approaches trained on your historical outage data, weather patterns and grid telemetry. For regulatory reporting, Agent Bricks Agent Bricks structures document generation workflows, with MLflow providing the governance and traceability required for compliance.  This approach delivers domain-specific AI that aligns with AGL's CTAP objectives and regulatory requirements whilst maintaining full control over your proprietary data and intellectual property.  	References: Databricks-hosted foundation models available in Foundation Model APIs Supported foundation models on Agent Bricks Model Serving LLMOps workflows on Azure Databricks Understanding LLM Pre-Training and Custom LLMs	Full	Yes
RES.128	Prompt Engineering / RAG	How do you implement retrieval-augmented generation for enterprise data?	Databricks implements retrieval-augmented generation through a unified architecture where vector indexes, agents, and enterprise data operate under single governance without data duplication.  Vector Search operates directly on Delta tables in Unity Catalog, maintaining your lakehouse as the single source of truth. Delta Sync monitors source tables and incrementally updates vector indexes in real-time as documents change. One-click configuration handles embedding generation and failure recovery, with embeddings stored in open Parquet format for data portability.  For AGL, this architecture ensures operational data, customer insights, and technical documentation remain governed under Unity Catalog's consistent RBAC, audit logging, and encryption (AES-256 at rest, TLS 1.2+ in transit) throughout the RAG pipeline. End-to-end lineage tracks data flow from source tables through vector indexes to deployed agents, supporting your BUSINESS INTELLIGENCE value driver and regulatory requirements.  The Agent Bricks Agent Framework deploys RAG applications to scalable Model Serving endpoints with automatic authentication passthrough. Agent Evaluation provides automated assessment of retrieval relevance and answer correctness, integrated with your existing data engineering platform.  This unified approach accelerates time-to-value for customer service automation and technical knowledge retrieval while maintaining governance standards required for energy sector operations and your transition to a lower-emissions future.  	References: RAG (Retrieval Augmented Generation) on Databricks Improve RAG chain quality Production-Quality RAG Applications with Databricks Example: use features with structured RAG applications	Full	Yes
RES.129	Prompt Engineering / RAG	Describe your version control and optimization strategies for prompts.	Databricks provides enterprise-grade version control and optimisation for prompts through MLflow's Prompt Registry, integrated with Unity Catalog for governance.  Version Control: The Prompt Registry delivers Git-style versioning with complete lineage tracking, visual diffs between prompt versions, and environment aliases for production, staging, and development. All prompts are governed through Unity Catalog, enabling centralised access control and audit trails across your data and AI teams.  Optimisation Strategies: Databricks integrates DSPy (Declarative Self-improving Language Programs) into MLflow for automated prompt optimisation. Rather than manual trial-and-error, DSPy uses your evaluation datasets and metrics to programmatically improve prompts. The platform includes mlflow.dspy.autolog() for experiment tracking and native deployment through Model Serving.  For AGL, this means prompt engineering becomes reproducible and auditable, critical for regulatory compliance and operational consistency. Teams collaborate on prompt development with full version history, while automated optimisation reduces time from prototype to production. All prompt assets integrate with Unity Catalog's governance framework, ensuring appropriate access controls across your organisation.  	References: Track prompt versions alongside application versions Prompt Registry documentation Evaluate and compare prompt versions Swiggy enterprise AI agent case study	Full	Yes
RES.130	Prompt Engineering / RAG	What capabilities exist for integrating RAG pipelines with orchestration workflows?	Databricks integrates RAG pipelines through two native capabilities: Lakeflow Jobs and Agent Bricks Agent Framework, both available on AWS and Azure.  Lakeflow Jobs orchestrates complete RAG workflows using visual DAG or API-based configuration. This coordinates data ingestion, Vector Search index synchronisation, model training, and agent deployment as unified tasks. Built-in monitoring, retry logic, and dependency management ensure reliable execution across AGL's data platform.  The Agent Bricks Agent Framework deploys RAG applications to Model Serving endpoints with a single deploy() command, automatically integrating tracing, evaluation, and production monitoring. Lakeflow Jobs tasks support notebooks, Python scripts, Lakeflow Spark Declarative Pipelines, and Vector Search operations, enabling end-to-end automation from document processing through inference.  For integration with existing AGL orchestration platforms, Databricks Jobs/Endpoints integrate with enterprise orchestrators; This ensures RAG pipelines participate in broader data workflows while maintaining governance and observability through Unity Catalog.  This orchestration capability directly supports AGL's Business Intelligence value driver, enabling reliable and auditable AI workflows that integrate seamlessly with existing data engineering processes.  	References: Run pipelines in Lakeflow Jobs Announcing the General Availability of Databricks Lakeflow What's New: Lakeflow Jobs Provides More Efficient Data Orchestration Managed connectors in Lakeflow Connect	Full	Yes
RES.131	Prompt Engineering / RAG	How do you support RAG pipelines using OT telemetry and IT operational data for compliance reporting?	Databricks supports RAG pipelines using the Agent Bricks: Knowledge Assistant pattern over OT telemetry and IT operational data through three integrated capabilities addressing AGL's compliance reporting requirements.  Lakeflow Connect streams operational telemetry from industrial control systems and SCADA platforms into Unity Catalog Delta tables, providing low-latency data capture for real-time monitoring of generation assets and distribution networks.  Agent Bricks Vector Search indexes operational data within Unity Catalog, enabling semantic retrieval across historical telemetry, maintenance logs, and incident reports. The Agent Bricks Agent Framework builds RAG applications that answer compliance queries by retrieving relevant operational context. MLflow tracing captures full request-response lineage for audit purposes, critical for AEMO and AER reporting.  Unity Catalog provides unified governance with role-based access controls, column-level lineage, and comprehensive audit logging through system tables. This enables compliance dashboards tracking data access patterns and supporting regulatory reporting requirements.  Agent Bricks Gateway logs all agent inference requests to Unity Catalog for quality monitoring. Lakehouse Monitoring detects data quality issues in telemetry streams and tracks agent performance over time, ensuring reliable compliance reporting.  	References: RAG (Retrieval Augmented Generation) on Azure Databricks Agent inference tables: Request and assessment logs Improve RAG chain quality Securing the Grid: A Practical Guide to Cyber Analytics for Energy & Utilities	Full	Yes
RES.132	Industry Differentiators	What industry-specific accelerators do you provide for Australian utilities (e.g., NEM bidding, AEMO integration)?	Databricks provides utilities accelerators customisable for NEM market participation and AEMO compliance, validated through Australian deployments.  NEM/AEMO Data Accelerator (in development): We are building an OpenElectricity API integration with enterprise Python SDK for automated NEM market data ingestion. This delivers facility generation and network demand at 5-minute intervals with pre-built ETL functions, Unity Catalog governance, and optimised PySpark schemas aligned to AEMO settlement processes for energy trading analytics and regulatory reporting.  Field-proven accelerators: Our Grid-Edge Analytics accelerator unifies IoT sensor data for grid optimisation and fault detection. The IoT Predictive Maintenance accelerator processes real-time IIoT telemetry for asset uptime. Digital Twins enables real-time simulation for DER integration and Virtual Power Plant optimisation. These reduce proof-of-concept time from months to 2 weeks and adapt to Australian market requirements.  Local validation: AEMO deployed Databricks with Unity Catalog supporting their Data Centre of Excellence. Alinta Energy migrated from Azure Synapse to Databricks for scale. The platform supports Energy Australia, APA, and Essential Energy for industrial AI, demonstrating regulatory understanding and local delivery capability.  Partner ecosystem: Celebal Technologies' PUFF framework delivers renewable energy forecasting integrating weather and pricing data. Capgemini's IDEA framework accelerated time-to-business-outcomes by 40% at Australia's largest utilities using platform-as-code delivery.  	References: Databricks Data Intelligence Platform Expands Brickbuilder Program to Include Unity Catalog Accelerators Introducing Databricks Cross-Industry Partner Accelerators for Agentic AI, GenAI and LLMOps Introducing Data Intelligence Platform for Energy Databricks Solutions Accelerators Grid-Edge Analytics Accelerator	Full	Yes
RES.133	Industry Differentiators	How do you support OT/IT convergence for distributed energy resources and smart grid operations?	Databricks enables OT/IT convergence for distributed energy resources and smart grid operations through unified streaming ingestion, real-time processing, and governed analytics across operational and enterprise domains.  Zerobus Ingest (part of Lakeflow Connect, Public Preview) streams telemetry directly from SCADA systems, smart meters, solar inverters, battery storage, and EV chargers into Unity Catalog Delta tables. It supports industrial protocols including MQTT and OPC-UA, eliminating traditional message bus complexity between edge devices and analytics platforms. This directly supports AGL's Virtual Power Plant initiatives and renewable integration under your Climate Transition Action Plan.  Structured Streaming processes AMI meter events, voltage anomalies, and DER telemetry for operational workloads requiring rapid grid response. This enables near real-time outage detection, voltage violation alerts, backflow monitoring from rooftop solar, and EV charging load balancing across millions of daily meter readings.  For renewable forecasting and DER optimisation, AI Functions, including ai_forecast(), support multivariate time-series analysis, factoring in weather patterns and grid conditions. MLflow manages the lifecycle of forecasting models for solar generation prediction, battery dispatch optimisation, and transformer loading analysis. Mosaic H3 geospatial indexing supports DER siting and proximity analysis.  Agent Bricks enables production agents for grid operations, including DER dispatch recommendations and field service guidance based on asset telemetry.  Unity Catalog enforces role-based access control on sensitive customer meter data and SCADA telemetry, with automated lineage tracking data flows from industrial edge through ML models to dispatch decisions. Lakehouse Federation enables federated queries across SCADA historians and enterprise systems without replication, supporting phased OT/IT integration aligned to security boundaries.  	References: Zerobus Ingest overview Structured Streaming for real-time processing Data Intelligence Platform for Energy Data Intelligence accelerating IT/OT convergence Powering the Future: Data and AI in Action Across the Energy Industry Revolutionizing IoT and M2M Solutions for Industry Innovation	Full	Yes
RES.134	Industry Differentiators	Do you offer pre-built connectors for SCADA, historian systems, and AEMO APIs?	Databricks does not provide pre-built managed connectors for SCADA, historian systems, or AEMO APIs in Lakeflow Connect. However, proven integration patterns exist for these industrial data sources.  For historian systems, AVEVA CONNECT aggregates AVEVA PI System and OSIsoft PI historian data with native Delta Sharing integration. AVEVA Adapters collect data from third-party historians including Honeywell IP21 and AspenTech PHD via OPC UA servers. Seeq industrial analytics platform offers bi-directional integration for process monitoring.  For custom SCADA and AEMO integrations, PySpark Custom Data Sources (GA in DBR 15.4 LTS+) enables building reusable Python-based connectors for proprietary industrial protocols and REST APIs. This framework integrates with Lakeflow Spark Declarative Pipelines and Unity Catalog for governed credential management.  Standard streaming connectors support OT/IT convergence through Apache Kafka, Azure Event Hubs, AWS Kinesis, and Google Pub/Sub for SCADA edge systems. Zerobus Ingest (Public Preview) accepts direct gRPC streams from industrial IoT. Auto Loader incrementally ingests files from SCADA historians with automatic schema detection.  	References: What is Lakeflow Connect? - Azure Databricks PySpark Custom Data Sources - Azure Databricks Zerobus Ingest Overview - Azure Databricks Announcing General Availability of Python Data Source API Standard connectors in Lakeflow Connect - Databricks on AWS	Full	Yes
RES.135	Industry Differentiators	What compliance certifications and frameworks do you support for Australian energy regulations?	Azure Databricks supports the following certifications or compliances via annual audits - ISO 27001, ISO 27017, ISO 27018, ISO 27701, SOC1 Type II, SOC 2 Type II, SOC 3, HIPAA, HITRUST, and UK Cyber Essentials Plus.  AWS Databricks supports the following certifications or compliances via annual audits - ISO 27001, ISO 27017, ISO 27018, ISO 27701, SOC1 Type II, SOC 2 Type II, SOC 3, HIPAA, PCI-DSS, FedRAMP Moderate, IRAP, and UK Cyber Essentials Plus.  	References: Infosec Registered Assessors Program (IRAP) | Databricks on AWS Compliance | Databricks on AWS Security and compliance - Azure Databricks | Microsoft Learn	Partial	Yes
RES.136	Industry Differentiators	How do you enable edge-to-cloud integration for real-time grid operations?	Databricks enables bidirectional edge-to-cloud integration supporting AGL's grid modernisation and distributed energy resource orchestration requirements.  Edge-to-cloud ingestion: Lakeflow Connect streams telemetry from substations, smart meters, solar inverters and battery systems directly into Unity Catalog Delta tables. Lightweight forwarding agents deployed at grid edge locations batch and stream operational data including voltage readings, breaker status and generation output with low latency, providing real-time visibility across AGL's generation, transmission and distribution assets.  Real-time processing: Serverless compute with Structured Streaming processes grid events continuously, auto-scaling during peak demand events such as storms or network faults, then scaling to zero during normal operations. Lakeflow Spark Declarative Pipelines enable continuous streaming of meter data, SCADA alarms and DER telemetry through medallion architecture with automated quality checks.  Cloud-to-edge integration: Unity Catalog syncs curated lakehouse data including ML predictions, load forecasts and equipment health scores into operational systems via managed synced tables. Custom PySpark Sinks in Lakeflow Spark Declarative Pipelines write analytics results back to SCADA systems, enabling closed-loop control where cloud-computed insights flow back to field devices and control room displays.  This architecture supports AGL's autonomous grid operations, enabling faster response to network events whilst reducing infrastructure complexity and operational costs.  	References: July 2025 Release Notes - Real-time mode for Structured Streaming What is Lakeflow Connect? Introducing the Data Intelligence Platform for Energy Revolutionizing IoT and M2M Solutions for Communications	Full	Yes
RES.137	Industry Differentiators	Do you provide accelerators for retail billing analytics and customer churn prediction in the Australian energy market?	Yes. Databricks provides configurable Solution Accelerator frameworks for retail billing analytics and customer churn prediction that can be adapted to Australian energy market requirements.  For churn prediction, the platform supports ML frameworks that analyse customer behaviour, consumption patterns, and engagement metrics. These can incorporate NEM pricing dynamics, smart meter interval data, and AER regulatory requirements specific to Australian energy retail.  Billing analytics capabilities enable real-time processing of high-volume meter data with integration to retail systems. Lakeflow Connect provides managed connectors for CRM platforms like Salesforce, creating unified customer views across billing, service, and engagement touchpoints.  The platform has proven experience in the Australian energy market, including deployments with Alinta Energy for customer analytics and with AEMO for market operations. This supports AGL's customer retention and operational efficiency objectives in the competitive retail environment.  Implementation requires integrating AGL's billing systems, smart meter data, customer touchpoint data, and energy market context into the accelerator frameworks.  	References: Solution Accelerators Data Intelligence Platform for Energy Salesforce Connectors for Lakehouse Federation and Lakeflow Connect Demo: Predict customer churn using the Salesforce ingestion connector From Search to Sale: How AI Is Redefining Customer Engagement and Loyalty in Retail Curating High-Quality Customer Identities with Databricks and Amperity Public Energy Customer Links	Full	Yes
RES.138	Industry Differentiators	How do you support integration with smart meter data for dynamic pricing and demand response programs?	Databricks enables AGL to leverage smart meter data for dynamic pricing and demand response programs that deliver customer value while optimising grid operations.  The platform natively connects to Azure IoT Hub and Event Hubs using Spark Structured Streaming, processing meter readings at 15-minute or hourly intervals with near-real-time availability. This handles telemetry, voltage events, and consumption patterns at the scale required for AGL's customer base.  AI-powered forecasting integrates smart meter data with weather, customer, and GIS data to predict demand patterns and optimise energy distribution. This supports time-of-use pricing programs that help customers manage costs while maintaining grid stability, directly supporting AGL's customer-focused business value drivers.  Machine learning models built on unified meter, market, and customer data enable real-time price optimisation and automated demand response targeting. The platform identifies usage patterns and consumption anomalies at household level, enabling behavioural segmentation for personalised programs.  This approach is proven at scale. Hydro-Québec analyses 4.5 million meters with over 1 trillion data points, DTE Energy processes grid reliability data in near-real-time for 2.3 million customers, and Octopus Energy processes large-scale smart meter datasets to deliver customer behaviour insights.  	References: July 2025 Release Notes - Real-time Structured Streaming Evolution of Utilities: The Rise Of The Data Intelligent Utility Introducing the Data Intelligence Platform for Energy Revolutionizing IoT and M2M Solutions for CSPs Manufacturing Insights: Calculating Streaming Integrals on Low-Latency Sensor Data	Full	Yes
RES.139	Industry Differentiators	What capabilities exist for integrating generation asset telemetry with AEMO dispatch and bidding systems?	Databricks provides the core capabilities for generation asset telemetry integration with AEMO dispatch and bidding systems, though custom development is required for direct AEMO connectivity.  Real-time telemetry processing: Lakeflow Spark Declarative Pipelines and Structured Streaming ingest generation data from wind, solar, battery storage, and conventional assets via Azure Event Hubs, Kafka, or Zerobus Ingest. This continuous monitoring supports AGL's growing renewable generation and firming capacity portfolio.  Forecasting for market participation: The platform processes weather patterns, solar irradiance, wind speed, and grid constraints to predict generation availability. These forecasts directly support optimised bidding strategies for NEM participation, critical to AGL's energy trading operations.  Market integration architecture: Real-time processing combined with workflow orchestration (Lakeflow Jobs) enables integration of asset performance data with AEMO pricing signals. This supports day-ahead offers and trading decisions across AGL's generation portfolio.  Integration framework: While no pre-built AEMO accelerators exist, the platform's REST APIs, streaming connectors, and orchestration capabilities provide the foundation for custom AEMO system integration. Notably, AEMO itself uses Databricks for energy market operations, demonstrating platform suitability for Australian energy market requirements.  Implementation would leverage AGL's existing Azure infrastructure and Databricks' open integration framework to connect generation assets with AEMO dispatch and bidding systems.  	References: June 2025 Release Notes - Lakeflow Spark Declarative Pipelines OpenTelemetry Export for telemetry integration How Data Intelligence is Accelerating IT/OT Convergence The scope of the lakehouse platform - Orchestration	Full	Yes
RES.140	Industry Differentiators	Do you provide pre-built workflows for wholesale market settlement and compliance reporting?	No. Databricks does not provide pre-built workflows for wholesale market settlement and compliance reporting. Custom development is required to implement NEM settlement calculations, AEMO format requirements, and Australian regulatory compliance templates.  However, the platform provides robust orchestration and governance foundations for building these workflows:  Lakeflow Jobs delivers fully managed orchestration, enabling custom multitask workflows for settlement processes, ETL pipelines, and compliance reporting with advanced control flow and scheduling capabilities.  Unity Catalog provides comprehensive data governance essential for regulatory compliance, including automated lineage tracking, audit logging, and data quality controls. This ensures auditable, traceable data flows critical for AEMO settlement processes and regulatory reporting.  For AGL's NEM operations, this means building settlement workflows tailored to your specific market participation, trading strategies, and compliance requirements. The platform's governance and orchestration capabilities support reliable SLA delivery during high regulatory demand periods, but the business logic and settlement calculations require custom implementation aligned to your operational needs.  	References: Audit log reference Securing the Grid: A Practical Guide to Cyber Analytics for Energy & Utilities Lakeflow Jobs Transforming Regulatory Data and Risk Analytics	Partial	No (Customisation)
RES.141	Industry Differentiators	How do you support carbon accounting and renewable energy certificate tracking in compliance with Australian standards?	Databricks provides the data governance and analytical foundation to build custom carbon accounting and renewable energy certificate tracking solutions. Pre-built compliance templates are not currently available.  Custom development is required to implement Australian-specific calculation methodologies and reporting formats. SAP Green Ledger integration, when available, can provide carbon emission accounting tied to financial dimensions using double-entry accounting principles, supporting integrated climate and financial reporting. 	References: Adopting Databricks and Unity Catalog Governance to Support ITGC Compliance Infosec Registered Assessors Program (IRAP) Compliance Overview Introducing the Data Intelligence Platform for Energy	Partial	No (Customisation)
RES.142	Industry Differentiators	Do you offer AI-driven forecasting for renewable generation variability and grid balancing?	Yes. Databricks delivers AI-driven forecasting capabilities specifically designed for renewable generation variability and grid balancing, supporting AGL's transition to a lower-emissions future.  The platform provides three core capabilities for managing renewable intermittency:  1. Agent Bricks AutoML Forecasting (Public Preview) serverless time series forecasting with automatic algorithm selection. It supports multivariate forecasting with covariates, enabling you to incorporate weather data, grid conditions, and external factors critical for optimising dispatch decisions and managing renewable variability across AGL's portfolio.  2. Multi-series forecasting at scale enables simultaneous forecasting across multiple renewable generation sources and grid segments. This is essential for portfolio-level renewable variability management and day-ahead market optimisation in the NEM.  3. The ai_forecast() Databricks SQL AI Function provides table-valued time series extrapolation for grouped, multivariate, or mixed-granularity data. This supports scenarios where multiple variables like wind speed, solar irradiance, and hydro inflow must be considered simultaneously.  These capabilities enable enhanced load demand forecasting, outage prediction, and usage pattern identification to manage renewable intermittency, optimise market offers, and balance supply and demand. The platform's open architecture supports integration with AEMO systems and third-party forecasting frameworks where required.  	References: Train forecasting models with serverless compute Covariate forecasting with AutoML An Introduction to Time Series Forecasting with Generative AI Introducing the Data Intelligence Platform for Energy	Full	Yes
RES.143	Master Data Management	Which MDM styles do you support (operationalransactional MDM, registry, consolidation, coexistence, centralized hub)? In which scenarios do you recommend each?	Which MDM styles do you support (operational/transactional MDM, registry, consolidation, coexistence, centralized hub)? In which scenarios do you recommend each? Databricks supports all five MDM styles through Unity Catalog, Delta Lake, Lakebase Postgres (Public Preview), and Databricks Lakeflow (including Lakeflow Connect, Lakeflow Spark Declarative Pipelines, and Lakeflow Jobs).  Registry MDM uses Unity Catalog as the central metadata catalog with Lakehouse Federation querying source systems directly. Recommended when source systems remain authoritative and you need unified governance without data movement, ideal for AGL’s distributed operational systems.  Consolidation MDM leverages medallion architecture to consolidate data into Unity Catalog Delta tables, creating golden records through MERGE operations and quality rules. Recommended for analytics and BI use cases supporting decision-making across customer, asset, and market data.  Coexistence MDM enables bidirectional synchronization between Unity Catalog analytical tables and Lakebase operational databases using snapshot and incremental sync patterns. Recommended when both analytical and operational systems require consistent master data, such as customer information across billing and service platforms.  Centralized Hub positions Unity Catalog and Delta Lake as the authoritative system of record, with Delta Sharing distributing governed golden records to downstream systems. Provides enterprise‑wide governance, lineage, and access control. Recommended for regulatory reporting requirements under TCFD, SASB, and emissions tracking.  Operational/Transactional MDM uses Lakebase Postgres for low‑latency transactional access with ACID guarantees, with reverse ETL syncing curated master data from Unity Catalog. Recommended for customer‑facing applications requiring sub‑second response times.  Third‑party MDM tools like Profisee and CluedIn available on Azure Marketplace provide packaged solutions integrating with Databricks if pre‑built MDM functionality is preferred.	References: Accelerating Provider MDM in Healthcare with Databricks and AI Best practices for data and AI governance What is a data lakehouse? Building a robust data stewardship tool in life sciences	Full	Yes
RES.144	Master Data Management	What master domains are natively supported (Customer, Asset, Meter/Device, Premise/Site, Product/Tariff, Supplier, Employee)? How extensible are the domain models?	Databricks does not provide native master domain models for Customer, Asset, Meter/Device, Premise/Site, Product/Tariff, Supplier, or Employee entities. Instead, the platform enables building custom domain models through Delta Lake tables with fully customisable schemas tailored to AGL’s specific business requirements.  Solution Accelerators provide reference implementations for common patterns including Customer Entity Resolution and Customer 360, reducing development time whilst requiring customisation for AGL’s energy operations and data sources. For the energy sector, partner-built accelerators exist for Smart Meter Analytics (AMI data ingestion, usage patterns, billing automation), grid health monitoring, and asset management.  Domain model extensibility is unrestricted through Delta Lake schema evolution capabilities: adding columns without data rewrites, changing data types via type widening, automatic schema evolution during MERGE operations, support for complex nested types (structs, arrays, maps), and schema enforcement with CHECK constraints. Models evolve as business needs change.  Organisations requiring pre-built master domains can integrate packaged MDM solutions (Profisee, CluedIn) available on Azure Marketplace, with bi-directional synchronisation to Delta tables implemented via Lakeflow Jobs/pipelines and connectors/APIs.  Building master domains requires data engineering effort to design schemas, implement data quality rules, create golden record logic, and establish governance policies in Unity Catalog.	References: Accelerating Provider MDM in Healthcare with Databricks and AI Unity Catalog privileges and securable objects Best practices for data and AI governance Building High-Quality and Trusted Data Products with Databricks	Full	Yes
RES.145	Master Data Management	How do you create and manage the golden record (survivorship, match/merge, trust scores)? Describe your matching algorithms (deterministic, probabilistic, ML‑assisted).	Databricks creates golden records through Delta Lake MERGE operations with configurable survivorship logic, supporting deterministic, probabilistic, and ML-assisted matching. This is a platform-based approach requiring data engineering rather than packaged MDM tooling.  Golden Record Creation: Delta Lake MERGE INTO operations consolidate duplicates into authoritative records using SQL‑based survivorship rules. CASE WHEN statements and window functions (ROW_NUMBER() OVER (PARTITION BY … ORDER BY …)) define attribute‑level precedence based on recency, source system priority, data completeness, or custom business logic. ACID guarantees ensure atomic upserts across distributed datasets.  Matching Algorithms: Three approaches are supported. 1. Deterministic matching uses exact SQL predicates on business keys (customerid, email, phone) with multi-column composite logic. 2. Probabilistic matching via Zingg (open-source Apache Spark library) applies locality-sensitive hashing for candidate pair generation, multiple string distance algorithms (edit distance, Jaccard, cosine), and binary classification models trained on labelled pairs to produce match probabilities. 3. ML-assisted matching leverages Zingg's active learning, custom MLflow models for quality scoring, or Vector Search with semantic similarity for fuzzy matching.  Trust Scores and Survivorship: Implementation options include Zingg match probabilities (0.0-1.0 confidence scores), custom MLflow models evaluating completeness/accuracy/recency, or storing quality scores as table attributes. Automated thresholds enable decisioning (e.g., greater than 0.80 auto-merge, 0.50-0.80 human review).  For AGL's customer and asset master data consolidation supporting CTAP objectives, this approach provides flexibility to encode domain-specific survivorship rules whilst leveraging Databricks' scale and governance. Partner solutions (LakeFusion, Profisee) offer packaged alternatives if turnkey MDM UI and stewardship workflows are preferred.  	References: Accelerating Provider MDM in Healthcare with Databricks and AI Building a robust data stewardship tool in life sciences Best practices for reliability	Full	Yes
RES.146	Master Data Management	Can we define source precedence and attribute‑level survivorship? How is versioning and lineage of master attributes handled?	Yes, Databricks supports all three capabilities through Delta Lake and Unity Catalog, with implementation approaches ranging from custom SQL logic to partner solutions.  Source Precedence and Survivorship: Implement through custom SQL logic in MERGE operations. Define source system priority using CASE statements or window functions like ROWNUMBER() OVER PARTITION BY to rank sources by trust level, recency, or data quality score. Attribute-level survivorship requires column-specific MERGE logic where each attribute applies different rules within a single statement. For example, customer name from CRM, email from marketing platform, address from most recent update. This provides flexibility but requires data engineering effort to build and maintain survivorship rules.  Versioning: Delta Lake transaction log automatically creates immutable versions for every write operation. DESCRIBE HISTORY provides complete audit trail including operation type, user, timestamp, and metrics for rows inserted, updated, or deleted. Default retention is 30 days for metadata (configurable via delta.logRetentionDuration) and 7 days for data files (delta.deletedFileRetentionDuration). Time travel queries using VERSION AS OF or TIMESTAMP AS OF enable point-in-time recovery and historical attribute analysis for compliance workflows.  Lineage: Unity Catalog automatically captures column‑level lineage across transformations, tracking data flows through notebooks, jobs, and Lakeflow Spark Declarative Pipelines. Lineage is accessible via Catalog Explorer, system tables (for example, system.access.column_lineage), and the REST API; system tables typically retain lineage up to 365 days. This supports impact analysis, compliance requirements like GDPR/SOX, and root‑cause debugging by tracing attribute origins and downstream dependencies.  For organisations preferring turnkey functionality over custom implementation, partner MDM solutions like LakeFusion or Profisee provide pre-built survivorship engines.  	References: Delta Lake History Delta Lake MERGE Unity Catalog Data Lineage Change Data Capture ROWNUMBER Function Data Governance with Databricks Best Practices for Data and AI Governance Building High-Quality and Trusted Data Products	Full	Yes
RES.147	Master Data Management	What APIs/SDKs do you provide (REST, GraphQL, event-streams, CDC)? Do you support near‑real‑time propagation to downstream systems (CRM, ERP, Billing)?	Databricks provides comprehensive REST APIs and SDKs in Python, Java, and Go for programmatic access to all platform capabilities, including data operations, job orchestration, and Unity Catalog governance. GraphQL is not offered as a public API, REST provides the required programmatic interface.  For near real-time propagation to AGL's CRM, ERP, and billing systems:  Event Streams: Native Structured Streaming with connectors for Apache Kafka, Azure Event Hubs, AWS Kinesis, and Google Pub/Sub delivers sub-minute latency with exactly-once semantics.  Change Data Capture: Lakeflow Spark Declarative Pipelines processes CDC with SCD Type 1 and Type 2 support, handling out-of-order events and enabling incremental propagation of inserts, updates, and deletes.  Downstream Integration: Four primary approaches support AGL’s operational systems. First, Structured Streaming writes directly to Kafka and Event Hubs. Second, Lakebase Postgres (Public Preview) can host operational workloads and supports syncing lakehouse data to operational tables for reverse ETL  These capabilities ensure AGL's customer-facing operations maintain access to current master data, supporting accurate billing and service delivery across the customer base.  	References: July 2025 Release Notes - Real-time mode for Structured Streaming Real-time mode in Structured Streaming Tutorial: Build an ETL pipeline using change data capture What is Lakeflow Connect?	Full	Yes
RES.148	Master Data Management	How do you integrate with common platforms (Databricks, Azure Synapse, SAP S/4HANA, Salesforce, ServiceNow)? Provide reference architectures and connectors.	Databricks integrates with all specified platforms through native connectors, zero-copy federation, Delta Sharing, and REST APIs. This supports AGL's requirement to unify customer, billing, and operational data across enterprise systems.  Integration capabilities by platform:  1. Databricks-to-Databricks: Unity Catalog provides zero-copy data sharing across workspaces, accounts, and clouds (AWS, Azure, GCP) without data movement. Supports tables, views, volumes, models, and notebooks with unified governance.  2. Lakehouse Federation delivers zero‑ETL integration via JDBC with query pushdown, enabling federated queries with Unity Catalog governance for ad‑hoc reporting without data duplication.  3. SAP S/4HANA: Three integration patterns available. SAP Business Data Cloud Connector via Delta Sharing for RISE deployments. SAP Datasphere Replication Flow for CDC-based extraction. Partner ETL tools including Azure Data Factory SAP CDC, Qlik Replicate, and Fivetran for application-layer extraction.  4. Salesforce: Lakeflow Connect Salesforce connector provides managed ingestion from Sales Cloud and Service Cloud with incremental CDC. Lakehouse Federation enables federated queries to Salesforce Data Cloud.  5. ServiceNow: Lakeflow Connect ServiceNow connector (GA) delivers managed ingestion via Table API v2 with OAuth authentication, incremental reads, and support for standard and custom tables.  Reference architectures document end-to-end patterns from source to analysis, covering Azure Synapse, SQL Server, IoT Hub, Event Hubs, Data Factory, ADLS Gen2, Cosmos DB, and Power BI integrations with Lakeflow Connect, Lakehouse Federation, Delta Sharing, and Unity Catalog governance.  	References: Lakehouse reference architectures Azure Synapse Lakehouse Federation Lakeflow Connect overview Introducing Salesforce Connectors for Lakehouse Federation and Lakeflow Connect What is a data lakehouse	Full	Yes
RES.149	Master Data Management	What embedded DQ capabilities exist (profiling, rules, thresholds, anomaly detection)? Can we write custom rules and reuse them across domains?	Yes, Databricks provides comprehensive embedded data quality capabilities with full support for custom rules reusable across all domains through Unity Catalog governance.  Data Profiling delivers automated statistical analysis with 20+ metrics including nulls, distinct values, quantiles, and distributions. Drift detection uses Chi-squared, Wasserstein distance, PSI, and KS tests. Results are stored in queryable Delta tables with auto-generated Lakeview dashboards for monitoring.  Declarative Quality Rules in Lakeflow Spark Declarative Pipelines enable row-level validation with three enforcement actions: EXPECT (log violations), EXPECT OR DROP (quarantine invalid records), and EXPECT OR FAIL (halt pipeline). Rules support SQL Boolean constraints for completeness, uniqueness, boundaries, patterns, cross-field validation, and temporal checks.  AI-Powered Anomaly Detection (Beta) automatically identifies freshness and completeness anomalies by analysing commit history and row volumes, with intelligent scanning prioritising high-impact tables.  Custom Rule Reusability: Rules are defined once and reused across domains through multiple mechanisms. Python or SQL code stored in Git repositories deploys via Databricks Asset Bundles for CI/CD. Expectation dictionaries import across pipeline tables. DQX (Databricks Labs, 500+ customers) provides YAML or JSON rule definitions stored in Unity Catalog volumes for centralised rule libraries. Tag Policies (Beta) enable consistent classification and attribute-based access control across all catalogs, schemas, and tables, ensuring standardised quality enforcement organisation-wide.  	References: Data quality monitoring Lakeflow Spark Declarative Pipelines expectations DQX - Data Quality Extensions Governed tags for Unity Catalog	Full	Yes
RES.150	Master Data Management	Describe stewardship workflows (task routing, bulk edits, approvals, audit trail). Do you support role-based queues and SLA tracking?	Databricks provides robust building blocks for data stewardship workflows through configuration rather than pre-built stewardship UI, enabling AGL to integrate with existing enterprise systems.  Task Routing and Orchestration: Lakeflow Jobs orchestrates workflows with up to 1,000 tasks per job, supporting conditional logic, task dependencies, and programmatic creation via REST API, CLI, and Terraform. This enables custom stewardship workflows integrated with AGL's approval systems.  Bulk Edit Operations: Multiple mechanisms support bulk operations including SQL DDL statements for batch metadata updates (GRANT/REVOKE, ALTER TABLE, tag assignments), Databricks SDK for Python/Java/Go enabling programmatic bulk operations, and Unity Catalog REST APIs for custom workflows.  Approval Workflows: Unity Catalog Access Requests (Public Preview) routes requests to email, Slack, Microsoft Teams, or webhooks, integrating with enterprise approval systems. Hierarchical ownership models allow catalog and schema owners to delegate MANAGE privileges to data stewards for controlled approvals.  Audit Trail: System tables provide queryable audit logs (system.access.audit) with 365-day retention capturing all Unity Catalog operations including user identity, action, timestamp, source IP, and complete lineage with column-level granularity.  Role-Based Queues and SLA Tracking: Unity Catalog implements RBAC through groups with hierarchical privilege grants. Lakeflow Jobs supports SLA enforcement through completion time thresholds, duration warnings, monitoring dashboards, and system tables enabling custom SLA queries. Databricks SQL alerts (Public Preview) provide automated threshold-based monitoring with notifications.  	References: System tables audit logs Lakeflow Jobs documentation Unity Catalog access control Building a robust data stewardship tool in life sciences	Full	Yes
RES.151	Master Data Management	How are hierarchies (customer-to-premise, meter-to-asset) modeled and governed?	Databricks models utility hierarchies through relational constraints and Unity Catalog governance, supporting customer-to-premise and meter-to-asset relationships with complete audit trails. This capability directly supports AGL's ASSETS and BUSINESS INTELLIGENCE value drivers by maintaining accurate operational data lineage.  Primary Key and Foreign Key constraints declare parent-child relationships in SQL DDL. These constraints appear in Catalog Explorer's entity relationship diagrams and are discoverable by BI tools via JDBC/ODBC. While informational, they enable query optimisations when marked with RELY option.  For temporal tracking, Lakeflow Spark Declarative Pipelines support SCD Type 2 patterns through AUTO CDC APIs, maintaining __START_AT and __END_AT  columns to track relationship validity periods (e.g., meter-to-asset associations, customer premise occupancy). This handles out-of-order events and preserves complete history for regulatory compliance.  Recursive Common Table Expressions (Public Preview in DBSQL 2025.20) enable SQL-based traversal of multi-level hierarchies, supporting meter-to-submeter-to-device chains or customer-to-account-to-premise-to-meter structures.  Unity Catalog provides centralised governance with metadata queryable via information_schema tables, column-level lineage tracking relationship propagation, and fine-grained access control. Audit logs (365-day retention) track all hierarchy modifications. Data quality monitoring validates referential integrity through CHECK constraints and pipeline expectations, alerting on orphaned records or broken relationships.  	References: Constraints Entity Relationship Diagrams in Catalog Explorer Introducing Recursive Common Table Expressions in Databricks Data warehousing architecture - Azure Databricks What is a data lakehouse?	Full	Yes
RES.152	Master Data Management	Detail security: RBAC/ABAC, attribute-based masking, consent flags, PII handling, Right‑to‑be‑Forgotten (GDPR/Privacy Act), and audit logging.	Databricks Unity Catalog delivers enterprise-grade security controls for master data management, supporting Australian Privacy Act and GDPR compliance requirements critical to AGL's customer data governance.  RBAC and ABAC: Unity Catalog implements role-based access control through granular privilege grants at catalog, schema, table, and column levels. Attribute-Based Access Control uses governed tags and policies to dynamically enforce access based on data sensitivity, classification, or business context, enabling policy-driven security that scales with data growth.  Attribute-based masking: Column mask policies apply governed tags and user-defined functions to control data visibility at query time. Sensitive values such as customer identifiers, contact details, or financial data are redacted or hashed based on user identity and group membership, preserving data utility for analytics whilst protecting privacy.  Consent flags: Consent management is implemented through governed tags applied at table or column level, combined with ABAC policies. Tags support custom key-value pairs for tracking consent status and withdrawal, queryable through information schema for compliance reporting.  PII handling: Data Classification capabilities use AI to automatically discover and tag PII across catalogs, including credit cards, tax file numbers, email addresses, and phone numbers. Classification results integrate with ABAC policies to automate protection of discovered sensitive data.  Right-to-be-Forgotten: Delta Lake DELETE and MERGE operations remove records, with VACUUM permanently deleting data files after configurable retention periods. REORG TABLE with PURGE ensures physical deletion, supporting Privacy Act and GDPR deletion requirements across bronze, silver, and gold data layers.  Audit logging: All Unity Catalog operations are captured in system.access.audit with 365-day retention, recording user identity, timestamps, operations, and data accessed. This provides comprehensive audit trails for compliance reporting and security forensics.  	References: Table access control GA release Governed tags for consent management How to scale data governance with ABAC Data governance with Databricks	Full	Yes
RES.153	Master Data Management	How do you enforce data residency and sovereign cloud requirements?	For AWS and GCP Databricks, our workspaces are deployed in a cloud provider and region of the customer’s choosing, and virtually all data is stored within the customer's cloud service provider account (such as in S3 or GCS buckets in the customer's account) and not within Databricks-owned accounts. Databricks does not move workspaces without the Customer's express permission.  Azure Databricks is a Microsoft product that is powered by Databricks. Ultimately, Microsoft is responsible for answering customer questions about how Azure Databricks meets customer data protection requirements.  To meet sovereign cloud requirements, Databricks operates in the Azure Government and Azure China regions, with environment-isolated control planes. Customer-managed encryption keys through Azure Key Vault ensure cryptographic control remains within designated geographies. Data is encrypted in transit using TLS 1.2+ and at rest using AES-256 with FIPS 140 validated modules for regulated workloads.  This architecture supports AGL's Australian data sovereignty requirements while enabling secure collaboration across business units.  	References: Infosec Registered Assessors Program (IRAP) Security, compliance, and privacy for the data lakehouse Databricks' Commitment to European Sovereignty and Growth A Unified Approach to Data Exfiltration Protection on Databricks	Full	Yes
RES.154	Master Data Management	What are your SLA benchmarks for creation/update/read of golden records at scale (e.g., tens of millions of customers/meters)?	Databricks does not publish operation-level SLA benchmarks for create, update, or read operations on golden records. The platform guarantees 99.95% workspace availability on Azure, but individual operation performance depends on workload configuration rather than fixed response times.  At your scale of tens of millions of customers and metres, performance is determined by cluster sizing, Photon acceleration, liquid clustering configuration, and table optimisation. Key capabilities supporting high-performance golden record operations include:  Photon acceleration speeds up UPDATE, DELETE, MERGE INTO, and INSERT operations on Delta tables. Liquid clustering with automatic key selection optimises query performance by skipping data. Row-level concurrency (GA in Runtime 14.2+) reduces conflicts when concurrent operations modify different rows. Delta Lake's optimistic concurrency control enables multiple clusters to write simultaneously without locks, supporting high-throughput operations.  For AGL's customer and metre data volumes, we recommend performance testing during proof-of-concept with representative schemas and access patterns. This validates throughput and latency requirements for your specific golden record operations. Unity Catalog managed Delta tables with liquid clustering and predictive optimisation provide the foundation for optimal performance at scale.  We can provide reference architectures from similar-scale implementations to inform your performance expectations and configuration approach.  	References: Azure Databricks SLA Azure Databricks reliability Photon acceleration Liquid clustering Databricks SQL release notes 2025 Best practices for performance efficiency	Full	Yes
RES.155	Master Data Management	How do you handle multi-region deployments, high availability, disaster recovery (RPO/RTO), and schema evolution without downtime?	Azure Databricks is a Microsoft product that is powered by Databricks. Ultimately, Microsoft is responsible for answering customer questions about how Azure Databricks meets customer data protection requirements.	References: Disaster recovery Azure Databricks Azure Databricks reliability Update Delta Lake table schema Delta Sharing documentation Disaster recovery Databricks on AWS Disaster recovery Databricks on Google Cloud	Partial	Yes
RES.156	Master Data Management	Do you provide ML-assisted matching and anomaly detection? Can models be trained on our data and governed (MLOps, drift monitoring)?	Yes. Databricks provides comprehensive ML-assisted anomaly detection and full MLOps governance for models trained on your data.  Anomaly detection is built-in through data profiling. The system analyses historical patterns to automatically assess data freshness and completeness across schemas. Per-table predictive models learn expected commit times and row volumes, flagging tables as unhealthy when commits are late or row counts fall below predicted ranges. Automated root cause analysis identifies upstream jobs contributing to issues.  For ML-assisted matching, you build custom models using Apache Spark MLlib, Python libraries (scikit-learn, XGBoost, PyTorch), fuzzy matching algorithms, and similarity functions. Models train directly on Unity Catalog tables containing your customer and meter data, using supervised learning for duplicate detection or neural networks for semantic matching.  MLOps governance through Unity Catalog provides centralised model management including RBAC access control, audit logging, chronological lineage from training data to production endpoints, model versioning with deployment aliases, and cross-workspace discovery. MLflow handles experiment tracking, automated deployment pipelines, A/B testing, and CI/CD integration.  Drift monitoring operates at two levels. Data profiling monitors statistical distribution changes between current data and baselines. For deployed models, inference tables automatically log inputs and predictions to Unity Catalog, enabling continuous monitoring of model performance and data drift with automated alerting when metrics exceed thresholds.  	References: Training 10,000 Anomaly Detection Models on One Billion Records with Explainable Predictions MLOps workflows on Databricks MLflow for ML model lifecycle Best practices for operational excellence	Full	Yes
RES.157	Master Data Management	How do you expose explanations for match decisions and allow human-in-the-loop corrections?	Databricks exposes match decision explanations through SHAP (Shapley Additive Explanations), which calculates feature-level contributions for each matching decision. AutoML-generated notebooks include built-in SHAP calculations showing which customer attributes (name, address, account number, date of birth) drove match or non-match outcomes. Feature importance visualisations help data stewards understand matching logic and identify issues.  Human-in-the-loop corrections operate through three mechanisms. First, MLflow Review Apps provide structured web interfaces where MDM specialists review match decisions, test matching logic in real-time, and provide corrections. Review Apps support labelling sessions for systematic batch review, with all feedback stored as Assessments attached to individual traces for complete audit trails.  Second, Databricks Apps enables custom review interfaces using Streamlit or similar frameworks. These display side-by-side record comparisons with match scores and contributing features, collect user decisions, and write corrections to Unity Catalog tables with full permission governance.  Third, Delta Lake provides ACID transactions with complete audit history through DESCRIBE HISTORY and Change Data Feed, recording user, timestamp, and operation for every modification. This creates versioned correction trails meeting regulatory requirements.  Human corrections feed directly into model improvement through MLflow Evaluation Datasets, creating continuous learning where expert decisions systematically improve matching accuracy over time.  	References: AutoML documentation Review App for human feedback Human feedback in MLflow Delta Change Data Feed Align judges with humans Best practices for data and AI governance Building a robust data stewardship tool in life sciences The Unconscious Patient Problem Entity Resolution in Healthcare	Full	Yes
RES.158	Master Data Management	Describe typical implementation timelines and roles. Provide case studies in utilities	Implementation Timelines for Regulated Utilities  Platform deployment requires 3-6 months for architecture design, workspace setup, Unity Catalog governance, and pilot onboarding. However, AGL should plan 9-12 months end-to-end given organisational realities common to regulated utilities: security review cycles, resource constraints across BAU operations, and knowledge transfer requirements while maintaining operational continuity.  Critical Implementation Roles  Enterprise Data Architect provides strategic oversight and governance decisions. Transformation Lead coordinates day-to-day delivery with protected capacity from BAU demands. Databricks Resident Solutions Architect delivers embedded expertise, production-grade code, and pre-validated security patterns. Certified Partner Resources (Australian ecosystem includes Mantel Group, NCS Australia) execute migration, testing, and knowledge transfer. Project Manager orchestrates approval processes across security, architecture, and compliance stakeholders. Executive Sponsor provides air cover protecting transformation resources and accelerates approval cycles.  Australian Utilities Case Studies  AusNet Services (Victoria) migrated all production workloads serving 1.5 million electricity and gas customers within months despite resource constraints. Achieved 50% cost savings, 3x faster data processing, and 20-30% operational overhead reduction. Implemented predictive maintenance across asset classes for risk reduction and cost optimisation.  Australian Energy Market Operator (AEMO) established Unity Catalog-enabled Data Centre of Excellence providing centralised governance across enterprise data services supporting National Electricity Market operations, forecasting, and analytics.  Tabcorp unified fragmented data across 4,000+ venues serving 1 million+ customers in similarly regulated environment. Achieved 70-90% marketing efficiency improvement, $1.75M platform savings, and deployed 40+ ML models via MLflow while maintaining strict regulatory compliance through Unity Catalog governance.  	References: Data governance with Databricks Best practices for data and AI governance Accelerating Provider MDM in Healthcare with Databricks and AI Data and AI governance for the data lakehouse	Full	Yes
RES.159	Master Data Management	How do you migrate existing masters from legacy hubs/Hive metastore/ERP masters while preserving lineage and history?	Yes. Databricks provides documented migration paths preserving both lineage and history through Unity Catalog and Delta Lake.  For Hive Metastore migrations, four approaches are available. SYNC creates external tables pointing to existing storage without data replication, preserving file-level history while enabling Unity Catalog governance. DEEP CLONE copies managed Delta tables with full transaction logs for time travel access. Hive Metastore Federation creates a foreign catalog mirroring HMS, enabling phased migration where workloads access data through Unity Catalog governance. UCX (Unity Catalog Migration Assistant) automates assessment and migration workflows for large-scale environments.  Unity Catalog automatically captures runtime lineage at table and column levels across all migrated tables. System tables provide 365-day lineage retention with programmatic query access. Delta Lake maintains versioned transaction logs enabling time travel queries via VERSION AS OF syntax, with configurable retention periods (default 30-day log history, 7-day data file retention). Change Data Feed captures row-level changes for compliance requirements.  For ERP master data from SAP systems, extraction is supported via SAP Datasphere Replication Flow, third-party tools (Qlik Replicate, Fivetran, Azure Data Factory SAP CDC Connector), or SAP Business Data Cloud integration. Master data hierarchies and time-dependent attributes are reconstructed in Unity Catalog with full lineage tracking.  For AGL, this supports CTAP reporting requirements and regulatory compliance through auditable data lineage across energy trading, customer, and asset master data.  	References: November 2025 Release Notes - External Tables from Hive Metastore Enable Hive metastore federation for a legacy workspace Hive metastore Enterprise-Scale Governance: Migrating from Hive Metastore to Unity Catalog Upgrade Hive tables and views to Unity Catalog	Full	Yes
RES.160	Master Data Management	Licensing model (per domain, per record, per environment)?	Databricks uses a consumption-based licensing model, not per domain, record, or environment. You pay only for compute resources used, measured in Databricks Units (DBUs) and billed per-second.  The model operates on three principles:  1. Pay-as-you-go consumption: No per-user fees or upfront costs. Multiple teams, business units, and environments (dev/test/prod) share a unified DBU pool without artificial constraints.  2. Workload-differentiated pricing: Different compute types (Jobs, All-Purpose, SQL, Serverless) have different DBU rates, enabling cost optimisation by matching workload to appropriate compute.  3. Flexible commitments: Pre‑paid DBU commitments for 1–3 year terms provide discounted rates. On Azure, Databricks operates as a first‑party Microsoft service, allowing application of existing Azure commitment discount agreements.  Cloud infrastructure (VMs, storage, networking) is billed separately by your cloud provider for non-serverless products. Serverless options bundle infrastructure costs into DBU rates for simplified billing.  This model scales naturally with AGL's usage patterns across customer analytics, asset optimisation, and renewable forecasting without licensing barriers as data volumes or user counts grow.  	References: Databricks Pricing Databricks on AWS Pricing Azure Databricks Pricing Azure Databricks Pricing Documentation	Full	Yes
RES.161	Master Data Management	Estimated TCO for a 3‑year period for ~20M customer devices and multi-domain governance	TCO estimation for 20 million customer devices with multi-domain governance requires detailed workload profiling aligned to AGL's architecture and CTAP objectives. A comprehensive Enterprise tier deployment would encompass several key cost drivers.  Platform components include streaming data ingestion from smart metres and DER assets using Lakeflow Spark Declarative Pipelines with Delta Lake optimisation. Unity Catalog manages separate business domain catalogs for customer markets, energy markets, wholesale trading, renewables, and regulatory reporting with fine-grained access control and automated lineage tracking supporting compliance requirements.  Lakeflow Connect provides integration with operational systems including Kaluza and Salesforce environments. Databricks SQL Serverless warehouses serve transactional workloads for customer master data, device registry, and billing systems. Production AI/ML workloads for demand forecasting and customer analytics utilise Agent Bricks with MLflow governance and real-time model serving.  Serverless compute architecture eliminates idle infrastructure costs through intelligent auto-scaling across data pipelines, SQL warehouses, and ML workloads. Delta Sharing enables zero-copy data exchange with market operators and regulators whilst maintaining governance boundaries.  Precise TCO modelling requires bottom-up sizing incorporating actual workload profiles, peak demand patterns, data retention policies, and multi-year growth projections. Enterprise tier pricing with volume-based discount programmes applies to committed multi-year platform adoption. We recommend a detailed scoping workshop to model your specific requirements and provide accurate three-year projections.  	References: Data and AI governance for the data lakehouse Best practices for data and AI governance Guiding principles for the lakehouse Data governance with Databricks	Partial	Yes
RES.162	Reference Data Management	What types of reference data do you support (codes, lookups, enumerations, classification schemes, value lists, tariffs, weather codes, market identifiers)?	Yes. Databricks Unity Catalog fully supports all reference data types including codes, lookups, enumerations, classification schemes, value lists, tariffs, weather codes, and market identifiers.  Unity Catalog provides centralised reference data management with unified governance, fine-grained permissions, complete lineage tracking, and search functionality. Key capabilities include:  Data type support: All standard types (STRING, INT, DECIMAL, DATE, TIMESTAMP) plus complex types (ARRAY, MAP, STRUCT) and VARIANT for semi-structured data like JSON codes or hierarchical classifications.  Dimension table patterns: Native support for slowly changing dimensions (SCD Type 1 and Type 2) through Lakeflow Spark Declarative Pipelines using applychanges() and storedasscdtype parameters.  Governed classification: Governed tags enforce consistent classification across catalogs. AI-powered Data Classification (Public Preview) automatically detects and tags sensitive reference data types using predefined system tags.  Data reliability: Delta Lake provides ACID transactions, schema enforcement, versioning, and constraints (NOT NULL, CHECK) to ensure reference data integrity and quality.  Azure integration: Reference tables on ADLS Gen2 integrate with Entra ID for authentication, Azure Key Vault for secrets management, and support private endpoints for secure access.  	References: Unity Catalog Overview Unity Catalog Table Types Database Objects and Tags Implementing Dimensional Data Warehouse	Full	Yes
RES.163	Reference Data Management	Can we define multi-level hierarchies, crosswalks (e.g., internal vs. market codes), and versioning with effective dating?	"Yes. Databricks supports all three capabilities through Unity Catalog, Delta Lake, and Lakeflow Spark Declarative Pipelines.  Multi-level hierarchies: Unity Catalog provides a three-level namespace organisation (catalog.schema.table). Hierarchical relationships, such as customer-to-premise or meter-to-asset, are modelled using Delta tables with primary/foreign key relationships, complex types (STRUCT, ARRAY, MAP) for nested structures, and standard SQL joins.  Crosswalks: Dimension tables support both natural keys (business identifiers, such as internal product codes) and surrogate keys (system-generated identifiers), enabling mapping between internal and market codes. Lookup tables store crosswalks with MERGE operations for updates. IDENTITY columns auto-generate sequential surrogate keys for dimension joins.  Versioning with effective dates: SCD Type 2 tracks the complete change history with START_AT and END_AT timestamp columns that mark version validity periods. Lakeflow Spark Declarative Pipelines provides declarative AUTO CDC with storedasscd_type=""2"", automatically handling out-of-order events. Delta Lake time travel queries any historical version using VERSION AS OF or TIMESTAMP AS OF syntax with configurable retention (default 7-30 days).  This directly supports AGL's energy market data governance requirements, including AEMO code mappings, regulatory audit trails, and maintaining historical accuracy for NEM settlement reconciliation.  "	References: June 2025 Release Notes - Lakeflow Spark Declarative Pipelines Databricks Unity Catalog table types Best practices for data and AI governance Enterprise-Scale Governance: Migrating to Unity Catalog Implementing Dimensional Data Warehouse Lakeflow Pipelines Tutorial Delta Lake History and Time Travel	Full	Yes
RES.164	Reference Data Management	How are change requests initiated and approved (workflow, roles)?	Databricks implements enterprise change management through Git-based workflows integrated with Azure DevOps or GitHub Actions.  Change Workflow: Developers submit pull requests from feature branches, triggering automated CI pipelines for validation. Code review and approval gates enforce peer review before merging to main branches. Databricks Asset Bundles provide infrastructure-as-code for all platform resources including jobs, pipelines, notebooks, and policies, ensuring version-controlled deployments across dev, staging, and production environments. Service principals execute deployments with audited, least-privilege permissions.  Key Roles: Account Admins manage metastores and workspace provisioning. Metastore Admins control catalogs and external storage credentials across workspaces. Workspace Admins manage workspace-scoped resources and policies. catalog and Schema Owners grant privileges on objects they own. Service Principals automate CI/CD with full audit trails.  Azure Privileged Identity Management integrates via SCIM provisioning for just-in-time, time-bound elevation into privileged groups. Access requests flow through PIM approval workflows with automatic removal upon expiration.  Unity catalog audit logs capture all change activity, supporting AGL's compliance and governance requirements for reference data management across your energy operations.  	References: Announcing Public Preview of Request for Access in Unity Catalog Building High-Quality and Trusted Data Products with Databricks Governed tags - Azure Databricks	Full	Yes
RES.165	Reference Data Management	Do you support draft vs. published states, effective/expiry dates, and rollback?	Yes. Databricks provides comprehensive version control through integrated platform capabilities.  MLflow Model Registry in Unity Catalog manages model versions with aliases (Champion/Challenger) and stages (Staging/Production/Archived), enabling controlled promotion workflows from draft to published state.  Delta Lake maintains a versioned transaction log for every table modification. The RESTORE TABLE command enables rollback to any prior version or timestamp. Default retention is 7 days via delta.deletedFileRetentionDuration, extendable to 30+ days for compliance requirements. Unity Catalog's UNDROP TABLE recovers accidentally dropped tables within 8 days, preserving metadata and privileges.  Databricks Git folders integrate with Azure DevOps and GitHub for version control of notebooks, dashboards, and code. Pull request approvals separate draft (feature branches) from published (main/production branches). Databricks Asset Bundles provide development mode (draft, paused schedules) and production mode (published, validated) deployment targets.  Effective and expiry dates are configured through Delta Lake retention policies, Lakeflow Jobs scheduling (start/end dates, pause/unpause), and temporal SQL logic. Notebook version history and Git commit timestamp records are automatically recorded for audit and recovery.  AGL can configure Delta retention policies beyond the default 7 days if extended time travel is required for regulatory compliance, alternatively they can store their data with history (SCD Type 2) to keep a record of all changes.  	References: Delta Lake History Managed Tables UNDROP TABLE Databricks Asset Bundles Deployment Modes Building High-Quality and Trusted Data Products with Databricks Archival Support in Databricks	Full	Yes
RES.166	Reference Data Management	How do you publish/distribute reference data to downstream systems (APIs, events, files, DB sync)?	Databricks provides four governed distribution patterns for reference data, all managed through Unity Catalog.  API-Based Access: Unity Catalog REST API and Iceberg REST Catalog API enable programmatic access with credential vending for short-lived, scoped tokens. JDBC/ODBC drivers through Databricks SQL warehouses support BI tools and custom applications requiring SQL-based integration.  Event Streaming: Structured Streaming natively writes to Kafka, Azure Event Hubs, AWS Kinesis, and Google Pub/Sub. Lakeflow Spark Declarative Pipelines Sink API (Public Preview) provides declarative configuration for Kafka endpoints with exactly-once semantics. Custom PySpark sinks via foreachBatch() handle complex integration patterns.  File-Based Distribution: Unity Catalog volumes provide governed file storage for CSV, JSON, Parquet, Avro, or ORC exports, accessible via REST API, CLI, and SDK. External tables on ADLS Gen2 or S3 allow direct cloud storage reads with Unity Catalog governance maintained.  Database Synchronisation: Lakebase (Public Preview on AWS, Azure) offers fully-managed Postgres with automated Delta table synchronisation in three modes: Snapshot, Triggered, or Continuous. Delta Sharing enables zero-copy sharing to internal and external systems. JDBC writes support bi-directional sync to PostgreSQL, MySQL, and SQL Server.  All patterns maintain Unity Catalog lineage and access controls, ensuring governance extends to downstream consumers. This supports AGL's data governance requirements across operational systems and analytics platforms.  	References: Replicate an external RDBMS table using AUTO CDC Configure PostgreSQL for ingestion into Databricks Tutorial: Build an ETL pipeline using change data capture How Kythera Labs saves using Delta Sharing Introducing Salesforce Connectors for Lakehouse Federation and Lakeflow Connect	Full	Yes
RES.167	Reference Data Management	Can you guarantee consistency across environments (dev/test/prod) and regions? How is cache invalidation handled?	Databricks guarantees environment consistency through Unity Catalog governance and automated cache invalidation across all layers.  Environment Consistency: Unity Catalog enforces isolation through catalog-level separation within a single regional metastore. AGL maintains dev_catalog, test_catalog, and prod_catalog with an identical three-level namespace structure, enabling code portability by parameterising catalog names. Centralised governance ensures access controls, lineage, and audit policies apply uniformly across environments.  Databricks Asset Bundles provide a declarative, version-controlled way to deploy jobs, pipelines, notebooks, and permissions. Git integration enables branch-based promotion with pull request approvals. Infrastructure-as-code via Terraform or Asset Bundles ensures environment parity through identical resource definitions with environment-specific variable overrides.  Cross-Region Consistency: Databricks-to-Databricks Delta Sharing provides zero-copy, governed data access across Australian regions with Unity Catalog managing centralised access controls. Shared data updates appear in near real-time. For frequently accessed reference data that requires local performance, DEEP CLONE with Change Data Feed enables incremental replication orchestrated by Lakeflow Jobs.  Cache Invalidation: All caches invalidate automatically. Unity Catalog metadata cache invalidates on metadata updates with write-through consistency. Delta Lake disk cache automatically detects file changes and evicts stale entries. Query result caches are invalidated when the underlying Delta tables are updated. No manual REFRESH TABLE commands required. 	References: Unity Catalog best practices Query caching Best practices for reliability Databricks Asset Bundles deployment modes Architecting global data collaboration with Delta Sharing	Full	Yes
RES.168	Reference Data Management	Can you enforce validation rules (format, range, uniqueness, referential checks)? Do you provide impact analysis: what tables/processes will be affected by a code change?	Yes. Databricks enforces validation rules through Delta Lake constraints and Lakeflow Spark Declarative Pipelines expectations, while Unity Catalog lineage provides comprehensive impact analysis for code changes.  Validation Rule Enforcement:  Format and Range: Delta Lake CHECK constraints enforce format rules using SQL expressions and regex patterns on every write. Lakeflow SDP expectations provide declarative data quality checks with configurable actions (warn, drop invalid records, or fail pipeline). Schema enforcement prevents non-conforming writes.  Uniqueness: Primary key constraints (GA, Unity Catalog required) declare unique identifiers and enable query optimisations. Uniqueness is enforced upstream using Lakeflow SDP expectations and Spark transformations.  Referential Integrity: Foreign key constraints (GA, Unity Catalog required) define table relationships for documentation and BI integration. These are informational but validated upstream in pipelines using Lakeflow SDP expectations with SQL join patterns.  Impact Analysis:  Unity Catalog automatically captures table and column-level lineage across all languages, showing upstream sources and downstream consumers, including notebooks, jobs, dashboards and ML models. Lineage is visualised in near real-time in Catalog Explorer and accessible via system tables and REST API.  Pre-change impact analysis can be performed using the lineage system tables, allowing you to identify which assets will be affected by schema changes and enabling the safe evolution of AGL's data estate. The system maintains 365-day lineage retention with cross-workspace visibility, supporting trusted analytics for CTAP decision-making.  	References: Constraints in Delta Lake Lakeflow Spark Declarative Pipelines expectations Unity Catalog data lineage Primary key and foreign key constraints GA announcement June 2025 Lakeflow Spark Declarative Pipelines updates Clean and validate data with batch or stream processing Building High-Quality and Trusted Data Products with Databricks	Full	Yes
RES.169	Reference Data Management	RBAC/ABAC for lists and values; segregation of duties between authors and approvers.	Yes. Unity Catalog provides RBAC and ABAC with configurable segregation of duties between authors and approvers.  RBAC operates through hierarchical privilege assignment at catalog, schema, table, column, volume, and function levels. Standard privileges (SELECT, MODIFY, CREATE, USAGE, MANAGE) inherit downward with ownership granting full control. Workspace-level roles provide tiered administrative capabilities with distinct scopes.  ABAC (Public Preview) enables tag-driven policies for scalable access control. Governed tags defined at the account level represent data sensitivity or classification. Policies apply automatically to child objects based on tag values. Row filters and column masks dynamically enforce fine-grained access using user-defined functions that evaluate tags and user attributes at query time.  Segregation of duties is enforced through privilege separation: authors receive CREATE TABLE and MODIFY privileges whilst approvers receive MANAGE privilege or ownership for review and approval without creation rights. Git-based CI/CD workflows enforce approval gates, requiring pull requests to be reviewed before production deployment. Catalog-workspace bindings restrict production catalogs to production workspaces preventing unauthorised modifications. Unity Catalog audit logs (365-day retention) capture all privilege grants, ownership transfers, and data access with full user identity and timestamp details.  A dedicated STEWARDSHIP permission is planned on the product roadmap to enable delegated metadata management without the ability to modify the data itself, allowing data stewards to update glossary terms, classifications, and business metadata without requiring full table ownership.  	References: Unity Catalog attribute-based access control (ABAC) Row filters and column masks How to scale data governance with Attribute-Based Access Control in Unity Catalog	Full	Yes
RES.170	Reference Data Management	Auditability of changes with full diff, who/when/why, and downstream distribution logs.	Yes. Databricks provides complete auditability through three integrated capabilities.  1. Full Diff Tracking: Delta Lake transaction logs maintain an immutable record of every table modification. DESCRIBE HISTORY retrieves version, timestamp, user, operation type, and parameters for every write. Change Data Feed tracks row-level changes with before/after values, supporting GDPR and similar compliance requirements. Transaction logs enable comparing table state between any two versions to identify exact data changes.  2. Who/When/Why Tracking: Unity Catalog audit logs (system.access.audit table, 365-day retention) capture every action with user identity, timestamp, source IP, action name, and request parameters. This includes table access, permission changes with full before/after diff, privilege grants/revokes, and schema modifications. Verbose audit logs capture individual notebook commands showing exact SQL/Python executed. Lineage tables automatically link audit events to notebooks, jobs, and dashboards that performed actions, providing full accountability context.  3. Downstream Distribution Logs: Delta Sharing logs all recipient access via audit events capturing share name, recipient, table, query predicates, and timestamp. System lineage tables (system.access.table_lineage) track every read/write event showing source tables, target tables, and consuming systems. Lakeflow Spark Declarative Pipelines event logs capture data quality results, row counts processed, and records dropped. All logs are queryable via SQL with 365-day retention, supporting AGL's operational transparency and regulatory compliance requirements.  	References: Use Delta Lake change data feed on Databricks Audit log system table reference Best practices for data and AI governance How KPMG uses Delta Sharing to access and audit tens of billions of transactions	Full	Yes
RES.171	Reference Data Management	Out-of-the-box connectors (SAP, Salesforce, Databricks/Delta, SQL DW, message buses)	Databricks provides production-ready connectors for AGL's reference data sources through Lakeflow Connect and Lakehouse Federation, supporting real-time ingestion and zero-copy querying.  Enterprise Applications: Salesforce (Sales Cloud, Service Cloud with SCD Type 2), ServiceNow, Google Analytics, Workday Reports, and Salesforce Data Cloud federation.  Databases: Microsoft SQL Server with CDC and Change Tracking via ExpressRoute, MySQL, PostgreSQL. Lakehouse Federation enables zero-copy queries across Azure Synapse, Amazon Redshift, Snowflake, BigQuery, Oracle, and Teradata without data movement.  Message Buses: Zerobus Ingest provides gRPC API with Python, Java, and Rust SDKs for event streaming ingestion.  Cross-Platform: Databricks-to-Databricks federation for multi-workspace reference data sharing across AGL's energy trading, customer, and asset systems.  Emerging capabilities in Public Preview include NetSuite, Microsoft Dynamics 365, SFTP, and enhanced CDC connectors.  Where applicable, connectors support incremental ingestion to optimise costs and latency, with unified governance through Unity Catalog. Auto Loader with managed file events handles batch reference data updates from legacy systems.  	References: What is Lakeflow Connect? Zerobus Ingest connector overview Introducing Salesforce Connectors for Lakehouse Federation and Lakeflow Connect Announcing the General Availability of SAP Business Data Cloud Connect to Databricks	Full	Yes
RES.172	Reference Data Management	Support for multilingual labels, localization, and industry dictionaries (AEMO/NEM terminology, NAESB, GS1)	Yes, through configuration. Unity Catalog provides the metadata framework to implement multilingual labels and industry-specific terminology, requiring setup rather than pre-loaded dictionaries.  For industry terminology (AEMO/NEM, NAESB, GS1), configure governed tags with industry-specific keys and controlled vocabularies. Tags like nemparticipanttype or gs1gtinlevel enforce standard classifications across data assets, supporting up to 1,000 characters at catalog, schema, table, and column levels.  AI/BI Genie spaces support business terms, synonyms, and value dictionaries that map industry terminology to technical schemas. Space creators document metadata in multiple languages, with natural language interfaces working across Portuguese, French, and other languages.  For AGL's energy market data, this enables standardising AEMO market terminology across trading, settlements, and forecasting datasets whilst maintaining GS1 product hierarchies for retail operations. Table and column comments include standard definitions from these frameworks.  Implementation requires upfront configuration to encode industry standards into Unity Catalog's metadata layer. Once established, definitions are enforced consistently across all data assets and accessible through both technical and business user interfaces.  	References: What is an AI/BI Genie space Revolutionizing Enterprise Data Analytics at ReaderLink - Business Term Mapping Best practices for interoperability and usability	Full	Yes
RES.173	Reference Data Management	Benchmarks for high‑frequency updates and large lists (10k–100k values).	Databricks delivers verified performance for high-frequency updates and large-scale reference data through TPC-DS standard benchmarks and production-proven capabilities.  Performance at scale: TPC‑DS benchmarks and production workloads demonstrate strong price‑performance and near‑linear scaling on large datasets, applicable to reference lists of 10k–100k values.  High-frequency update capabilities rely on three core technologies:  1. Delta Lake ACID transactions enable concurrent MERGE, UPDATE, and DELETE operations with optimistic concurrency control. Multiple writers can simultaneously modify table partitions with serializable isolation, ensuring consistency during frequent reference data updates.  2. Photon acceleration delivers 4x faster scans and 3x faster reads after write optimisation, critical for lookups across large value lists.  3. Automatic liquid clustering optimises data layout during writes without background maintenance, reducing query latency for filtered lookups on reference tables.  Architectural advantages include native ADLS storage avoiding data duplication costs, and Delta Lake’s transaction log and ACID semantics support transactional consistency and idempotent upserts for streaming and batch pipelines. DBSQL Serverless scales elastically to support high concurrency across business units.  This performance foundation ensures reference data remains accurate and accessible across AGL's energy trading, customer, and asset management systems, supporting operational excellence objectives.  	References: Competitive Benchmarks (Under NDA) Delta Lake MERGE operations Isolation levels in Delta Lake Databricks SQL release notes 2025 (Photon improvements) Databricks SQL release notes 2024 (Parallel update operations)	Full	Yes
RES.174	Reference Data Management	How do you prevent breaking changes and support blue/green or canary deployments?	Asset Bundles provide deployment validation and version control integration to prevent breaking changes. Production deployments enforce Git branch alignment, service principal authentication, and automated validation that catches configuration errors and schema conflicts before deployment.  Blue/green deployments work by deploying new versions to separate environments, validating functionality, then switching traffic by updating job configurations to reference new artifact locations. Rollback is immediate through configuration reversion.  For canary releases, MLflow deployment jobs enable percentage-based traffic splits, allowing gradual model rollout with monitoring before full production release. This applies to reference data pipelines where new transformation logic can be validated against production traffic patterns before complete cutover.  Multi-environment promotion through development, staging and production targets provides controlled progression with environment-specific configurations. Deployment locks in production mode prevent concurrent conflicts while maintaining audit trails through Git integration.  This approach supports safe iteration in development environments while enforcing rigorous controls for production changes, ensuring reference data updates deploy without service disruption to AGL's operational systems.  	References: Databricks Asset Bundles deployment modes CI/CD best practices Create a source-controlled pipeline Best practices for operational excellence	Full	Yes
RES.175	Reference Data Management	Planned features (rule expressions, policy ties to reference changes, auto-sync to catalogs).	Unity Catalog provides centralised reference data management today through metadata governance, fine-grained access controls, and lineage tracking. SQL-based row and column filters enable policy enforcement through attribute-based access control using data classification tags.  Regarding the three planned features:  Rule expressions: Unity Catalog currently supports SQL-based validation rules. More sophisticated rule expression engines for reference data validation are under evaluation by product teams, with timelines dependent on customer prioritisation.  Policy ties to reference changes: Change tracking exists through audit logs and lineage today. Native policy triggers that automatically respond to reference data modifications are under consideration for future releases.  Auto-sync to catalogs: Automated metadata discovery works for supported data sources now. Bidirectional synchronisation capabilities across multiple catalog systems are on the product roadmap.  For AGL's immediate requirements, existing Unity Catalog governance combined with Lakeflow Spark Declarative Pipelines can implement reference data validation workflows through configuration. Automated testing and promotion patterns ensure data quality before production deployment, supporting reliable reference data management across AGL's energy operations.  	References: Data governance with Azure Databricks Governed tags Best practices for data and AI governance What's new with Databricks Unity Catalog at Data + AI Summit 2025	Partial	Yes
RES.176	Reference Data Management	Monitoring/alerting, DR, and SLA commitments for distribution latencies.	Databricks provides comprehensive monitoring and alerting capabilities with platform SLAs, while disaster recovery is customer-implemented.  Monitoring and Alerting: Databricks SQL Alerts automate query execution and send notifications via Slack, PagerDuty, or email when thresholds are breached. Lakehouse Monitoring delivers unified observability across data pipelines and ML models with customisable dashboards and automated anomaly detection. Lakeflow Jobs monitoring includes visual dashboards and duration warnings. System Tables expose audit logs, billing usage, and operational metrics queryable via SQL for custom dashboards.  Disaster Recovery: Databricks does not provide native automated DR. Customers implement cross-region DR using Terraform, Delta Deep Clone for data replication, and documented multi-workspace strategies. You define your own RPO and RTO targets through configuration.  SLA Commitments: Platform availability is 99.95% (on Azure). Model Serving provides sub-50ms overhead latency at P99. Distribution latency for streaming and Delta Sharing depends on workload configuration, network proximity, and optimisation settings. No universal distribution latency SLA exists as performance varies by implementation.  	References: Databricks SQL Alerts Lakehouse Monitoring Disaster Recovery Databricks Support and SLAs July 2025 Release Notes - Real-time Streaming Best Practices for Reliability Data Reliability Explained	Full	Yes
RES.177	Reference Data Management	Licensing basis (number of lists, values, API calls)? Cost of multi-region distribution and HA options.	Databricks uses consumption-based pricing measured in Databricks Units (DBUs) per second, not per user, API call, or data volume. DBU rates vary by workload type (Jobs, All-Purpose, SQL), Azure region, and tier (Standard, Premium, Enterprise). You pay only for compute consumed.  Multi-region distribution requires separate workspaces in each Azure region. Each workspace incurs DBU consumption plus Azure infrastructure costs (compute, storage, networking). Cross-region data sharing via Delta Sharing minimises duplication, though egress charges apply for inter-region data movement. For AGL's multi-site operations across the NEM, this enables regional data sovereignty while maintaining centralised governance through Unity Catalog.  High availability within a region uses multi‑AZ resilience; recovery times vary by service, included at no additional cost. Multi-region disaster recovery requires active-passive workspace configuration, with costs for secondary region DBU consumption, data replication (Deep Clone for Delta tables), and periodic synchronisation.  Committed use contracts (1–3 years) offer discounted rates on standard DBU pricing, aligning with AGL’s strategic planning horizons.  	References: Unity Catalog best practices Monitor and manage Delta Sharing egress costs Architecting Global Data Collaboration with Delta Sharing Databricks Pricing Azure Databricks disaster recovery	Full	Yes
RES.178	Metadata Management	Which source types are supported for automated metadata harvesting (data lakes, Delta/Parquet, warehouses, streaming, APIs, BI tools)?	Yes. Unity Catalog supports automated metadata harvesting across all requested source types through runtime-based capture during normal query execution.  Data lakes and Delta/Parquet: Native metadata capture for Delta Lake tables (managed and external), Parquet, CSV, JSON, Avro, ORC, and text files in ADLS Gen2 and S3. Automatic partition discovery supported for external tables.  Warehouses and databases: Lakehouse Federation enables metadata discovery via query federation for MySQL, PostgreSQL, SQL Server, Oracle, Redshift, Synapse, BigQuery, and Snowflake, plus catalog federation for Hive Metastore, AWS Glue, and Snowflake Horizon Catalog. Lakeflow Connect provides managed ingestion connectors with automated schema evolution for SQL Server, MySQL, and PostgreSQL.  Streaming sources: Automatic metadata capture for streaming tables supporting Kafka, Kinesis, Pub/Sub, and Pulsar. Requires Databricks Runtime 11.3 LTS or above for streaming lineage tracking.  SaaS APIs: Lakeflow Connect offers managed connectors for Salesforce, ServiceNow, Workday, NetSuite, Google Analytics, SharePoint, and Microsoft Dynamics 365 with automated schema discovery.  BI tools: External lineage metadata (Public Preview) enables manual registration of downstream tools like Tableau and Power BI via REST API or Catalog Explorer UI. This capability requires configuration and is not fully automated.  Unity Catalog integrates with Microsoft Purview for centralised governance across your entire data estate.  	References: What is a data lakehouse? Databricks integrations overview Databricks technical terminology glossary	Full	Yes
RES.179	Metadata Management	Do you support Unity Catalog, Azure Fabric, Azure Data Factory, Azure AI Services, Delta Lake, Iceberg, lakehouse schemas, and lineage capture from notebooks/ETL jobs?	Yes. Databricks natively supports all listed capabilities through Unity Catalog, our centralised governance layer managing data and AI assets across Azure and AWS. Databricks is the creator of Unity Catalog, Spark, Delta Lake, MLflow, and is the primary contributor to Iceberg via the Tabular acquisition. We pioneered the Lakehouse category and continue to make significant contributions to the open source community.  Unity Catalog provides the three-level namespace (catalog.schema.table), access control, auditing, and automatic lineage capture across notebooks, jobs, and Lakeflow Spark Declarative Pipelines. Lineage tracks data flow to column level with visualisation in catalog Explorer and programmatic access via system tables.  Delta Lake is the default optimised table format, delivering ACID transactions and schema enforcement with data stored in Azure Data Lake Storage. Apache Iceberg tables are supported in Public Preview as Unity Catalog managed tables (read/write) and through Lakehouse Federation for external catalogs, including AWS Glue.  Azure integrations include Azure Data Factory for orchestrating Databricks workloads via native activities and REST API. Azure Fabric can read Unity Catalog Delta tables via OneLake shortcuts using credential vending APIs, though this bypasses Unity Catalog governance for downstream Fabric users. Azure AI Services integration supports Azure OpenAI Service for external models in Agent Bricks Model Serving and Databricks Assistant.  This unified governance approach ensures consistent data quality, security, and compliance across AGL's multicloud data estate while maintaining full visibility into data lineage for regulatory and operational requirements.  	References: Use Microsoft Fabric to read data that is registered in Unity Catalog Announcing General Availability of Lakehouse Federation Expanding support for OneLake in Unity Catalog Lakehouse reference architectures	Full	Yes
RES.180	Metadata Management	How do you model business glossary, data products, domains, ownership, criticality, SLAs?	Unity Catalog provides native metadata management for ownership, lineage, and classification, while business glossary and SLA concepts require organisational conventions.  Ownership: Every object (catalogs, schemas, tables, views, volumes, models, functions) has a designated owner with full privileges. Ownership is transferable, with best practice assigning production objects to groups for continuity.  Business semantics: AI-generated and manual documentation on all objects and columns. Automated column-level lineage tracks relationships across notebooks, jobs, dashboards, and queries. Catalog Explorer provides search, discovery, usage insights, and relationship diagrams.  Domains and data products: No dedicated objects exist. Organisations typically use catalogs to represent business domains or organisational units, with schemas providing granular organisation for teams or use cases. Tags and governed tags enable flexible categorisation.  Criticality and quality: Data quality monitoring (GA) provides anomaly detection, freshness tracking, and profiling. System tags flag certified or deprecated assets. Data Classification uses AI to auto-detect and tag sensitive data.  SLAs: Not a native feature. Implement through tagging conventions combined with data quality monitors configured for freshness and availability thresholds, or integrate external monitoring tools.  	References: October 2023 Release Notes - AI-generated table and column comments Governed tags for Unity Catalog What are catalogs in Databricks Enterprise-Scale Governance: Migrating from Hive Metastore to Unity Catalog Building High-Quality and Trusted Data Products with Databricks	Full	Yes
RES.181	Metadata Management	Can terms be linked to physical assets, policies, KPIs, and stewardship roles?	Yes, Unity Catalog supports linking governance concepts to physical assets through configuration, though it uses a tag-based approach rather than traditional business glossary terms.  Key capabilities include:  1. KPIs and Metrics: Unity Catalog Metrics (Metric Views) define business KPIs as governed assets with semantic metadata, documentation, lineage, and access controls.  2. Tags and Classification: Governed tags apply to catalogs, schemas, tables, columns, volumes, views, and models. These support data classification, resource discovery, and operational automation with enforced consistency rules.  3. Policy Integration: Governed tags serve as attributes in Attribute-Based Access Control (ABAC) policies, enabling dynamic access policies at catalog, schema, or table level based on tag values.  4. Stewardship Roles: Ownership and permissions can be assigned to groups or users for metric views, tags, and other assets. Data stewards create and manage governed tags and tag policies.  Unity Catalog does not provide standalone term objects that link directly to physical assets. For organisations requiring traditional business glossary functionality, Databricks Labs offers the open-source Ontos project, which adds business glossary, data contracts, and master data management capabilities on top of Unity Catalog.  	References: Databricks SQL release notes 2025 Governed tags - Azure Databricks Unity Catalog attribute-based access control (ABAC) Enforce consistent, secure tagging across data and AI assets with Governed Tags in Unity Catalog	Full	Yes
RES.182	Metadata Management	Describe end‑to‑end lineage (table/column, jobask, notebook, BI semantic layer). Do you support time-aware lineage, versioned schemas, and impact analysis for proposed changes?	Yes. Unity Catalog delivers comprehensive automated end-to-end lineage with time-aware capabilities and impact analysis, built on Delta Lake's versioned architecture.  End-to-End Lineage Coverage: Unity Catalog automatically captures runtime lineage across all languages (Python, SQL, R, Scala) without instrumentation. Coverage spans table and column-level lineage where you can click any column to view upstream and downstream dependencies, job and task lineage including notebooks, jobs, Lakeflow Spark Declarative Pipelines, and dashboards, plus BI semantic layer lineage with dashboards and Unity Catalog Metric Views tracked as first-class objects.  Time-Aware Lineage: Lineage system tables include eventtime and eventdate fields for each lineage event. You can filter lineage by time frame within the 365-day retention window. For example, query tables accessed in the last 7 days using eventdate filters.  Versioned Schemas: Delta Lake maintains full table version history via transaction logs. The DESCRIBE HISTORY command shows all schema changes with timestamps, users, and operations. Time travel queries allow accessing any historical schema version by version number or timestamp (default 7 days, configurable to 30+ days). Schema evolution supports adding or modifying columns without breaking production workloads.  Impact Analysis: View complete downstream dependencies including pipelines, notebooks, dashboards, models, and jobs before making changes. Programmatic access via system.access.tablelineage and system.access.column_lineage enables automated impact analysis workflows. Data Quality Monitoring includes downstream impact and root cause analysis based on lineage.  Note: Lineage is not preserved for renamed objects (catalogs, schemas, tables, views, columns).  	References: View data lineage using Unity Catalog Lineage system tables Delta Lake table history Unity Catalog Metric Views May 2025 Release Notes - Updated lineage Unity Catalog Governance in Action - Monitoring, Reporting, and Lineage	Full	Yes
RES.183	Metadata Management	How are policies (masking, row/column filtering, consent) defined and enforced across platforms?	Unity Catalog enforces policies through Attribute-Based Access Control (ABAC), currently in Public Preview. Policies are defined using governed tags assigned to catalogs, schemas, tables or columns, combined with SQL User-Defined Functions for row filtering or column masking. Policies inherit hierarchically to child objects, ensuring consistent enforcement across SQL queries, notebooks and dashboards.  For cross-platform enforcement, Unity Catalog uses the Iceberg REST Catalog API (Preview) to apply policies server-side. When external engines like Apache Spark, Trino, Snowflake or DuckDB request data access, Unity Catalog evaluates policies and returns filtered scan plans. External engines receive only authorised data without implementing custom policy logic.  Critical limitation: Microsoft Fabric Mirroring does not enforce Unity Catalog governance policies. Fabric uses short-lived credentials tied to the configuring user rather than the querying user, allowing downstream Fabric users to bypass Unity Catalog governance entirely. This creates potential information security policy violations. For governed Power BI access, use Direct Query or Azure Data Factory, both of which respect Unity Catalog policies.  Consent management is not native to Unity Catalog but can be implemented through custom UDFs combined with governed tags within the ABAC framework, enabling policy-driven data access aligned with consent requirements.  	References: Unity Catalog attribute-based access control (ABAC) - Azure Databricks Governed tags - Azure Databricks Access control in Unity Catalog - Azure Databricks How to scale data governance with Attribute-Based Access Control in Unity Catalog	Full	Yes
RES.184	Metadata Management	Do you integrate with IAM (Azure AD/Entra, SCIM), and support attribute-based policies synced to execution engines (e.g., Databricks UC, Snowflake, Synapse)?	Yes. Databricks provides native Azure AD/Entra integration with SCIM provisioning and attribute-based access control enforced through Unity Catalog across all execution engines.  Identity management: SCIM synchronises users, groups, and service principals at the account level, enabling centralised identity governance across your Databricks environment. This integrates directly with your existing Azure AD/Entra infrastructure.  Attribute-based policies: Unity Catalog uses governed tags to define data attributes such as sensitivity, classification, and region. You define policies at catalog, schema, or table levels that apply row filters and column masks based on these attributes and user identity. Policies automatically inherit to child objects, eliminating repetitive configuration.  Real-time enforcement: When users query tagged assets, Unity Catalog evaluates applicable policies dynamically across Databricks Runtime, SQL warehouses, and serverless compute. This ensures consistent ABAC enforcement without per-engine configuration.  For AGL, this provides a single governance layer for data access controls across diverse workloads, reducing administrative overhead while maintaining fine-grained security aligned with your data classification requirements.  	References: https://learn.microsoft.com/en-us/azure/databricks/admin/users-groups/scim/aad https://docs.databricks.com/aws/en/data-governance/unity-catalog/abac https://docs.databricks.com/aws/en/data-governance/unity-catalog/abac/policies Access control in Unity Catalog - Azure Databricks Enforce consistent, secure tagging across data and AI assets with Governed Tags in Unity Catalog	Full	Yes
RES.185	Metadata Management	Do you store DQ metrics (freshness, completeness, accuracy, anomaly scores) as metadata and tie them to assets and SLAs?	Yes. Databricks stores data quality metrics as metadata in Unity Catalog system tables and links them directly to data assets through catalog, schema, and table identifiers.  Data quality metrics include freshness, completeness, accuracy, and anomaly scores. Anomaly detection automatically monitors tables and stores results in Unity Catalog system tables with documented retention windows (for example, up to 365 days). Quality metrics can be joined with lineage to enable downstream impact analysis (affected tables and queries) and upstream context from jobs and pipelines.  For SLA enforcement, Databricks provides threshold-based alerting through SQL alerts that monitor quality metric tables and trigger notifications when thresholds are breached. Lakeflow Jobs support duration thresholds and streaming backlog alerts. Webhooks integrate with incident response tools including PagerDuty, Slack, and Microsoft Teams.  This approach gives AGL's data teams visibility into quality metrics across the lakehouse, supporting the Business Intelligence value driver through automated alerting that maintains data reliability standards for critical business processes. This is particularly important for AGL's renewable generation forecasting, customer analytics, and regulatory reporting requirements.  	References: Databricks SQL release notes 2025 October 2025 Release Notes Governed tags documentation Use semantic metadata in metric views Unity Catalog Governance in Action	Full	Yes
RES.186	Metadata Management	Support for SLO/SLA monitoring, alerts, and CI/CD checks on metadata changes.	Databricks provides native SLO/SLA monitoring, alerting, and CI/CD validation for metadata changes across Unity Catalog.  SLO/SLA monitoring uses anomaly detection to track data freshness and completeness metrics, comparing predicted versus actual values. Results are stored in system.dataqualitymonitoring.tableresults with downstream impact analysis and root cause identification through upstream job metadata.  Alerts are configured through SQL queries against data quality monitoring system tables, triggering notifications when thresholds are breached. The audit log system table (system.access.audit) captures Unity Catalog metadata events including createTable, updateTable, and updatePermissions, enabling scheduled queries with automated alerting on schema or permission changes.  Metadata change tracking leverages information schema tables (system.informationschema.tables, .columns) with last_altered timestamps. Audit logs record all schema and permission changes with user identity, timestamp, and change details. Column-level lineage system tables track read/write events on Unity Catalog objects.  CI/CD checks are supported through Databricks Asset Bundles with databricks bundle validate for configuration syntax verification. Pipelines can validate schema compatibility using information schema queries during pull requests, enabling automated schema validation before staging or production deployment.  	References: Anomaly detection Data quality monitoring system tables Information schema CI/CD best practices	Full	Yes
RES.187	Metadata Management	How do users discover assets (faceted search, domains, ratings, endorsements)? Do you provide APIs/SDKs for embedding catalog search into internal portals and for programmatic onboarding?	"Yes. Unity Catalog provides comprehensive discovery and programmatic access for AGL's data governance and self-service requirements.  Discovery capabilities:  Unity Catalog Discover delivers faceted search filtering by catalog, schema, owner, modification date, tags, object type, and certification status. Popularity signals rank assets by user interaction frequency. AI-generated comments incorporate organisation-specific terminology into search results, improving discoverability of AGL's energy generation, retail, and customer operations data.  Catalogs function as domain containers mirroring business units, with schemas representing teams or use cases. This logical organisation supports AGL's federated data architecture across energy hubs and customer platforms.  Certification status tagging (Public Preview) enables marking assets as certified (meeting internal standards, checkmark icon) or deprecated (outdated/unreliable, restricted icon). Users filter via certificationStatus:certified or certificationStatus:deprecated. Applies to catalogs, schemas, tables, views, volumes, models, dashboards, and AI/BI Genie spaces.  Databricks One interface (Public Preview) provides business users a unified discovery experience for curated data products, dashboards, and apps. The ""For you"" page surfaces recently opened items, shared assets, and trending content among similar users.  Programmatic access:  Unity Catalog REST API (/api/2.1/unity-catalog) enables metadata access, search, and table operations. Databricks SDK for Python, Java, and Go provides methods for creating catalogs, schemas, tables, and volumes. External clients register tables via POST /api/2.0/unity-catalogables/, supporting integration with AGL's internal portals and automated onboarding workflows.  "	References: October 2025 Release Notes - Certification Status Feature Discover Data Documentation February 2024 Release Notes - AI-Powered Search What is Databricks One	Full	Yes
RES.188	Metadata Management	Connectors to developer tools (dbt, Airflow, ADF, Fabric pipelines), BI (Power BI, Tableau), and ML platforms.	Yes. Databricks provides native connectors and validated integrations across developer tools, BI platforms, and ML ecosystems, all governed through Unity Catalog.  Developer tools: Native dbt adapter runs as Lakeflow Jobs tasks with full Unity Catalog integration. Apache Airflow provider package orchestrates jobs via REST API. Azure Data Factory integrates directly via Web activity for notebook and script execution. Microsoft Fabric reads Unity Catalog tables via OneLake shortcuts with credential vending, though Unity Catalog governance does not extend to Fabric users.  Power BI and Tableau integrate through Partner Connect with OAuth support. Both support publishing datasets directly from the Databricks UI and exploring Unity Catalog tables. Power BI connects via ODBC/JDBC drivers, whilst Tableau uses the Databricks connector (JDBC) and ODBC across Desktop and Server.  ML platforms: Agent Bricks Gateway provides governed access to foundation models whilst maintaining lineage and access controls.  All integrations leverage open interfaces and APIs, ensuring AGL retains flexibility whilst maintaining data governance across the analytics ecosystem supporting your Business Intelligence value driver.  	References: https://docs.databricks.com/aws/en/partners/prep/dbt https://docs.databricks.com/aws/en/jobs/how-to/use-airflow-with-jobs https://learn.microsoft.com/en-us/azure/databricks/jobs/ https://docs.databricks.com/aws/en/partners/bi/power-bi https://docs.databricks.com/aws/en/partners/bi/tableau Interoperability and usability for the data lakehouse The scope of the lakehouse platform How FedEx built a scalable enterprise-grade data platform with Unity Catalog Announcing the General Availability of Databricks Lakeflow	Full	Yes
RES.189	Metadata Management	Export/import standards (OpenMetadata, Egeria, OpenLineage). How do you interoperate with Microsoft Purview and/or other catalogs?	Unity Catalog provides REST APIs and partner connectors for catalog interoperability. OpenMetadata, Egeria, and OpenLineage integration requires custom development using Unity Catalog's open APIs.  Microsoft Purview integration is in Public Preview on Azure. Purview scans Unity Catalog using a dedicated connector, pulling schema and lineage metadata via open APIs. Metadata synchronisation is available, with Microsoft controlling the rollout pace for advanced lineage and classification capabilities.  Alation and Collibra both integrate directly with Unity Catalog through REST APIs without requiring compute clusters. They extract tables, schemas, column-level lineage, and governance policies. Both partners leverage Unity Catalog's native lineage APIs.  Unity Catalog captures column-level lineage and exposes it through REST APIs, system tables (system.access.table_lineage, system.access.column_lineage), and Bring Your Own Data Lineage (Public Preview) for external systems like Tableau, Power BI, or custom tools.  The Unity Catalog REST API and Iceberg REST Catalog API enable 30+ external engines including Trino, DuckDB, Snowflake, and Starburst to access Unity Catalog metadata with credential vending.  For AGL's multi-vendor data ecosystem, this open API approach supports metadata federation across existing catalog investments whilst maintaining Unity Catalog as the authoritative source for governed data and AI assets. This aligns with your Business Intelligence value driver by enabling consistent governance across your renewable generation, customer, and operational data platforms.  	References: A Year of Interoperability: How Enterprises Are Scaling Governance with Unity Catalog What's New with Databricks Unity Catalog at Data + AI Summit 2024 Expanding support for OneLake in Unity Catalog	Full	Yes
RES.190	Metadata Management	Performance at enterprise scale (millions of assets, thousands of users). Multi‑tenant support, environment isolation, HA/DR, regional deployments, data residency.	Databricks delivers enterprise-scale performance proven across thousands of global deployments, supporting millions of assets and thousands of concurrent users through Unity Catalog and Delta Lake architecture.  Performance and scale: Unity Catalog governs millions of assets, while Delta Lake manages table partitions and high‑volume event streams at scale. The platform supports thousands of concurrent users, critical for AGL’s real‑time energy market operations and customer data processing across 4 million accounts.  Multi-tenant isolation: Unity Catalog provides workspace-catalog binding for logical segregation. Fine-grained controls including row-level security and column masking enforce tenant-level isolation within shared infrastructure. This supports separation between production and development environments and enables secure data sharing across business units whilst maintaining governance.  High availability and disaster recovery: Intra‑region HA with multi‑AZ resilience ensures robustness against zonal failures. Cross‑region DR uses IaC‑based workspace redeploy, Delta Deep Clone or region‑to‑region data replication, and CI/CD automation patterns, supporting AGL’s operational continuity requirements for NEM participation.  Regional deployment and data residency: Databricks operates across multiple AWS and Azure regions. Databricks Geos enforce data residency, ensuring customer content is processed within the workspace geography unless explicitly enabled otherwise. This directly supports AGL’s Australian data sovereignty requirements and compliance obligations.    	References: Unity Catalog best practices High-level architecture Best practices for data and AI governance Enterprise-Scale Governance: Migrating from Hive Metastore to Unity Catalog Disaster recovery on Azure Disaster recovery on AWS Databricks Geos	Full	Yes
RES.191	Metadata Management	Do you provide automated classification, PII detection, and semantic tagging using AI/ML? Can AI assist in term suggestion, lineage inference, and generating documentation from schema/usage?	Yes, with configuration required. Databricks provides AI and ML-powered data governance capabilities through Unity Catalog's Data Classification system, supporting AGL's Business Intelligence value driver with automated metadata management.  Automated PII detection and classification: Data Classification (Public Preview) uses an agentic AI system combining pattern recognition, metadata analysis, and LLMs to automatically identify and tag 15+ sensitive data types at the column level. This includes Australian-specific classifiers for Medicare numbers and Tax File Numbers, as well as global patterns such as credit card numbers, email addresses, and passport numbers. New tables are scanned within 24 hours, and the results are stored in system.dataclassification.results.  Automated semantic tagging: When enabled, the system applies governed tags (e.g., class.email_address, class.au_medicare) to detected columns automatically and continuously as new data arrives. These tags integrate with attribute-based access control policies for dynamic data protection.  AI-generated documentation: Databricks Assistant auto-generates descriptive comments for Unity Catalog objects (catalogs, schemas, tables, columns, functions, models, volumes) using schema metadata and naming patterns. Users review and approve suggestions before application.  Lineage and insights: Unity Catalog automatically captures runtime column-level lineage through execution tracking (not AI-inferred). Databricks Assistant then generates natural language summaries of lineage graphs and usage patterns from this metadata.  Term suggestion for business glossaries is not currently a native AI feature. Unity Catalog Business Semantics requires manual definition of metrics and KPIs, though AI can assist in bootstrapping semantic definitions from existing table structures.  	References: Databricks SQL release notes 2025 Data Classification (AWS) Data Classification (GCP) Find Sensitive Data at Scale with Data Classification in Unity Catalog Automating Data Documentation with AI: How 7-Eleven Bridged the Metadata Gap AI-generated comments documentation	Full	Yes
RES.192	Metadata Management	Typical rollout approach, training, adoption KPIs, and stewardship operating model. Migration strategy from legacy catalogs; maintaining backwards compatibility and redirects.	Databricks Unity Catalog enables a phased, federation-first migration maintaining backwards compatibility whilst establishing centralised governance across AGL's data estate supporting CTAP objectives and AI/BI value drivers.  Migration Strategy: Unity Catalog's Hive Metastore Federation creates a foreign catalog mirroring existing metadata, enabling soft migration where workloads access HMS tables through Unity Catalog's three-level namespace without immediate code changes. This maintains backwards compatibility through legacy two-level namespace semantics, allowing AGL teams to migrate incrementally without disrupting production workloads supporting customer analytics and operational reporting.  We recommend back-to-front migration progressing through Gold (customer-facing analytics), Silver (cleansed datasets), and Bronze (raw ingestion) layers. This prioritises high-visibility business assets supporting AGL's customer experience and energy market operations first, whilst maintaining bidirectional synchronisation during transition.  Training and Enablement: Databricks provides Platform Onboarding, Data Management and Governance courses, solution architect-led workshops, and self-paced training covering Unity Catalog setup, migration best practices, and stewardship operations. Training is tailored to AGL's technical teams managing energy market data, customer analytics, and operational systems.  Stewardship Operating Model: Implement federated governance with defined data steward roles responsible for metadata management, data quality monitoring, and policy enforcement. Unity Catalog's centralised catalog, RBAC/ABAC controls, automated lineage, and system tables provide stewards with observability into data usage patterns and compliance posture across AGL's renewable generation, customer, and asset data.  Adoption KPIs: Track Unity Catalog-enabled workspace percentage, managed table adoption rates, metadata completeness, policy coverage, and governance insights through dashboards powered by Unity Catalog information schema and lineage system tables. These metrics provide visibility into migration progress and governance maturity supporting AGL's data-driven decision making.  	References: November 2025 Release Notes - Foreign Table Conversion Migrate from Legacy Features Best Practices for Data and AI Governance Databricks on Databricks: Kicking off the Journey to Governance with Unity Catalog	Full	Yes
RES.193	Metadata Management	Licensing drivers (assets, connectors, users, compute) and metering approach. Estimated implementation and run costs for a multi‑EDP landscape over 3 years	Databricks uses consumption-based licensing measured in Databricks Units (DBUs), fundamentally different from traditional per-user models. You pay for actual compute usage, not headcount or data volume, which aligns well with AGL's variable workloads across energy trading, customer analytics, and renewable forecasting.  Primary cost drivers are: 1. Compute clusters charged per DBU-hour by workload type (Interactive, Lakeflow Spark Declarative Pipelines, SQL Warehouse, Model Serving) 2. Foundation Model Serving for AI applications (DBUs per million tokens) 3. Managed Storage (Databricks Storage Units) 4. Lakebase Compute (serverless PostgreSQL workloads) 5. Networking charges (passed through at Azure rates)  Unity Catalog provides unlimited metadata management and governance across all workspaces at no additional licence cost, essential for your federated data architecture  across multiple EDPs.  For 3-year cost estimation, we recommend a structured sizing exercise using your current workload profiles: batch job frequencies, streaming pipeline volumes (particularly real-time market data), concurrent analyst patterns, and ML training cadence for demand forecasting. Databricks provides real-time cost monitoring dashboards and budget  alerts within the platform.  We will map representative workloads to DBU consumption patterns, incorporating your growth projections and seasonal demand variations typical of energy operations. This  approach delivers a detailed TCO model accounting for your transition to renewable generation and the data requirements of the Kaluza platform migration. Typical enterprise  implementations show predictable monthly costs within 10 per cent variance after the first quarter.	null	Full	Yes
RES.194	Metadata Management	Security & Compliance Addendum: ISO 27001, SOC 2, IRAP/ASD Essential Eight (for AU), data residency, customer-managed keys (CMK), BYOK. SaaS vs. Private Cloud: Deployment options, sovereign regions, managed VNET/Private Link, peering. Support & Services: SLAs, L3 support, release cadence, roadmap transparency, partner ecosystem.	See responses for RES.135 and RES.153  Deployment Options: Databricks offers standard SaaS with managed infrastructure and VNet injection for customer-managed virtual networks. Azure Private Link provides secure connectivity for both front-end (user-to-workspace) and back-end (compute-to-control plane) connections, eliminating exposure to the public internet. VNet peering enables integration with AGL's existing Azure infrastructure and on-premises systems via ExpressRoute.  Support and Services: As a first-party Azure service, Databricks provides a 99.95% platform uptime guarantee. Technical support cases are managed through the Azure Portal with escalation to Databricks engineering for platform-specific issues. The platform follows a structured release cadence with LTS versions supported for 3 years, monthly platform updates, and quarterly roadmap webinars.  For AWS, Databricks provides a number of plans that provide you with dedicated support and timely service for the Databricks platform and Apache Spark. See Databricks Support Tiers.  The partner ecosystem includes major system integrators (Accenture, Deloitte, Capgemini) with experience in Australian energy-sector implementations.  	References: Security Profile and Compliance IRAP Compliance for Azure Databricks Customer-Managed Keys Configuration Azure Databricks Geographic Regions and Data Residency VNet Injection for Private Networking Azure Private Link Configuration Azure Service Level Agreements Databricks Runtime Versioning and LTS Support Product Roadmap Webinars Databricks Partner Ecosystem	Full	Yes
RES.195	Metadata Management	Does your platform support automated metadata capture across ingestion, transformation, and storage?	Yes. Unity Catalog automatically captures metadata across the entire data lifecycle with no additional licensing or configuration required.  Runtime lineage tracking operates across all languages (Python, SQL, Scala, R) and captures column-level lineage for notebooks, jobs, and dashboards during query execution. Coverage spans ingestion (Lakeflow Connect, Auto Loader, streaming), transformation (Lakeflow Spark Declarative Pipelines, SQL queries), and storage (Delta Lake, Iceberg tables).  Metadata capture is observational and automatic with no performance overhead. Lineage visualises in near real-time via Catalog Explorer and is queryable through system tables (system.access.tablelineage, system.access.columnlineage) with 365-day retention.  For AGL's Business Intelligence value driver, this automated metadata capture supports regulatory reporting requirements (TCFD, SASB, GRI) and enables data governance at scale across energy trading, customer analytics, and asset performance data.  	References: Unity Catalog data lineage documentation Unity Catalog overview System tables documentation Unity Catalog on Azure How FedEx built a scalable, enterprise-grade data platform with IT automation and Databricks Unity Catalog Automating Data Documentation with AI: How 7-Eleven Bridged the Metadata Gap	Full	Yes
RES.196	Metadata Management	Can metadata be versioned and linked to data lineage for audit purposes?	Yes. Unity Catalog provides native metadata versioning fully integrated with data lineage for audit purposes.  Every table modification creates a versioned commit in the Delta Lake transaction log. DESCRIBE HISTORY captures version, timestamp, user identity, operation type, and metrics for each change with configurable retention (30-day default). Time travel queries enable auditors to reconstruct the exact state of data and metadata at any point in time.  Lineage data (1-year retention) links directly to audit logs (365-day retention). System tables including system.access.tablelineage, system.access.columnlineage, and system.access.audit record every read/write event, metadata change, permission update, and data access with full user attribution.  Unity Catalog audit events capture all metadata operations including schema changes, tag assignments, and permission modifications. Each event records securable type, full name, user identity, workspace/metastore ID, and specific changes made.  This integrated approach ensures metadata versioning and lineage tracking meet audit standards without requiring separate systems or manual reconciliation.  	References: Delta table history View data lineage using Unity Catalog System tables Audit logs Best practices for data and AI governance How KPMG uses Delta Sharing to access and audit tens of billions of transactions Enterprise-Scale Governance: Migrating from Hive Metastore to Unity Catalog	Full	Yes
RES.197	Metadata Management	How do you ensure metadata consistency during schema evolution and data migrations?	Databricks ensures metadata consistency through three integrated mechanisms that support AGL's data governance requirements during platform evolution.  Delta Lake provides ACID transaction guarantees via a file-based transaction log. Every schema change is atomic and versioned, preventing corruption during concurrent operations. Schema validation occurs before commit, ensuring writes match the target schema or fail cleanly. This transactional approach maintains consistency for AGL's critical energy data during complex migrations.  Schema evolution is automated and safe. Enable automatic schema evolution using MERGE WITH SCHEMA EVOLUTION syntax or mergeSchema options. New columns are added atomically as part of write transactions with case preservation maintained. Schema enforcement validates all writes against defined constraints (NOT NULL, CHECK), blocking invalid data before table entry.  Unity Catalog maintains metadata synchronisation across AGL's data estate. During Hive metastore federation, Unity Catalog continuously updates table metadata while preserving version history. The SYNC command copies metadata from source systems, and MSCK REPAIR TABLE SYNC METADATA handles external table changes. This ensures consistent metadata visibility across all data assets, supporting AGL's governance and compliance requirements.  These capabilities work together without additional tooling, reducing operational complexity while maintaining metadata integrity essential for regulatory reporting and operational analytics.  	References: Schema evolution in Databricks Databricks SQL release notes 2024 November 2025 Release Notes	Full	Yes
RES.198	Metadata Management	Do you provide APIs for integrating metadata with external governance tools?	Yes. Unity Catalog provides comprehensive REST APIs enabling seamless metadata integration with external governance tools, supporting AGL's enterprise data governance framework.  Key integration capabilities include:  1. Native Azure Purview connector (GA): Microsoft Purview directly scans Unity Catalog metadata including metastores, catalogs, schemas, tables, columns, and views. The connector uses Databricks SQL Warehouse endpoints with token authentication for scheduled metadata extraction.  2. Unity Catalog REST API: The /api/2.1/unity-catalog endpoint exposes metadata for read access by external systems. Access is governed through Unity Catalog privileges, ensuring consistent security policies across integrated tools.  3. Data Lineage API: Table-level and column-level lineage metadata is available via /api/2.0/lineage-tracking endpoints, enabling integration with governance platforms like Purview, Alation, and Collibra. Lineage system tables provide 365 days of queryable history.  4. Bi-directional synchronisation: Advanced integrations support pushing metadata to external catalogs, enabling synchronisation of tags, classifications, comments, and ownership attributes across AGL's governance ecosystem.  This API-first approach ensures Unity Catalog integrates with existing governance investments whilst maintaining a single source of truth for metadata.  	References: External access integrations Unity Catalog REST API Data lineage documentation Governed tags for metadata management Data governance best practices	Full	Yes
RES.199	Metadata Management	How do you enable business users to enrich metadata without compromising governance?	Unity Catalog's security-first architecture creates a governance tension for AGL's data stewards. Enriching metadata requires MODIFY privilege, which also grants INSERT, UPDATE, DELETE, and ALTER capabilities. For views and catalogs, ownership or MANAGE privileges are required. This means choosing between granting broader permissions than desired or maintaining metadata externally with manual synchronisation overhead.  Three practical approaches address this:  1. Databricks Apps: Build a custom metadata management application using a service principal with backend permissions. Authorised stewards enrich metadata through the app interface without direct table privileges. This provides audit trails and controlled access aligned with AGL's governance requirements.  2. Governed tags: Use APPLY TAG permission for limited metadata enrichment without MODIFY access. Tags support classification and discovery workflows for AGL's data catalog.  3. AI-assisted metadata: For users with appropriate access, Databricks Assistant accelerates metadata creation through AI-generated descriptions.  A dedicated metadata-only permission is on the product roadmap to address this gap directly, enabling business users to enrich metadata without data modification rights.  For AGL's immediate needs, the Databricks Apps approach offers the most scalable governance model while maintaining least-privilege principles across your data platform.  	References: Governed tags - Azure Databricks Apply tags to Unity Catalog securable objects Best practices for data and AI governance Data governance with Databricks	Full	Yes
RES.200	Metadata Management	How do you support metadata integration for AEMO market data and DER asset registries?	Databricks supports metadata integration for AEMO market data and DER asset registries through three mechanisms.  First, Lakeflow Connect via Custom PySpark Data Sources ingests data from AEMO DER Registration APIs and NEMWeb market data feeds. Auto Loader handles quarterly CSV downloads at postcode-level aggregation, supporting AGL's grid forecasting and trading operations with current DER asset intelligence.  Second, Unity Catalog creates governed metadata entities representing AEMO DER Register records and market data feeds. Configurable lineage tracks how DER asset data flows into forecasting models, grid analytics, and trading systems, supporting regulatory reporting under National Electricity Rules.  Third, Lakehouse Federation enables in-place querying of AEMO backend databases through foreign catalog registration, avoiding data migration whilst maintaining governance controls.  Unity Catalog applies fine-grained access control, automated lineage tracking, and audit logging across all AEMO integration points. This ensures compliance with National Electricity Rules data privacy requirements whilst enabling AGL data architects to trace DER metadata from source through to forecasting and trading applications.  Authentication supports OAuth and TLS certificates for secure AEMO API connectivity.  	References: Governed tags - Azure Databricks Data Classification - Databricks on AWS Enforce consistent, secure tagging across data and AI assets with Governed Tags in Unity Catalog	Full	Yes
RES.201	Metadata Management	Do you provide accelerators for mapping smart meter data and OT telemetry into enterprise metadata?	Yes. Databricks provides solution accelerators and reference architectures specifically for mapping smart meter and OT telemetry into governed enterprise metadata structures.  The Grid-Edge Analytics accelerator unifies data from smart meters, IoT devices, and sensors into Delta Lake tables with automated schema inference and governance via Unity Catalog. This supports grid behaviour analysis and fault detection relevant to distribution network operations.  For OT integration, the AVEVA CONNECT partnership enables native access to OSI PI historian data with five-minute refresh intervals, including asset hierarchy metadata from PI AF. This maps directly into Unity Catalog schemas, supporting heterogeneous OT environments common in energy operations.  The Energy Grid Operations Reference Architecture demonstrates integration patterns for AMI, SCADA, and historian sources using Lakeflow Connect and medallion architecture. Bronze, Silver, and Gold layers progressively transform OT telemetry into governed enterprise metadata while preserving OT semantic hierarchies such as ISA-95 and Purdue model structures.  Metadata-driven transformation frameworks using Lakeflow Spark Declarative Pipelines map OT sensor streams into Silver manufacturing data models and Gold business-level aggregations, all discoverable through Unity Catalog.  Customer evidence includes Alabama Power, Xcel Energy, and UK Power Networks processing millions of smart meter readings and SCADA telemetry for grid reliability and outage detection.  	References: Grid-Edge Analytics Accelerator Data Intelligence Platform for Energy IT/OT Convergence with Data Intelligence What is Lakeflow Connect Introducing Databricks Lakeflow The scope of the lakehouse platform	Full	Yes
